\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{color}
\usepackage{alltt}
\usepackage{underscore}
\usepackage{environ}
\usepackage{graphicx}
\include{preamble}

\begin{document}

\apih{INITIALIZE}{Initialize GA}

\begin{capi}
\begin{ccode}
void NGA_Initialize()
void GA_Initialize()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_initialize()
subroutine ga_initialize()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GA::Initialize(int argc, char *argv[], size_t limit=0)
\end{cxxcode}
\begin{funcargs}
\inarg{int}{argc}{number of command line arguments}
\inarg{char**}{argv}{command line arguments}
\inarg{size_t}{limit}{amount of memory in bytes per process}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
from ga4py import ga
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Allocate and initialize internal data structures in Global Arrays.

This is a collective operation.

\end{desc}

\apih{INITIALIZE LTD}{Initialize GA with memory limit}

\begin{capi}
\begin{ccode}
void GA_Initialize_ltd(size_t limit)
\end{ccode}
\begin{funcargs}
\inarg{size_t}{limit}{amount of memory in bytes per process}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_initialize_ltd(limit)
\end{fcode}
\begin{funcargs}
\inarg{integer}{limit}{amount of memory in bytes per process}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GA::Initialize(int argc, char *argv[], unsigned long heapSize,
                    unsigned long stackSize, int type, size_t limit=0)
\end{cxxcode}
\begin{funcargs}
\inarg{int}{argc}{number of command line arguments}
\inarg{char**}{argv}{command line arguments}
\inarg{size_t}{limit}{amount of memory in bytes per process}
\inarg{unsigned long}{heapSize}{all of the dynamically allocated local memory}
\inarg{unsigned long}{stackSize}{all of the dynamically allocated local memory}
\inarg{int}{type}{data type}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
None initialize_ltd(size_t limit)
\end{pycode}
\begin{funcargs}
\inarg{size_t}{limit}{amount of memory in bytes per process}
\end{funcargs}
\end{pyapi}

\gcoll

\begin{desc}

Allocate and initialize internal data structures and set the limit for memory
used in Global Arrays. The limit is per process: it is the amount of memory
that the given processor can contribute to collective allocation of Global
Arrays. It does not include temporary storage that GA might be allocating (and
releasing) during execution of a particular operation.

$*limit < 0$ means ``allow unlimited memory usage" in which case this operation
is equivalent to GA_initialize.

This is a collective operation.

\end{desc}

\seealso{SET MEMORY LIMIT,INITIALIZE}

\apih{PGROUP CREATE}{Create a GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_create(int *list, int size)
\end{ccode}
\begin{funcargs}
\inarg{int*}{list[size]}{list of processor IDs in group}
\inarg{int}{size}{number of processors in group}
\outarg{int}{}{pgroup handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_create(list, size)
\end{fcode}
\begin{funcargs}
\inarg{integer}{size}{number of processors in group}
\inarg{integer}{list(size)}{list of processors in processor group}
\outarg{integer}{}{pgroup handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
PGroup::PGroup(int *plist, int size)
\end{cxxcode}
\begin{funcargs}
\inarg{int}{size}{number of processors in group}
\inarg{int*}{plist[size]}{list of processor IDs in group}
\outarg{PGroup}{}{pgroup object}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
ret = pgroup_create(list)
\end{pycode}
\begin{funcargs}
\inarg{iter of ints}{list}{list of processor IDs in group}
\outarg{int}{pgroup}{pgroup handle}
\end{funcargs}
\end{pyapi}

\dcoll

\begin{desc}

This command is used to create a processor group. At present, it must be
invoked by all processors in the current default processor group. The list of
processors use the indexing scheme of the default processor group. If the
default processor group is the world group, then these indices are the usual
processor indices. This function returns a process group handle that can be
used to reference this group by other functions.

This is a collective operation on the default processor group.

\end{desc}

\apih{PGROUP DESTROY}{Destroy a GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_destroy(int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{int}{p_handle}{processor group handle}
\outarg{int}{}{0 if processor group was not previously active}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_pgroup_destroy(p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\outarg{integer}{}{.FALSE. if processor group was not previously active}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
PGroup::~PGroup()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
ret = pgroup_destroy(int pgroup)
\end{pycode}
\begin{funcargs}
\inarg{int}{pgroup}{processor group handle}
\outarg{bool}{ret}{False if processor group was not previously active}
\end{funcargs}
\end{pyapi}

\gcoll

\begin{desc}

This command is used to free up a processor group handle. It returns 0 if the
processor group handle was not previously active.

This is a collective operation on the default processor group.

\end{desc}

\apih{PGROUP SET DEFAULT}{Set a default GA processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_set_default(int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{int}{p_handle}{processor group handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_set_default(p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static void PGroup::setDefault(PGroup *p_handle)
\end{cxxcode}
\begin{funcargs}
\inarg{PGroup*}{p_handle}{processor group}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
None pgroup_set_default(int pgroup)
\end{pycode}
\begin{funcargs}
\inarg{int}{pgroup}{processor group handle}
\end{funcargs}
\end{pyapi}

\gcoll

\begin{desc}

This function can be used to reset the default processor group on a collection
of processors. All processors in the group referenced by p_handle must make a
call to this function. Any standard global array call that is made after
resetting the default processor group will be restricted to processors in that
group. Global arrays that are created after resetting the default processor
group will only be defined on that group and global operations, such as GA_Sync
or GA_Igop, and will be restricted to processors in that group. The
GA_Pgroup_set_default call can be used to rapidly convert large applications,
written with GA, into routines that run on processor groups.

The default processor group can be overridden by using GA calls that require an
explicit group handle as one of the arguments.

This is a collective operation on the group represented by the handle p_handle.

\end{desc}

\apih{CREATE}{Create a global array}

\begin{capi}
\begin{ccode}
int NGA_Create(int type, int ndim, int dims[], char *array_name, int chunk[])
\end{ccode}
\begin{funcargs}
\inarg{char*}{array_name}{a unique character string}
\inarg{int}{type}{data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_create(type, dim1, dim2, array_name, chunk1, chunk2, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{MA type}
\inarg{integer}{dim1,dim2}{array (dim1,dim2) as in FORTRAN}
\inarg{integer}{chunk1,chunk2}{minimum size that dimensions should be chunked up into}
\outarg{integer}{g_a}{handle for future references}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_create(type, ndim, dims, array_name, chunk, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray* createGA(int type, int ndim, int dims[],
                                   char *arrayname, int chunk[])
GlobalArray * GAServices::createGA(int type, int ndim, int dims[],
                                   char *arrayname, int chunk[])
\end{cxxcode}
\begin{funcargs}
\inarg{int}{type}{data type(MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{char*}{arrayname}{a unique character string}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
g_a = create(int gtype, dims, char *name='', chunk=None, int pgroup=-1)
\end{pycode}
\begin{funcargs}
\inarg{str}{name}{a unique character string}
\inarg{int}{type}{data type e.g. C_DBL, C_INT, C_DCPL}
\inarg{iter of ints}{dims}{shape of array}
\inarg{iter of ints}{chunk}{each element specifies minimum size that given dimensions should be chunked up into}
\inarg{int}{pgroup}{processor group handle}
\outarg{int}{g_a}{global array handle}
\end{funcargs}
\end{pyapi}

\dcoll

\begin{desc}

Creates an ndim-dimensional array using the regular distribution model and
returns an integer handle representing the array.

The array can be distributed evenly or not. The control over the distribution
is accomplished by specifying chunk (block) size for all or some of array
dimensions. For example, for a 2-dimensional array, setting chunk[0]=dim[0]
gives distribution by vertical strips (chunk[0]*dims[0]); setting
chunk[1]=dim[1] gives distribution by horizontal strips (chunk[1]*dims[1]).
Actual chunks will be modified so that they are at least the size of the
minimum and each process has either zero or one chunk. Specifying chunk[i] as
less than 1 will cause that dimension to be distributed evenly.

As a convenience, when chunk is specified as NULL, the entire array is
distributed evenly.

Return value: a non-zero array handle means the call was succesful.  This is a
collective operation.

\end{desc}

\apih{CREATE CONFIG}{Create a GA with a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_config(int type, int ndim, int dims[], char *array_name,
                      int chunk[], int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{char*}{array_name}{a unique character string}
\inarg{int}{type}{data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{int}{p_handle}{processor list handle}
\outarg{int}{g_a}{global array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_config(type, ndim, dims, array_name, chunk,
                                   p_handle, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{integer}{p_handle}{processor group handle}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[],
                         char *arrayname,
                         int chunk[],PGroup* p_handle)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[],
                         char *arrayname,
                         int64_t chunk[], PGroup* p_handle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{data type(MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{arrayname}{a unique character string}
\inarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
g_a = create(int gtype, dims, char *name='', chunk=None, int pgroup=-1)
\end{pycode}
\begin{funcargs}
\inarg{str}{name}{a unique character string}
\inarg{int}{type}{data type e.g. C_DBL, C_INT, C_DCPL}
\inarg{iter of ints}{dims}{shape of array}
\inarg{iter of ints}{chunk}{each element specifies minimum size that given dimensions should be chunked up into}
\inarg{int}{pgroup}{processor group handle}
\outarg{int}{g_a}{global array handle}
\end{funcargs}
\end{pyapi}

\dcoll

\begin{desc}

Creates an ndim-dimensional array using the regular distribution model but with
an explicitly specified processor list handle and returns an integer handle
representing the array.

This call is essentially the same as the NGA_Create call, except for the
processor list handle p_handle. It can be used to create mirrored arrays.

Return value: a non-zero array handle means the call was succesful.

This is a collective operation.

\end{desc}

\apih{CREATE GHOSTS}{Create a GA with ghost cells}

\begin{capi}
\begin{ccode}
int NGA_Create_ghosts(int type, int ndim, int dims[], int width[],
                      char *array_name, int chunk[])
\end{ccode}
\begin{funcargs}
\inarg{char*}{array_name}{a unique character string}
\inarg{int}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{int*}{width[ndim]}{array of ghost cell widths}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\outarg{int}{}{global array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts(type, ndim, dims, width, array_name,
                                   chunk, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{width(ndim)}{array of ghost cell widths}
\inarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname, int chunk[], char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], int64_t width[],
                         char *arrayname, int64_t chunk[], char ghosts)
GlobalArray * GAServices::createGA_Ghosts(int type, int ndim, int dims[],
                                          int width[], char *array_name,
                                          int chunk[])
\end{cxxcode}
\begin{funcargs}
\inarg{int}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{int*}{width[ndim]}{array of ghost cell widths}
\inarg{char*}{array_name}{a unique character string}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{char}{ghosts}{this is a dummy parameter: added to increase the number of arguments, in order to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
g_a create_ghosts(int gtype, dims, width, char *name='',
                  chunk=None, int pgroup=-1)
\end{pycode}
\begin{funcargs}
\inarg{char*}{name}{a unique character string}
\inarg{int}{gtype}{data type (C_DBL,C_INT,C_DCPL)}
\inarg{iter of ints}{dims}{array of dimensions}
\inarg{iter of ints}{width}{array of ghost cell widths}
\inarg{iter of ints}{chunk}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\outarg{int}{}{global array handle}
\end{funcargs}
\end{pyapi}

\dcoll

\begin{desc}

Creates an ndim-dimensional array with a layer of ghost cells around the
visible data on each processor using the regular distribution model and returns
an integer handle representing the array.

The array can be distributed evenly or not evenly. The control over the
distribution is accomplished by specifying chunk (block) size for all or some
of the array dimensions. For example, for a 2-dimensional array, setting
chunk(1)=dim(1) gives distribution by vertical strips (chunk(1)*dims(1));
setting chunk(2)=dim(2) gives distribution by horizontal strips
(chunk(2)*dims(2)). Actual chunks will be modified so that they are at least
the size of the minimum and each process has either zero or one chunk.
Specifying chunk(i) as \textless 1 will cause that dimension (i-th) to be
distributed evenly. The width of the ghost cell layer in each dimension is
specified using the array width(). The local data of the global array residing
on each processor will have a layer width[n] ghosts cells wide on either side
of the visible data along the dimension n.

Return value: a non-zero array handle means the call was successful.

This is a collective operation.

\end{desc}

\apih{CREATE GHOSTS CONFIG}{Create a GA with ghost cells and specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_ghosts_config(int type, int ndim, int dims[],
                             int width[], char *array_name, int chunk[],
                             int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{char*}{array_name}{a unique character string}
\inarg{int}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{int}{ndim}{number of array dimensions}
\inarg{int*}{dims[ndim]}{array of dimensions}
\inarg{int*}{width[ndim]}{array of ghost cell widths}
\inarg{int*}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{int*}{p_handle}{processor list handle}
\outarg{int}{}{global array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_config(type, ndim, dims, width, array_name, chunk, p_handle, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{width(ndim)}{array of ghost cell widths}
\inarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{integer}{p_handle}{processor group handle}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname, int chunk[], PGroup* p_handle,
                         char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], int64_t width[],
                         char *arrayname, int64_t chunk[], PGroup* p_handle,
                         char ghosts)
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{width[ndim]}{array of ghost cell widths}
\inarg{}{array_name}{a unique character string}
\inarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}
\inarg{}{p_handle}{processor group handle}
\inarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}
\end{funcargs}
\end{cxxapi}

\dcoll

\begin{desc}

  Creates an ndim-dimensional array with a layer of ghost cells around
  the visible data on each processor using the regular distribution
  model and an explicitly specified processor list and returns an
  integer handle representing the array.

  This call is essentially the same as the NGA_Create_ghosts call,
  except for the processor list handle p_handle. It can be used to
  create mirrored arrays.

  Return value: a non-zero array handle means the call was successful.
  This is a collective operation.

\end{desc}

\apih{CREATE IRREG}{Create an irregular-distributed GA}

\begin{capi}
\begin{ccode}
int NGA_Create_irreg(int type, int ndim, int dims[], char *array_name,
                     int block[], int map[])
\end{ccode}
\begin{funcargs}
\inarg{}{array_name}{a unique character string}
\inarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims}{array of dimension values}
\inarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{map[s]}{starting index for for each block; the size s is a sum all elements of nblock array}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_create_irreg(type, dim1, dim2, array_name, map1,
                                 nblock1, map2, nblock2, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{MA type}
\inarg{integer}{dim1,dim2}{array (dim1,dim2) as in FORTRAN}
\inarg{integer}{nblock1}{no. of blocks first dimension is divided into}
\inarg{integer}{nblock2}{no. of blocks second dimension is divided into}
\inarg{integer}{map1(*)}{ilo for each block}
\inarg{integer}{map2(*)}{jlo for each block}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_create_irreg(type, ndim, dims, array_name, map,
                                  nblock, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}
\inarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray * GAServices::createGA(int type, int ndim, int dims[],
                                   char *arrayname,
                                   int block[], int maps[])
GlobalArray::GlobalArray(int type, int ndim, int dims[], char *arrayname,
                         int block[],int maps[]);
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[],
                         char *arrayname, int64_t block[],
                         int64_t maps[])
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims}{array of dimension values}
\inarg{}{arrayname}{a unique character string}
\inarg{}{block[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{maps[s]}{starting index for for each block; the size s is a sum all elements of nblock array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_irreg(int gtype, dims, block, map, char *name='', int pgroup=-1)
   gtype (int)                       - the type of the array
   dims (1D array-like of integers)  - shape of the array
   block (1D array-like of integers) - the number of blocks each dimension
                                       is divided into
   map (1D array-like of integers)   - starting index for each block
                                       len(map) == sum of all elements of
                                       nblock array
   name (string)                     - the name of the array
   pgroup (int)                      - create array only as part of this
                                       processor group
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  Creates an array by following the user-specified distribution and
  returns an integer handle representing the array.

  The distribution is specified as a Cartesian product of
  distributions for each dimension. The array indices start at 0.
For example, Figure \ref{crirreg} demonstrates the distribution of a
2-dimensional 8x10 array on 6 (or more) processors.

nblock[2]=\{3,2\}, the size of the map array is s=5 and the array map contains the following

  elements map=\{0,2,6, 0, 5\}. The distribution is nonuniform because
  P1 and P4 get 20 elements each and processors P0, P2, P3, and P5 only
  10 elements each.


\begin{figure}
\includegraphics{CrIrreg}
\centering
\caption{Creating an Irregular Array}
\label{crirreg}
\end{figure}

  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.

\end{desc}


\apih{CREATE IRREG CONFIG}{Create an irregular-distributed GA with a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_irreg_config(int type, int ndim, int dims[],
                            char *array_name, int block[], int map[],
                            int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{array_name}{a unique character string}
\inarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims}{array of dimension values}
\inarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{map[s]}{starting index for for each block; the size s is a sum all elements of nblock array}
\end{funcargs}
   p_handle               - processor list handle
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_irreg_config(type, ndim, dims, array_name, map, nblock, p_handle, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}
\inarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}
\inarg{integer}{p_handle}{processor group handle}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::  GlobalArray(int type, int ndim, int dims[],
                           char *arrayname, int block[],
                           int maps[], PGroup* p_handle)
GlobalArray::  GlobalArray(int type, int ndim, int64_t dims[],
                           char *arrayname,
                           int64_t block[], int64_t maps[],
                           PGroup* p_handle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims}{array of dimension values}
\inarg{}{arrayname}{a unique character string}
\inarg{}{block[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{maps[s]}{starting index for for each block; the size s is a sum all elements of nblock array}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{cxxapi}
\dcoll

\begin{desc}

  Creates an array by following the user-specified distribution and an
  explicitly specified processor list handle and returns an integer
  handle representing the array.

  This call is essentially the same as the NGA_Create_irreg call,
  except for the processor list handle p_handle. It can be used to
  create mirrored arrays.

  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.

\end{desc}


\apih{CREATE GHOST IRREG}{Create an irregular-distributed GA with ghost cells}

\begin{capi}
\begin{ccode}
int NGA_Create_ghost_irreg(int type, int ndim, int dims[], width[],
                           char *array_name, nblock[], map[])
\end{ccode}
\begin{funcargs}
\inarg{}{array_name}{a unique character string}
\inarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{width[ndim]}{array of ghost cell widths}
\inarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{map[s]}{starting index for for each block; the size     s is a sum of all elements of nblock array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_irreg(type, ndim, dims, width, array_name, map, nblock, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{width(ndim)}{array of ghost cell widths}
\inarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}
\inarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname,
                         int block[], int maps[], char ghosts);
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[],
                         int64_t width[], char *arrayname,
                         int64_t block[], int64_t maps[], char ghosts)
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{width[ndim]}{array of ghost cell widths}
\inarg{}{arrayname}{a unique character string}
\inarg{}{block[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{maps[s]}{starting index for for each block; the size s is a sum of all elements of nblock array}
\inarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_ghosts_irreg(int gtype, dims, width, block, map, char *name='',
int pgroup=-1)
   gtype (int)                       - the type of the array
   dims (1D array-like of integers)  - shape of the array
   width (1D array-like of integers) - ghost cell widths
   block (1D array-like of integers) - number of blocks each dimension is
                                       divided into
   map (1D array-like of integers)   - starting index for each block
                                       len(map) == sum of all elements of
                                       nblock array
   name (string)                     - the name of the array
   pgroup (int)                      - create array only as part of this
                                       processor group
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  Creates an array with ghost cells by following the user-specified
  distribution and returns an integer handle representing the array.

  The distribution is specified as a Cartesian product of
  distributions for each dimension.

Figure \ref{crghostir} demonstrates distribution of a 2-dimensional
array 8x10 on 6 (or more) processors.

nblock(2)=\{3,2\}, the size of map array is s=5 and the array map contains
the following elements map=\{1,3,7, 1, 6\}. The distribution is nonuniform
because, P1 and P4 get 20 elements each and processors P0, P2, P3, and P5
only 10 elements each.

The array width[] is used to control the width of the ghost cell boundary
around the visible data on each processor. The local data of the Global Array
residing on each processor will have a layer width[n] ghosts cells wide on
either side of the visible data along the dimension n.

Return value: a non-zero array handle means the call was succesful.

This is a collective operation.

\begin{figure}
\includegraphics{CrGhostIr}
\centering
\caption{Creating an Array with Ghost Cells}
\label{crghostir}
\end{figure}

\end{desc}


\apih{CREATE GHOSTS IRREG CONFIG}{Create an irregular-distributed GA with ghost cells and a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_ghost_irreg_config(int type, int ndim, int dims[],
                                  width [], char*array_name,nblock[],
                                  map[], int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{array_name}{a unique character string}
\inarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{width[ndim]}{array of ghost cell widths}
\inarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{map[s]}{starting index for for each block; the size     s is a sum of all elements of nblock array}
\inarg{}{p_handle}{processor list handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_irreg_config(type, ndim,
                                                dims, width, array_name,
                                                map, nblock,
                                                p_handle, g_a)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a unique character string}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{dims(ndim)}{array of dimensions}
\inarg{integer}{width(ndim)}{array of ghost cell widths}
\inarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}
\inarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}
\inarg{integer}{p_handle}{processor group handle}
\outarg{integer}{g_a}{integer handle for future references}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname, int block[], int maps[],
                         PGroup* p_handle, char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[],
                         int64_t width[], char *arrayname,
                         int64_t block[], int64_t maps[],
                         PGroup* p_handle,char ghosts)
\end{cxxcode}
\begin{funcargs}
\inarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{dims[ndim]}{array of dimensions}
\inarg{}{width[ndim]}{array of ghost cell widths}
\inarg{}{arrayname}{a unique character string}
\inarg{}{block[ndim]}{no. of blocks each dimension is divided into}
\inarg{}{maps[s]}{starting index for for each block; the size s is a sum of all elements of nblock array}
\inarg{}{p_handle}{processor group handle}
\inarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}
\end{funcargs}
\end{cxxapi}
\dcoll

\begin{desc}

Creates an array with ghost cells by following the user-specified distribution
and returns integer handle representing the array.

This call is essentially the same as the NGA_Create_ghosts_irreg call, except
for the processor list handle p_handle. It can be used to create mirrored arrays.

Return value: a non-zero array handle means the call was succesful.

This is a collective operation.

\end{desc}


\apih{CREATE HANDLE}{Create a handle to a global array}

\begin{capi}
\begin{ccode}
int GA_Create_handle()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_create_handle()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_handle()
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  This function returns a Global Array handle that can then be used to
  create a new Global Array. This is part of a new API for creating
  Global Arrays that is designed to replace the old interface built
  around the NGA_Create_xxx calls. The sequence of operations is to
  begin with a call to GA_Greate_handle to get a new array handle. The
  attributes of the array, such as dimension, size, type, etc., can
  then be set using successive calls to the GA_Set_xxx subroutines.
  When all array attributes have been set, the GA_Allocate subroutine
  is called and the Global Array is actually created and memory for it
  is allocated.

  This is a collective operation.

\end{desc}


\apih{SET ARRAY NAME}{Set the array name for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_array_name (int g_a, char *name)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{name}{array name}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_array_name(g_a, name)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{character*(*)}{name}{a unique character string}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setArrayName(char *name) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{name}{array name}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_array_name(int g_a, char *name)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to assign a unique character string name
  to a Global Array handle that was obtained using the
  GA_Create_handle function.

  This is a collective operation.

\end{desc}


\apih{SET DATA}{Set the data properties for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_data (int g_a, int ndim, int dims[], int type)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{dimension of global array}
\inarg{}{dims[]}{dimensions of global array}
\inarg{}{type}{data type of global array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_data (g_a, ndim, dims, type)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{dimension of array}
\inarg{integer}{dims(ndim)}{array dimensions}
\inarg{integer}{type}{data type (MT_DBL,MT_INT,etc.)}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setData(int ndim, int dims[], int type) const
void GlobalArray::setData(int ndim, int64_t dims[], int type) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{ndim}{dimension of global array}
\inarg{}{dims}{dimensions of global array}
\inarg{}{type}{data type of global array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_data(int g_a, dims, int type)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the array dimension, the coordinate
  dimensions, and the data type assigned to a Global Array handle
  obtained using the GA_Create_handle function.

  This is a collective operation.
\end{desc}


\apih{SET IRREG DISTR}{Specify irregular distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_irreg_distr(int g_a, int mapc[], int nblock[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{mapc[s]}{starting index for each block; the size s is the sum of all elements of the array nblock}
\inarg{}{nblock[ndim]}{number of blocks that each dimension is divided into}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_irreg_distr(g_a, mapc, nblock)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}
\inarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setIrregDistr(int mapc[], int nblock[]) const
void GlobalArray::setIrregDistr(int64_t mapc[], int64_t nblock[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{mapc[s]}{starting index for each block; the size s is the sum of all elements of the array nblock}
\inarg{}{nblock[ndim]}{number of blocks that each dimension is divided into}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_irreg_distr(int g_a, mapc, nblock)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to partition the array data among the
  individual processors for a global array handle obtained using the
  GA_Create_handle function.

  The distribution is specified as a Cartesian product of
  distributions for each dimension. For example, the following figure
  demonstrates distribution of a 2-dimensional array 8x10 on 6 (or
  more) processors. nblock(2)=\{3,2\}, the size of mapc array is s=5 and
  array mapc contains the following elements mapc=\{1, 3, 7, 1, 6\}. The
  distribution is nonuniform because, P1 and P4 get 20 elements each
  and processors P0, P2, P3, and P5 only 10 elements each.

  The array width() is used to control the width of the ghost cell
  boundary around the visible data on each processor. The local data
  of the global array residing on each processor will have a layer
  width(n) ghosts cells wide on either side of the visible data along
  the dimension n.
An example is shown in Figure \ref{setirregdist}.

\begin{figure}
\centering
\includegraphics{SetIrregDist}
\caption{Set an Irregular Distribution}
\label{setirregdist}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{SET PGROUP}{Set the processor group for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_pgroup(int g_a, int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_pgroup(g_a, p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{p_handle}{processor group handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setPGroup(PGroup *pHandle) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{pHandle}{processor group handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_pgroup(int g_a, int pgroup)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the processor configuration
  assigned to a global array handle that was obtained using the
  GA_Create_handle function. It can be used to create mirrored arrays
  by using the mirrored array processor configuration in this function
  call. It can also be used to create an array on a processor group by
  using a processor group handle in this call.

  This is a collective operation.

\end{desc}

\apih{SET RESTRICTED}{Specify a GA handle to be allocated on a subset of processors}

\begin{capi}
\begin{ccode}
void GA_Set_restricted(int g_a, int list[], int nproc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{list[nproc]}{list of processor IDs that contain data}
\inarg{}{nproc}{number of processors that contain data}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_restricted(g_a, list, nproc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{list(nproc)}{list of processor IDs that contain data}
\inarg{integer}{nproc}{number of processors that contain data}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setRestricted(int list[], int nprocs) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{list}{list of processors that should contain data}
\inarg{}{nprocs}{number of processors in list}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

  This function restricts data in the global array g_a to only the
  nproc processors listed in the array list. The value of nproc must be
  less than or equal to the number of available processors. If this
  call is used in conjunction with GA_Set_irreg_distr, then the
  decomposition in the GA_Set_irreg_distr call must be done assuming
  that the number of processors is nproc. The data that ordinarily
  would be mapped to process 0 is mapped to the process in list[0],
  the data that would be mapped to process 1 will be mapped to
  list[1], etc. This can be used to remap the data distribution to
  different processors, even if nproc equals the number of available
  processors.

This is a collective operation.

\end{desc}

\apih{SET RESTRICTED RANGE}{Specify a GA handle to be created on a subset (as a range) of processors}

\begin{capi}
\begin{ccode}
void GA_set_restricted_range(int g_a, int lo_proc, int hi_proc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{lo_proc}{range of processors (inclusive) that contain data}
\inarg{}{hi_proc}{range of processors (inclusive) that contain data}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_restricted_range(g_a, lo_proc, hi_proc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{lo_proc}{range of processors (inclusive) that contain data}
\inarg{integer}{hi_proc}{range of processors (inclusive) that contain data}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setRestrictedRange(int lo_proc, int hi_proc) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo_proc}{low end of processor range}
\inarg{}{hi_proc}{high end of processor range}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

  This function restricts data in the global array to the range of
  processors beginning with lo_proc and ending with hi_proc. Both
  lo_proc and hi_proc must be less than or equal to the total number
  of processors minus one (e.g., in the range [0,N-1], where N is the
  total number of processors) and lo_proc must be less than or equal
  to hi_proc. If lo_proc = 0 and hi_proc = N-1 then this call has no
  effect on the data distribution.

  This is a collective operation.

\end{desc}

\apih{SET GHOSTS}{Specify ghost cells for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_ghosts(int g_a, int width[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{width[ndim]}{array of ghost cell widths}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_ghosts(g_a, width)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{width(ndim)}{array of ghost cell widths}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setGhosts(int width[]) const
void GlobalArray::setGhosts(int64_t width[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{width[ndim]}{array of ghost cell widths}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_ghosts(int g_a, width)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the ghost cell widths for a global
  array handle that was obtained using the GA_Create_handle function.
  The ghosts cells widths indicate how many ghost cells are used to
  pad the locally held array data along each dimension. The padding
  can be set independently for each coordinate dimension.

  This is a collective operation.

\end{desc}

\apih{SET CHUNK}{Specify chunk size for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_chunk(int g_a, int chunk[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{chunk[]}{array of chunk widths}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_chunk(g_a, chunk)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{chunk(ndim)}{array of chunk widths}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setChunk(int chunk[]) const
void GlobalArray::setChunk(int64_t chunk[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{chunk}{array of chunk widths}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_chunk(int g_a, chunk)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function is used to set the chunk array for a global array
  handle that was obtained using the GA_Create_handle function. The
  chunk array is used to determine the minimum number of array
  elements assigned to each processor along each coordinate direction.

  This is a collective operation.

\end{desc}

\apih{SET BLOCK CYCLIC}{Specify round-robin distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_block_cyclic(int g_a, int dims[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{dims[]}{array of block dimensions}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_block_cyclic(g_a, dims)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{dims(ndim)}{array of block dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setBlockCyclic(int dims[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{dims}{array of block dimensions}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_block_cyclic(int g_a, dims)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This subroutine is used to create a global array with a simple
  block-cyclic data distribution. The array is broken up into blocks
  of size dims and each block is numbered sequentially using a column
  major indexing scheme. The blocks are then assigned in a simple
  round-robin fashion to processors.

Figure \ref{stblkcy} illustrates an array containing 25 blocks distributed on 4 processors.

  Blocks at the edge of the array may be smaller than the block size
  specified in dims. In the example below, blocks
  4, 9, 14, 19, 20, 21, 22, 23, and 24 might be smaller than the remaining
  blocks. Most global array operations are insensitive to whether or
  not a block-cyclic data distribution is used, although performance
  may be slower in some cases if the global array is using a
  block-cyclic data distribution. Individual data blocks can be
  accessesed using the block-cyclic access functions.

\begin{figure}
\centering
\includegraphics{StBlkCy}
\caption{Set Block Cyclic Data Distribution}
\label{stblkcy}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{SET BLOCK CYCLIC PROC GRID}{Specify block-cyclic processor distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_block_cyclic(int g_a, int dims[], int proc_grid[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{dims[]}{array of block dimensions}
\inarg{}{proc_grid[]}{processor grid dimensions}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_block_cyclic_proc_grid(g_a, dims, proc_grid)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{dims(ndim)}{array of block dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setBlockCyclicProcGrid(int dims[], int proc_grid[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{dims}{array of block dimensions}
\inarg{}{proc_grid}{processor grid dimensions}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_block_cyclic_proc_grid(int g_a, block, proc_grid)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This subroutine is used to create a global array with a
  SCALAPACK-type block cyclic data distribution. The user specifies
  the dimensions of the processor grid in the array proc_grid. The
  product of the processor grid dimensions must equal the number of
  total number of processors and the number of dimensions in the
  processor grid must be the same as the number of dimensions in the
  global array. The data blocks are mapped onto the processor grid in
  a cyclic manner along each of the processor grid axes.

Figure \ref{setblkcyprocgrid} illustrates an array consisting of 25 data blocks distributed on 6 processors.

The 6 processors are configured in a 3 by 2 processor grid. Blocks at
the edge of the array may be smaller than the block size specified in dims.
Most global array operations  are insensitive to whether or not a block-cyclic
data distribution is used, although performance may be slower in some cases
if the global array is using a block-cyclic data distribution. Individual data
blocks can be accessesed using the block-cyclic access functions.

\begin{figure}
\centering
\includegraphics{SetBlkCyProcGrid}
\caption{Creating a SCALAPACK-type Block Cyclic Data Distribution}
\label{setblkcyprocgrid}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{ALLOCATE}{Allocate the array specified by a GA handle}

\begin{capi}
\begin{ccode}
int GA_Allocate(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{ga}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_allocate(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::allocate() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
allocate(int g_a)
   g_a (int)                         - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function allocates the memory for the global array handle
  originally obtained using the GA_Create_handle function. At a
  minimum, the GA_Set_data function must be called before the memory
  is allocated. Other GA_Set_xxx functions can also be called before
  invoking this function.

Returns True if allocation of g_a was successful.

  This is a collective operation.

\end{desc}

\apih{UPDATE GHOSTS}{Update ghost cells}

\begin{capi}
\begin{ccode}
void GA_Update_ghosts(int g_a)
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_update_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::updateGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
update_ghosts(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This call updates the ghost cell regions on each processor with the
  corresponding neighbor data from other processors. The operation
  assumes that all data is wrapped around using periodic boundary data
  so that ghost cell data that goes beyound an array boundary is
  wrapped around to the other end of the array. The GA_Update_ghosts
  call contains two GA_Sync calls before and after the actual update
  operation. For some applications these calls may be unecessary, if
  so they can be removed using the GA_Mask_sync subroutine.
This is a collective operation.

\end{desc}

\apih{UPDATE GHOST DIR}{Update ghost cells along a specific direction}

\begin{capi}
\begin{ccode}
int NGA_Update_ghost_dir(int g_a, int dimension, int idir, int cflag)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{dimension}{array dimension that is to be updated}
\inarg{}{idir}{direction of update (+/- 1)}
\inarg{}{cflag}{flag (0/1) to include corners in update}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_update_ghost_dir(g_a,dimension,idir,cflag)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{dimension}{array dimension that is to be updated}
\inarg{integer}{idir}{direction of update (+/-1)}
\inarg{logical}{cflag}{flag to include corners in update}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::updateGhostDir(int dimension, int idir, int cflag) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{dimension}{array dimension that is to be updated}
\inarg{}{idir}{direction of update (+/- 1)}
\inarg{}{cflag}{flag (0/1) to include corners in update}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
update_ghost_dir(int g_a, int dimension, int dir, int flag)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to update the ghost cells along individual
  directions. It is designed for algorithms that can overlap updates
  with computation. The variable dimension indicates which coordinate
  direction is to be updated (e.g. dimension = 1 would correspond to
  the y axis in a two or three dimensional system), the variable idir
  can take the values +/-1 and indicates whether the side that is to
  be updated lies in the positive or negative direction, and cflag
  indicates whether or not the corners on the side being updated are
  to be included in the update. The following calls would be
  equivalent to a call to GA_Update_ghosts for a 2-dimensional system:

\begin{verbatim}
     status = NGA_Update_ghost_dir(g_a,0,-1,1);
     status = NGA_Update_ghost_dir(g_a,0,1,1);
     status = NGA_Update_ghost_dir(g_a,1,-1,0);
     status = NGA_Update_ghost_dir(g_a,1,1,0);
\end{verbatim}

         The variable cflag is set equal to 1 (or non-zero) in the
         first two calls so that the corner ghost cells are update, it
         is set equal to 0 in the second two calls to avoid redundant
         updates of the corners. Note that updating the ghosts cells
         using several independent calls to the nga_update_ghost_dir
         functions is generally not as efficient as using
         GA_Update_ghosts unless the individual calls can be
         effectively overlapped with computation.

This is a  collective operation.

\end{desc}

\apih{HAS GHOSTS}{Check whether a GA has ghost cells}

\begin{capi}
\begin{ccode}
int GA_Has_ghosts(int g_a)
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_has_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::hasGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
has_ghosts(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This function returns 1 if the global array has some dimensions for
which the ghost cell width is greater than zero, it returns 0 otherwise.
This is a collective operation.

\end{desc}

\apih{ACCESS GHOSTS}{Access the ghost cells allocated locally on a GA}

\begin{capi}
\begin{ccode}
void NGA_Access_ghosts(int g_a, int dims[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{dims[ndim]}{array of dimensions of local patch, including ghost cells}
\outarg{}{ptr}{returns an index corresponding to the origin the global array patch held locally on the processor}
\outarg{}{ld[ndim-1]}{physical dimenstions of the local array patch, including ghost cells}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_ghosts(g_a, dims, index, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{integer}{dims(ndim)}{array of dimensions of local patch, including ghost cells}
\outarg{integer}{index}{returns an index corresponding to the origin the global array patch held locally on the processor}
\outarg{integer}{ld(ndim)}{physical dimenstions of the local array patch, including ghost cells}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessGhosts(int dims[], void *ptr, int ld[]) const
void GlobalArray::accessGhosts(int64_t dims[], void *ptr, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{dims[ndim]}{array of dimensions of local patch, including ghost cells}
\outarg{}{ptr}{returns an index corresponding to the origin the global array patch held locally on the processor}
\outarg{}{ld[ndim-1]}{physical dimensions of the local array patch, including ghost cells}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_ghosts(int g_a)
\end{pycode}
\begin{funcargs}
\inarg{g_a}{(int)}{the array handle}
\end{funcargs}
\end{pyapi}
\local

\begin{desc}

  Provides access to the local patch of the global array. Returns
  leading dimension ld and and pointer for the data.  This routine
  will provide access to the ghost cell data residing on each
  processor. Calls to NGA_Access_ghosts should normally follow a call
  to NGA_Distribution that returns coordinates of the visible data
  patch associated with a processor. You need to make sure that the
  coordinates of the patch are valid (test values returned from
  NGA_Distribution).

You can only access local data.
This is a local operation.

\end{desc}

\apih{ACCESS GHOST ELEMENT}{Access a specific ghost element locally allocated on a GA}

\begin{capi}
\begin{ccode}
void NGA_Access_ghost_element(int g_a, void *ptr, int subscript[],
                              int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{index}{index pointing to location of element indexed by subscript[]}
\inarg{}{subscript[ndim]}{array of integers that index desired element}
\outarg{}{ld[ndim-1]}{array of strides for local data patch.  These include ghost cell widths.}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_ghost_element(g_a, index, subscript, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{integer}{index}{index pointing to location of element indexed by subscript()}
\inarg{integer}{subscript(ndim)}{array of integers that index desired element}
\outarg{integer}{ld(ndim-1)}{array of strides for local data patch. These include ghost cell widths.}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessGhostElement(void *ptr, int subscript[],
int ld[]) const
void GlobalArray::accessGhostElement(void *ptr, int64_t subscript[],
int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{ptr}{index pointing to location of element indexed by subscript[]}
\inarg{}{subscript[ndim]}{array of integers that index desired element}
\outarg{}{ld[ndim-1]}{array of strides for local data patch.  These include ghost cell widths.}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_ghost_element(int g_a, subscript, ld)
   g_a (int)                             - the array handle
   subscript (1D array-like of integers) - index of the desired element
\end{pycode}
\end{pyapi}
\local

\begin{desc}

  This function can be used to return a pointer to any data element in
  the locally held portion of the global array and can be used to
  directly access ghost cell data. The array subscript refers to the
  local index of the element relative to the origin of the local patch
  (which is assumed to be indexed by (0,0,...)).

  This is a local operation.

\end{desc}

\apih{TOTAL BLOCKS}{Number of blocks allocated when using block-cyclic distribution}

\begin{capi}
\begin{ccode}
int GA_Total_blocks(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_total_blocks(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::totalBlocks() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
total_blocks(int g_a)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the total number of blocks contained in a global array
with a block-cyclic data distribution. This is a local operation.

\end{desc}

\apih{GET BLOCK INFO}{Information on block-cyclic information for a GA}

\begin{capi}
\begin{ccode}
void GA_Get_block_info(int g_a, int num_blocks[], int block_dims[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{num_blocks[ndim]}{number of blocks along each axis}
\outarg{}{block_dims[ndim]}{dimensions of block}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get_block_info(g_a, num_blocks, block_dims)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{integer}{num_blocks(ndim)}{number of blocks along each axis}
\outarg{integer}{block_dims(ndim)}{dimensions of block}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::getBlockInfo(int num_blocks[], int block_dims[])
\end{cxxcode}
\begin{funcargs}
\outarg{}{num_blocks[ndim]}{array containing number of blocks along each coordinate direction}
\outarg{}{block_dims[ndim]}{array containing block dimensions}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_block_info(int g_a)
   g_a (int)            - the array handle
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This subroutine returns information about the block-cyclic distribution
associated with global array g_a. The number of blocks along each of the
array axes are returned in the array num_blocks and the dimensions of the
individual blocks, specified in the GA_Set_block_cyclic or
GA_Set_block_cyclic_proc_grid subroutines, are returned in block_dims.
This is a local function.

\end{desc}

\apih{DUPLICATE}{Duplicate a GA}

\begin{capi}
\begin{ccode}
int GA_Duplicate(int g_a, char* array_name)
\end{ccode}
\begin{funcargs}
\inarg{}{array_name}{a character string}
\inarg{}{g_a}{integer handle for reference array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_duplicate(g_a, g_b, array_name)
\end{fcode}
\begin{funcargs}
\inarg{character*(*)}{array_name}{a character string}
\inarg{integer}{g_a}{Integer handle for reference array}
\outarg{integer}{g_b}{Integer handle for new array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(const GlobalArray &g_a, char *arrayname)
GlobalArray::GlobalArray(const GlobalArray &g_a)
GlobalArray * GAServices::createGA(const GlobalArray *g_b, char *arrayname)
GlobalArray * GAServices::createGA(const GlobalArray &g_b)
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_b}{integer handle for reference array}
\inarg{}{arrayname}{a character string}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
duplicate(int g_a, char *name='')
   g_a (int)     - the array handle
   name (string) - the new name of the created array
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Creates a new array by applying all the properties of another existing array.
It returns an array handle.

Return value: a non-zero array handle means the call was succesful.
This is a collective operation.

\end{desc}

\apih{DESTROY}{Destroy a global array}

\begin{capi}
\begin{ccode}
void GA_Destroy(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_destroy(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::~GlobalArray()
void GlobalArray::destroy()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
destroy(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Deallocates the array and frees any associated resources.

This is a collective operation.

\end{desc}

\apih{TERMINATE}{Terminate GA}

\begin{capi}
\begin{ccode}
void GA_Terminate()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_terminate()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GA::Terminate()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
terminate()
\end{pycode}
\end{pyapi}
\wcoll

\begin{desc}

Delete all active arrays and destroy internal data structures.

This is a collective operation.

\end{desc}

\apih{SYNC}{Synchronize all processes in the default processor group}

\begin{capi}
\begin{ccode}
void GA_Sync()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_sync()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::sync()
GA::sync()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
sync()
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

Synchronize processes (a barrier) and ensure that all GA operations completed.

This is a collective operation.

\end{desc}

\apih{MASK SYNC}{Mask GA synchronization operations}

\begin{capi}
\begin{ccode}
void GA_Mask_sync(int first,int last)
\end{ccode}
\begin{funcargs}
\inarg{}{first}{mask (0/1) for prior internal synchronization}
\inarg{}{last}{mask (0/1) for post internal synchronization}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_mask_sync(first,last)
\end{fcode}
\begin{funcargs}
\inarg{logical}{first}{mask for prior internal synchronization}
\inarg{logical}{last}{mask for post internal synchronization}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::maskSync(int first, int last)
void GA::maskSync(int first, int last)
\end{cxxcode}
\begin{funcargs}
\inarg{}{first}{masks the sync at the begining of the collective call}
\inarg{}{last}{masks the sync at the end of the collective call}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
mask_sync(int first, int last)
   first (bool)   - mask for prior internal synchronization
   last (bool)    - mask for post internal synchronization
\end{pycode}
\end{pyapi}
\dcoll
\begin{desc}

This subroutine can be used to remove synchronization calls from around collective
operations. Setting the parameter first = .false. removes the synchronization prior
to the collective operation, setting last = .false. removes the synchronization call
after the collective operation. This call is applicable to all collective operations.
It most be invoked before each collective operation.
This is a  collective operation.

\end{desc}

\apih{ZERO}{Zero a global array}

\begin{capi}
\begin{ccode}
void GA_Zero(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_zero(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zero() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
zero(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Sets value of all elements in the array to zero.

This is a collective operation.

\end{desc}

\apih{FILL}{Fill a global array with a specific value}

\begin{capi}
\begin{ccode}
void GA_Fill(int g_a, void *value)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_fill(g_a, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{double precision/complex/integer}{s}{fill value}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::fill(void *value) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type.}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
fill(int g_a, value, lo=None, hi=None)
   g_a (int)                      - the array handle
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Assign a single value to all elements in the array.

This is a collective operation.

\end{desc}

\apih{DOT}{Dot product of two global arrays}

\begin{capi}
\begin{ccode}
int GA_Idot(int g_a, int g_b)
long GA_Ldot(int g_a, int g_b)
float GA_Fdot(int g_a, int g_b)
double GA_Ddot(int g_a, int g_b)
double complex GA_Zdot(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{g_b}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
double precision function ga_ddot(g_a, g_b)
double complex function ga_zdot(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{g_b}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::idot(const GlobalArray * g_a) const
long GlobalArray::ldot(const GlobalArray * g_a) const
float GlobalArray::fdot(const GlobalArray * g_a) const
double GlobalArray::ddot(const GlobalArray * g_a) const
double complex GlobalArray::zdot(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{GlobalArray}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
dot(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None, int ta=False,
int tb=False)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b,
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b,
                                     exclusive
   ta (bool)                       - whether the transpose operator should
                                     be applied to g_a True=applied
   tb (bool)                       - whether the transpose operator should
                                     be applied to g_b True=applied
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the element-wise dot product of the two arrays which must be of the same types and same number of elements.
      Return value = SUM_ij a(i,j)*b(i,j)


This is a collective operation.

\end{desc}

\apih{SCALE}{Scale a global array with a specified value}

\begin{capi}
\begin{ccode}
void GA_Scale(int g_a, void *value)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scale(g_a, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{double precision/complex/integer}{s}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scale(void *value) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{value}{pointer to the value of appropriate type}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scale(int g_a, value, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales an array by the constant s. Note that the library is unable to detect
errors when the pointed value is of a different type than the array.

This is a collective operation.

\end{desc}

\apih{ADD}{Add corresponding values in two global arrays}

\begin{capi}
\begin{ccode}
void GA_Add(void *alpha, int g_a, void* beta, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{g_b}{array handle}
\inarg{}{g_c}{array handle}
\inarg{double/complex/int*}{alpha}{scale factor}
\inarg{double/complex/int*}{beta}{scale factor}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add(alpha, g_a, beta, g_b, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{g_b}{array handle}
\inarg{integer}{g_c}{array handle}
\inarg{double precision/complex/integer}{alpha,beta}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::add(void *alpha, const GlobalArray * g_a, void *beta, const GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{alpha}{scale factor}
\inarg{}{g_a}{array}
\inarg{}{beta}{scale factor}
\inarg{}{g_b}{array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add(int g_a, int g_b, int g_c, alpha=None, beta=None, alo=None, ahi=None,
blo=None, bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   alpha (object)                  - multiplier (converted to appropriate type)
   beta (object)                   - multiplier (converted to appropriate type)
   alo (1D array-like of integers) - lower bound patch coordinates of
                                     g_a, inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of
                                     g_a, exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of
                                     g_b, inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of
                                     g_b, exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of
                                     g_c, inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of
                                     g_c, exclusive
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

The arrays (which must be the same shape and identically aligned)
are added together element-wise.

\begin{verbatim}
        c = alpha * a  +  beta * b;
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).

This is a collective operation.

\end{desc}

\apih{COPY}{Copy a global array}

\begin{capi}
\begin{ccode}
void GA_Copy(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{g_b}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_copy(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{g_b}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::copy(const GlobalArray *g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{GlobalArray to copy}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
copy(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None, int trans=False)
   g_a (int)                       - the array handle copying from
   g_b (int)                       - the array handle copying to
   alo (1D array-like of integers) - lower bound patch coordinates of
                                     g_a, inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of
                                     g_a, exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of
                                     g_b, inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of
                                     g_b, exclusive
   trans (bool)                    - whether the transpose operator should
                                     be applied True=applied
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Copies elements in array represented by g_a into the array represented by g_b.
The arrays must be the same type, shape, and identically aligned.

For patch operations, the patches of arrays may be of different shapes but must
have the same number of elements. Patches must be nonoverlapping (if g_a=g_b).
Transposes are allowed for patch operations.

This is a collective operation.

\end{desc}

\apih{SET MEMORY LIMIT}{Limit the internal memory used by the GA runtime}

\begin{capi}
\begin{ccode}
void GA_Set_memory_limit(size_t limit)
\end{ccode}
\begin{funcargs}
\inarg{}{limit}{the amount of memory in bytes per process}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_set_memory_limit(limit)
\end{fcode}
\begin{funcargs}
\inarg{integer}{limit}{the amount of memory in bytes per process}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setMemoryLimit(size_t limit);
\end{cxxcode}
\begin{funcargs}
\inarg{}{limit}{the amount of memory in bytes per process}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_memory_limit(size_t limit)
   limit (size_t) - the amount of memory in bytes per process
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Sets the amount of memory to be used (in bytes) per process

This is a local operation.

\end{desc}

\seealso{INITIALIZE LTD}

\apih{GET}{Get data from a global array}

\begin{capi}
\begin{ccode}
void NGA_Get(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get(g_a, ilo, ihi, jlo, jhi, buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\outarg{double precision/complex/integer}{buf}{TODO}
\inarg{integer}{ld}{leading dimension}
\end{funcargs}
\end{fapi}

\begin{fapi}
\begin{fcode}
subroutine nga_get(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data goes  to}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\end{funcargs}
\end{fapi}

\begin{pyapi}
\begin{pycode}
get(int g_a, lo=None, hi=None, ndarray buffer=None)
   g_a (int)                      - the array handle
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
   buffer, ,                      - an ndarray of the appropriate type,
                                    large enough to hold lo,hi
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

  Copies data from global array section to the local array buffer. The
  local array is assumed to be have the same number of dimensions as
  the global array. Any detected inconsistencies or errors in the input
  arguments are fatal.

Example: For the ga_get operation transfering data from the [10:14, 0:4]
section of 2-dimensional 15x10 global array into a local buffer 5x10
array we have:

\begin{verbatim}
lo={10,0,} hi={14,4}, ld={10}
\end{verbatim}

Figure \ref{get} shows the GET operation.

\begin{figure}
\centering
\includegraphics{GET}
\caption{Copying Data from 2-dimensional 15x10 Global Array into Local Buffer 5x10 Array}
\label{get}
\end{figure}

This is a one-sided operation.
Return: The local array buffer.

 \end{desc}

\apih{PERIODIC GET}{Get, with periodic boundary conditions, data from a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_get(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_get(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data goes  to}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicGet(int lo[], int hi[], void* buf, int ld[]) const
void GlobalArray::periodicGet(int64_t lo[], int64_t hi[], void* buf,
								  int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
periodic_get(int g_a, lo, hi, buffer, alpha=None)
   g_a (int)                      - the array handle
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (array-like of integers)    - higher bound patch coordinates, exclusive
   buffer (array-like)            - must be contiguous and have same number of
                                    elements as patch
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

Same as nga_get except the indices can extend beyond the array boundary/dimensions
in which case the library wraps them around.
This is a one-sided operation.
The local array is assumed to be have the same number of dimensions as the
global array. Any detected inconsitencies/errors in the input arguments are fatal.

Returns: The local Array buffer.

\end{desc}

\apih{STRIDED GET}{Get strided data from a global array }

\begin{capi}
\begin{ccode}
void NGA_Strided_get(int g_a, int lo[], int hi[], int skip[],
                     void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_get(g_a, lo, hi, skip, buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\inarg{integer}{skip(ndim)}{array of strides for each dimension}
\outarg{type}{buf}{local buffer array where the data comes from}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedGet(int lo[], int hi[], int skip[],
                             void *buf, int ld[]) const
void GlobalArray::stridedGet(int64_t lo[], int64_t hi[], int64_t skip[],
                             void *buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for glob array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\outarg{}{buf}{pointer to local buffer array where data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{cxxapi}
\begin{pyapi}
\begin{pycode}
strided_get(int g_a, lo=None, hi=None, skip=None, ndarray buffer=None)
   g_a (int)                        - the array handle
   lo (1D array-like of integers)   - lower bound patch coordinates, inclusive
   hi (1D array-like of integers)   - higher bound patch coordinates, exclusive
   skip (1D array-like of integers) - strides for each dimension
   buffer (ndarray)                 - an ndarray of the appropriate type,
                                      large enough to hold lo,hi
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

This operation is the same as NGA_Get, except that the values corresponding to
dimension n in buf correspond to every skip[n] values of the global array g_a.
This is a one-sided operation.
The local array is assumed to be have the same number of dimensions as the
global array. Any detected inconsitencies/errors in the input arguments are fatal.

Returns: The local array buffer.
\end{desc}

\apih{PUT}{Put data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Put(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\outarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data is}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_put(g_a, ilo, ihi, jlo, jhi, buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\outarg{double precision/complex/integer}{buf}{TODO}
\inarg{integer}{ld}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_put(g_a, lo, hi, buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data comes from}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::put(int lo[], int hi[], void *buf, int ld[]) const
void GlobalArray::put(int64_t lo[], int64_t hi[], void *buf,
                      int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data is}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
put(int g_a, buffer, lo=None, hi=None)
    g_a (int)                      - the array handle
    buffer (array-like)            - the data to put
    lo (1D array-like of integers) - lower bound patch coordinates, inclusive
    hi (array-like of integers)    - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Copies data from the local array buffer to the global array section. The
local array is assumed to have the same number of dimensions as the global array.
Any detected inconsistencies or errors in input arguments are fatal.

This is a one-sided operation.

\end{desc}

\apih{PERIODIC PUT}{Put, with periodic boundary conditions, data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_put(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\outarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data is}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_put(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data comes from}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicPut(int lo[], int hi[], void* buf, int ld[]) const
void GlobalArray::periodicPut(int64_t lo[], int64_t hi[], void* buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
periodic_put(int g_a, buffer, lo=None, hi=None)
   g_a (int)                      - the array handle
   buffer (array-like)            - the data to put
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (array-like of integers)    - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

Same as nga_put except the indices can extend beyond the array boundary/dimensions
in which case the library wraps them around.
The indices can extend beyond the array boundary/dimensions in which case the
libray wraps them around.
Copies data from local array buffer to the global array section.
The local array is assumed to be have the same number of dimensions as the
global array. Any detected inconsitencies/errors in input arguments are fatal.
This is a one-sided operation.

\end{desc}

\apih{STRIDED PUT}{Put strided data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Strided_put(int g_a, int lo[], int hi[], int skip[],
                     void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_put(g_a, lo, hi, skip, buf, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global}
\inarg{integer}{hi(ndim)}{array of ending indices for global array}
\inarg{integer}{skip(ndim)}{array of strides for each dimension}
\outarg{type}{buf}{local buffer array where the data comes from}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for array section}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedPut(int lo[], int hi[], int skip[],
                             void*buf, int ld[]) const
void GlobalArray::stridedPut(int64_t lo[], int64_t hi[], int64_t skip[],
                             void *buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for glob array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\inarg{}{buf}{pointer to local buffer array where data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
strided_put(int g_a, buffer, lo=None, hi=None, skip=None)
   g_a (int)                        - the array handle
   buffer (array-like)              - the data to put
   lo (1D array-like of integers)   - lower bound patch coordinates, inclusive
   hi (array-like of integers)      - higher bound patch coordinates, exclusive
   skip (1D array-like of integers) - strides for each dimension
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Strided version of put.  This operation is the same as NGA_Put, except
that the values corresponding to dimension n in buf are copied to every
skip[n] values of the global array g_a.

Copies data from local array buffer to the global array section.

The local array is assumed to be have the same number of dimensions as
the global array.

Any detected inconsitencies/errors in input arguments are fatal.

This is a one-sided operation.

\end{desc}

\apih{ACC}{Accumulate data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Acc(int g_a, int lo[], int hi[], void* buf, int ld[],
             void* alpha)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\inarg{}{buf}{pointer to the local buffer array}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{double/double complex/long*}{alpha}{scale factor}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_acc(g_a, ilo, ihi, jlo, jhi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\inarg{double precision/complex}{buf}{TODO}
\inarg{integer}{ld}{}
\inarg{double precision/complex}{alpha}{TODO}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_acc(g_a, lo, hi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the local data is}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\inarg{type}{alpha}{scale argument for accumulate}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::acc(int lo[], int hi[], void *buf,
                      int ld[], void *alpha) const
void GlobalArray::acc(int64_t lo[], int64_t hi[], void *buf,
                      int64_t ld[], void *alpha) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\inarg{}{buf}{pointer to the local buffer array}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{}{alpha}{scale factor (double/double complex/long *)}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
acc(int g_a, buffer, lo=None, hi=None, alpha=None)
   g_a (int)           - the array handle
   buffer (array-like) - must be contiguous and have same number of
                         elements as patch
   lo (1D array-like)  - lower bound patch coordinates, inclusive
   hi (1D array-like)  - higher bound patch coordinates, exclusive
   alpha (object)      - multiplier (converted to appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

 Combines data from local array buffer with data in the global array section.
The local array is assumed to be have the same number of dimensions as the
global array.

If the buffer is not contiguous, a contiguous copy will be made.

    global array section (lo[],hi[]) += *alpha * buffer

This is a one-sided and atomic operation.

\end{desc}

\apih{PERIODIC ACC}{Accumulate, with periodic boundary conditions, data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_acc(int g_a, int lo[], int hi[], void* buf, int ld[],
                      void* alpha)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\inarg{}{buf}{pointer to the local buffer array}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{double/double complex/long*}{alpha}{scale factor}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_acc(g_a, lo, hi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the local data is}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\inarg{type}{alpha}{scale argument for accumulate}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicAcc(int lo[], int hi[], void* buf,
                              int ld[], void* alpha) const
void GlobalArray::periodicAcc(int64_t lo[], int64_t hi[], void* buf,
                              int64_t ld[], void* alpha) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\inarg{}{buf}{pointer to the local buffer array}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{}{alpha}{double/double complex/long scale factor}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
   g_a (int)                      - the array handle
   buffer (array-like)            - must be contiguous and have same
                                    number of elements as patch
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (array-like of integers)    - higher bound patch coordinates, exclusive
   alpha (object)                 - multiplier (converted to the
                                    appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Same as nga_acc except the indices can extend beyond the array boundary/dimensions
in which case the library wraps them around. For Python, this is the periodic
version of ga.acc.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as the
global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a one-sided and atomic operation.

\end{desc}

\apih{STRIDED ACC}{Accumulate strided data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Strided_acc(int g_a, int lo[], int hi[], int skip[], void* buf,
                     int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{double/DoublComplex/long*}{alpha}{scale factor}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_acc(g_a, lo, hi, skip, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo(ndim)}{array of starting indices for global array section}
\inarg{integer}{hi(ndim)}{array of ending indices for global array section}
\inarg{integer}{skip(ndim)}{array of strides for each dimension}
\outarg{type}{buf}{local buffer array where the data comes from}
\inarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}
\inarg{type}{alpha}{scale argument for accumulate}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedAcc(int lo[], int hi[], int skip[], void *buf,
                             int ld[], void *alpha) const;
void GlobalArray::stridedAcc(int64_t lo[], int64_t hi[], int64_t skip[],
                             void *buf, int64_t ld[], void *alpha) const;
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for glob array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{skip[ndim]}{array of strides for each dimension}
\inarg{}{buf}{pointer to local buffer array where data goes}
\inarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{}{alpha}{double/DoublComplex/long scale factor}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
strided_acc(int g_a, buffer, lo=None, hi=None, skip=None, alpha=None)
Parameters:
   g_a (int)                      - the array handle
   buffer (array-like)            - must be contiguous and have same number
                                    of elements as patch
   lo (1D array-like of integers) - lower bound patch coordinates,
                                    inclusive
   hi (1D array-like of integers) - higher bound patch coordinates,
                                    exclusive
   alpha (object)                 - multiplier (converted to the
                                    appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

This operation is the same as NGA_Acc, except that the values corresponding
to dimension n in buf are accumulated to every skip[n] values of the global array g_a.

For Python this is the strided version of ga.acc.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as
the global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a one-sided and atomic operation.


\end{desc}

\apih{DISTRIBUTION}{Inquire data range on a specified processor}

\begin{capi}
\begin{ccode}
void NGA_Distribution(int g_a, int iproc, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{iproc}{process number}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_distribution(g_a, iproc, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{iproc}{process number}
\outarg{integer}{ilo,ihi,jlo,jhi}{range held by process iproc}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_distribution(g_a, iproc, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{iproc}{process number}
\inarg{integer}{ndim}{number of dimensions}
\outarg{integer}{lo(ndim),hi(ndim)}{range held by process iproc}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::distribution(int me, int* lo, int* hi) const
void GlobalArray::distribution(int me, int64_t* lo, int64_t* hi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{iproc}{process number}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
distribution(int g_a, int iproc=-1)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

If no array elements are owned by process iproc, the range is returned as
lo[ ]=0 and hi[ ]= -1 for all dimensions.
This operation is local.


Return the distribution given to iproc. If iproc is not specified, then
ga.nodeid() is used. The range is returned as -1 for lo and -2 for hi if
no elements are owned by iproc.

\end{desc}

\apih{COMPARE DISTR}{Compare distribution of two global arrays}

\begin{capi}
\begin{ccode}
int GA_Compare_distr(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_compare_distr(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::compareDistr(const GlobalArray *g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{GlobalArray to compare against}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
compare_distr(int g_a, int g_b)
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Compares distributions of two global arrays. Returns 0 if distributions
are identical and 1 when they are not.

This is a collective operation.

\end{desc}

\apih{ACCESS}{Access data locally allocated for a global array}

\begin{capi}
\begin{ccode}
void NGA_Access(int g_a, int lo[], int hi[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\outarg{}{ptr}{points to location of first element in patch}
\outarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_access(g_a, ilo, ihi, jlo, jhi, index, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\outarg{integer}{index}{}
\outarg{integer}{ld}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_access(g_a, lo, hi, index, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{lo(ndim),hi(ndim)}{patch specification}
\outarg{integer}{index}{reference to local data}
\outarg{integer}{ld(ndim-1)}{array of leading dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::access(int lo[], int hi[], void *ptr, int ld[]) const
void GlobalArray::access(int64_t lo[], int64_t hi[], void *ptr, int64_t ld[]) cons
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\outarg{}{ptr}{points to location of first element in patch}
\outarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access(int g_a, lo=None, hi=None)
   g_a (int)          - the array handle
   lo (1D array-like) - lower bound patch coordinates, inclusive
   hi (1D array-like) - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Provides access to the specified patch of a global array. Returns array
of leading dimensions ld and a pointer to the first element in the patch.
This routine allows to access directly, in place elements in the local
section of a global array. It useful for writing new GA operations. A call
to ga_access normally follows a previous call to ga_distribution that
returns coordinates of the patch associated with a processor. You need
to make sure that the coordinates of the patch are valid (test values
returned from ga_distribution).

Each call to ga_access has to be followed by a call to either ga_release
or ga_release_update. You can access in this fashion only local data.
Since the data is shared with other processes, you need to consider issues
of mutual exclusion.
This operation is local.

Note: The entire local data is always accessed, but if a smaller patch is
requested, an appropriately sliced ndarray is returned.

Returns: ndarray representing local block

\end{desc}


\begin{fdesc}

  Provides access to the specified patch of array. Returns leading
  dimension ld and and MA-like index for the data. This routine is
  intended for writing new GA operations. Call to ga_access should
  normally follow a call to ga_distribution that returns coordinates
  of the patch associated with a processor. You need to make sure that
  the coordinates of the patch are valid (test values returned from
  ga_distribution).

Your code should include a MA include file, mafdecls.h.
\begin{verbatim}
          dbl_mb(index)  - for double precision data
          int_mb(index)  - for integer data
          dcpl_mb(index) - for double complex data
\end{verbatim}

The addressing convention refers the first element \verb|(ilo,jlo)|
of the patch. However, you can only pass that reference to another
subroutine where it could be used like a normal array, see the
following example. This constraint caused by the HP fortran compiler
inability to reference shared memory data properly. The C interface
has no such restrictions.

Example

For a given subroutine:
\begin{verbatim}
          subroutine foo(A,  nrows, ncols lda)
          double precision A(lda,*)
          integer nrows, ncols
             ....
          end
\end{verbatim}
you can reference A(ilo:ihi,jlo:jhi) in the following way:

\begin{verbatim}
          call foo(dbl_mb(index), ihi-ilo+1, jhi-jlo+1, lda)
\end{verbatim}
\end{fdesc}


\apih{ACCESS BLOCK SEGMENT}{Access local data for a specific global array block}

\begin{capi}
\begin{ccode}
void NGA_Access_block_segment(int g_a, int proc, void *ptr, int len)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{proc}{processor ID}
\outarg{}{ptr}{pointer to locally held data}
\outarg{}{len}{length of data on processor}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block_segment(g_a, proc, index, len)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{proc}{processor ID}
\outarg{integer}{index}{reference to local data}
\outarg{integer}{len}{length of data on processor}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlockSegment(int index, void *ptr, int *len) const
void GlobalArray::accessBlockSegment(int index, void *ptr,
                                     int64_t *len) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index}{processor ID}
\outarg{}{ptr}{points to location of first element}
\outarg{}{len}{length of locally held data}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_block(int g_a, int idx)
Do not use.
   g_a (int)        - the array handle
   proc (int)       - processor ID
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This function can be used to gain access to the all the locally held
data on a particular processor that is associated with a block-cyclic
distributed array. Once the index has been returned, local data can be
accessed as described in the documentation for NGA_Access. The parameter
len is the number of data elements that are held locally. The data
inside this segment has a lot of additional structure so this function
is not generally useful to developers. It is primarily used inside the
GA library to implement other GA routines. Each call to ga_access_block_segment
should be followed by a call to either NGA_Release_block_segment or
NGA_Release_update_block_segment.

This is a local operation.

Returns: ndarray representing local block

\end{desc}

\apih{ACCESS BLOCK}{Access a block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Access_block(int g_a, int idx, int index, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{idx}{block index}
\outarg{}{index}{pointer to locally held block}
\outarg{}{ld[ndim-1]}{array of leading dimensions}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block(g_a, idx, index, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{idx}{block index}
\outarg{integer}{index}{reference to local data}
\outarg{integer}{ld(ndim-1)}{array of leading dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlock(int idx, void *ptr, int ld[]) const
void GlobalArray::accessBlock(int64_t idx, void *ptr, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{idx}{index of block}
\outarg{}{ptr}{points to location of first element in patch}
\outarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
   g_a (int)  - the array handle
   idx (int)  - the block index
\end{pycode}
\end{pyapi}

\local

\begin{desc}

This function can be used to gain direct access to the data represented
by a single block in a global array with a block-cyclic data distribution.
The index idx is the index of the block in the array assuming that blocks
are numbered sequentially in a column-major order. A quick way of determining
whether a block with index idx is held locally on a processor is to calculate
whether mod(idx,nproc) equals the processor ID, where nproc is the total number
of processors. Once the index has been returned, local data can be accessed as
described in the documentation for NGA_Access. Each call to ga_access_block
should be followed by a call to either NGA_Release_block or NGA_Release_update_block.

This is a local operation.

Returns: ndarray representing local block
\end{desc}

\apih{ACCESS BLOCK GRID}{Access data block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Access_block_grid(int g_a, int subscript[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{subscript[ndim]}{subscript of block in array}
\outarg{}{ptr}{pointer to locally held bloc}
\outarg{}{ld[ndim-1]}{array of leading dimensions}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block_grid(g_a, subscript, index, ld)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{subscript(ndim)}{subscript of block in array}
\outarg{integer}{index}{reference to local data}
\outarg{integer}{ld(ndim-1)}{array of leading dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlockGrid(int index[], void *ptr, int ld[]) const
void GlobalArray::accessBlockGrid(int64_t index[], void *ptr, int64_t ld[])
                                  const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index[ndim]}{indices of block in processor grid}
\outarg{}{ptr}{points to location of first element in patch}
\outarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_block_grid(int g_a, subscript)
   g_a (int)                 - the array handle
   subscript (1D array-like) - subscript of the block in the array
\end{pycode}
\end{pyapi}

\local

\begin{desc}

This function can be used to gain direct access to the data represented by
a single block in a global array with a SCALAPACK block-cyclic data distribution
that is based on an underlying processor grid. The subscript array contains the
subscript of the block in the array of blocks. This subscript is based on the
location of the block in a grid, each of whose dimensions is equal to the number
of blocks that fit along that dimension. Once the index has been returned, local
data can be accessed as described in the documentation for NGA_Access. Each call
to ga_access_block_grid should be followed by a call to either NGA_Release_block_grid
or NGA_Release_update_block_grid.

This is a local operation.

Returns: ndarray representing local block

\end{desc}

\apih{RELEASE}{Release access to a global array}

\begin{capi}
\begin{ccode}
void NGA_Release(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_release(g_a, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_release(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{lo(ndim),hi(ndim)}{patch specification}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::release(int lo[], int hi[]) const
void GlobalArray::release(int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to a global array when the data was read only.

Your code should look like:
\begin{verbatim}
        NGA_Distribution(g_a, myproc, lo,hi);
        NGA_Access(g_a, lo, hi, \&ptr, ld);

             <operate on the data referenced by ptr>
        GA_Release(g_a, lo, hi);
\end{verbatim}
NOTE: see restrictions specified for ga_access.

This operation is local.

\end{desc}

\apih{RELEASE UPDATE}{Release access to a global array after an update}

\begin{capi}
\begin{ccode}
void NGA_Release_update(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_release_update(g_a, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo, ihi, jlo, jhi}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{integer}{lo(ndim),hi(ndim)}{patch specification}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdate(int lo[], int hi[]) const
void GlobalArray::releaseUpdate(int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the data. It must be used if the data was accessed for writing.
NOTE: see restrictions specified for ga_access.
This operation is local.
\end{desc}

\apih{RELEASE BLOCK}{Release access to a block of a global array}

\begin{capi}
\begin{ccode}
void NGA_Release_block(int g_a, int index)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{index}{block index}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block(g_a, index)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{index}{block index}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlock(int index) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index}{block index}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block(int g_a, int index)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of data specified by the integer index
when data was accessed as read only. This is only applicable to
block-cyclic data distributions created using the simple block-cyclic
distribution. This is a local operation.

\end{desc}

\apih{RELEASE UPDATE BLOCK}{Release after update access to a block in a global array}

\begin{capi}
\begin{ccode}
void NGA_Release_update_block(int g_a, int index)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{index}{block index}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block(g_a, index)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{index}{block index}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlock(int index) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index}{block index}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block(int g_a, int index)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the block of data specified by the integer index
when data was accessed in read-write mode. This is only applicable
to block-cyclic data distributions created using the simple block-cyclic
distribution. This is a local operation.
\end{desc}

\apih{RELEASE BLOCK GRID}{Release access to a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Release_block_grid(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{subscript[ndim]}{indices of block in array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block_grid(g_a, subscript)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{subscript(ndim)}{indices of block in array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlockGrid(int index[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index[ndim]}{indices of block in array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block_grid(int g_a, subscript)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of data specified by the subscript array
when data was accessed as read only. This is only applicable to
block-cyclic data distributions created using the SCALAPACK data
distribution. This is a local operation.

\end{desc}

\apih{RELEASE UPDATE BLOCK GRID}{Release after update access to a block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Release_update_block_grid(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{subscript[ndim]}{indices of block in array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block_grid(g_a, subscript)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{subscript(ndim)}{indices of block in array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlockGrid(int index[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{index[ndim]}{indices of block in array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block_grid(int g_a, subscript)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the block of data specified by the subscript array
when data was accessed in read-write mode. This is only applicable to
block-cyclic data distributions created using the SCALAPACK data
distribution. This is a local operation.

\end{desc}

\apih{RELEASE BLOCK SEGMENT}{Release access to a block in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_block_segment(int g_a, int iproc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{iproc}{processor ID}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block_segment(g_a, iproc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{iproc}{processor ID}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlockSegment(int proc) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{proc}{process ID/rank}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block_segment(int g_a, int iproc)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the block of locally held data for a block-cyclic
array, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE UPDATE BLOCK SEGMENT}{Release access to a block of a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_block_segment(int g_a, int iproc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{iproc}{processor ID}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block_segment(g_a, iproc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{iproc}{processor ID}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlockSegment(int proc) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{proc}{process ID/rank}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block_segment(int g_a, int iproc)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of locally held data for a block-cyclic
array, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE GHOST ELEMENT}{Release access to ghost cells in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_ghost_element(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{subscript[ndim]}{element subscript}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_ghost_element(g_a, subscript)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{subscript(ndim)}{element subscript}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseGhostElement(int subscript[]) const
void GlobalArray::releaseGhostElement(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{subscript[ndim]}{indices of element}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_ghost_element(int g_a, subscript)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the locally held data for an array with ghost
elements, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE UPDATE GHOST ELEMENT}{Release after update access to ghost cells in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_update_ghost_element(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{subscript[ndim]}{element subscript}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_ghost_element(g_a, subscript)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{subscript(ndim)}{element subscript}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateGhostElement(int subscript[]) const
void GlobalArray::releaseUpdateGhostElement(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{subscript[ndim]}{indices of element}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_ghost_element(int g_a, subscript)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the locally held data for an array with ghost elements,
when data was accessed in read-write mode. This is a local operation.

\end{desc}

\apih{RELEASE GHOSTS}{Release access to ghost cells}

\begin{capi}
\begin{ccode}
void NGA_Release_ghosts(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_ghosts(int g_a)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the locally held block of data containing ghost
elements, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE UPDATE GHOSTS}{Release after access to ghosts}

\begin{capi}
\begin{ccode}
void NGA_Release_update_ghosts(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_ghosts(int g_a)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the locally held block of data containing ghost
elements, when data was accessed in read-write mode.
This is a local operation.

\end{desc}

\apih{READ INC}{Atomically read and increment an element in a global array}

\begin{capi}
\begin{ccode}
long NGA_Read_inc(int g_a, int subscript[], long inc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{subscript[ndim]}{subscript array for the referenced element}
\inarg{}{inc}{amount element is incremented after read}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
integer function ga_read_inc(g_a, i, j, inc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{i, j, inc}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
integer function nga_read_inc(g_a, subscript, inc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{subscript}{(ndim)}{subscript array for the referenced element}
\inarg{inc}{}{amount element is incremented after read}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
long GlobalArray::readInc(int subscript[], long inc)
long GlobalArray::readInc(int64_t subscript[], long inc)
\end{cxxcode}
\begin{funcargs}
\inarg{}{subscript[ndim]}{subscript array for the referenced element}
\inarg{}{inc}{amount element is incremented after read}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
read_inc(int g_a, subscript, long inc=1)
   g_a (int)       - the array handle
   subscript (1D array-like of integers) - index for the referenced element
   inc (long)      - the increment
\end{pycode}
 \end{pyapi}

\ncoll
\begin{desc}

Atomically read and increment an element in an integer array.

\begin{verbatim}
   *BEGIN CRITICAL SECTION*
   old_value = a(subscript)
   a(subscript) += inc
   *END CRITICAL SECTION*
   return old_value
\end{verbatim}

This is a one-sided and atomic operation.

\end{desc}

\apih{SCATTER}{Scatter elements into a global array}

\begin{capi}
\begin{ccode}
void NGA_Scatter(int g_a, void *v, int* subsArray[], int n)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{subsArray[n][ndim]}{array of subscripts for each element}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scatter(g_a, v, i, j, n)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{double precision}{v(n)}{TODO}
\inarg{integer}{i(n), j(n), n}{TODO}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scatter(g_a, v, subsArray, n)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{n}{number of elements}
\inarg{type}{v(n)}{array containing values}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{}{subsArray(ndim,n)}{array of subscripts for each element}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scatter(void *v, int *subsarray[], int n) const
void GlobalArray::scatter(void *v, int64_t *subsarray[], int64_t n) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{subsarray[n][ndim]}{array of subscripts for each element}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scatter(int g_a, values, subsarray)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Scatters array elements into a global array. The contents of the input
arrays (v,subscrArray) are preserved, but their contents might be
(consistently) shuffled on return.

For(k=0; k<= n; k++)\{\ a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]... = v[k];\}\


subsarray will be converted to an ndarray if it is not one already.
A two-dimensional array is allowed so long as its shape is (n,ndim)
where n is the number of elements to gather and ndim is the number
of dimensions of the target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:

for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]

This is a one-sided operation.

\end{desc}

\apih{GATHER}{Gather elements from a global array}

\begin{capi}
\begin{ccode}
void NGA_Gather(int g_a, void *v, int* subsArray[], int n)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{subsArray[n][ndim]}{array of subscripts for each element}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_gather(g_a, v, i, j, n)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\outarg{double precision}{v(n)}{}
\inarg{integer}{i(n), j(n), n}{}
\end{funcargs}
\end{f2dapi}
\begin{fapi}
\begin{fcode}
subroutine nga_gather(g_a, v, subsArray, n)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{n}{number of elements}
\outarg{type}{v(n)}{array containing values}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{}{subsArray(ndim,n)}{array of subscripts for each element}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::gather(void *v, int * subsarray[], int n) const
void GlobalArray::gather(void *v, int64_t * subsarray[], int64_t n) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{subsarray[n][ndim]}{array of subscripts for each element}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
gather(int g_a, subsarray, ndarray values=None)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Gathers array elements from a global array into a local array. The
contents of the input arrays (v, subscrArray) are preserved, but their
contents might be (consistently) shuffled on return.


for (k=0; k<= n; k++)\{\\v[k] = a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]...;\}\

subsarray will be converted to an ndarray if it is not one already.
A two-dimensional array is allowed so long as its shape is (n,ndim)
where n is the number of elements to gather and ndim is the number
of dimensions of the target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:


for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]



This is a one-sided operation.

\end{desc}

\apih{SCATTER ACC}{Scatter accumulate elements into a global array}

\begin{capi}
\begin{ccode}
void NGA_Scatter_acc(int g_a, void *v, int* subsArray[], int n, void *alpha)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{subsArray[n][ndim]}{array of subscripts for each element}
\inarg{}{alpha}{multiplicative factor}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scatter_acc(g_a, v, i, j, n, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{n}{number of elements}
\inarg{type}{v(n)}{array containing value}
\inarg{integer}{i(n),j(n)}{arrays of indices}
\inarg{double precision/complex}{alpha}{multiplicative value}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scatter_acc(g_a, v, subsArray, n, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{n}{number of elements}
\inarg{type}{v(n)}{array containing value}
\inarg{integer}{ndim}{number of array dimensions}
\inarg{}{subsArray(ndim,n)}{array of subscripts}
\inarg{double precision/complex}{alpha}{multiplicative value}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scatterAcc(void *v, int *subsarray[], int n, void *alpha) const
void GlobalArray::scatterAcc(void *v, int64_t *subsarray[], int64_t n, void *alpha) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inarg{}{v[n]}{array containing values}
\inarg{}{subsarray[n][ndim]}{array of subscripts for each element}
\inarg{}{alpha}{multiplicative factor}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scatter_acc(int g_a, values, subsarray, alpha=None)
\end{pycode}
\end{pyapi}

\ncoll

\begin{desc}

Scatters array elements from a local array into a global array. Adds
values from the local array to existing values in the global array
after multiplying by alpha. The contents of the input arrays (v, subscrArray)
are preserved, but their contents might be (consistently) shuffled on return.

for(k=0; k<= n; k++)\{\\v[k] = a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]...;\}\

Like scatter, but adds values to existing values in the global array after
multiplying by alpha.

subsarray will be converted to an ndarray if it is not one already. A
two-dimensional array is allowed so long as its shape is (n,ndim) where n is
the number of elements to gather and ndim is the number of dimensions of the
target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:


for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]

This is a one-sided operation.

\end{desc}

\apih{ERROR}{Abort with an error}

\begin{capi}
\begin{ccode}
void GA_Error(char *message, int code)
\end{ccode}
\begin{funcargs}
\inarg{}{message}{string to print}
\inarg{}{code}{code to print}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_error(message, code)
\end{fcode}
\begin{funcargs}
\inarg{character*1}{message(*)}{}
\inarg{integer}{code}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::error(const char *message, int code)
\end{cxxcode}
\begin{funcargs}
\inarg{}{message}{string to print}
\inarg{}{code}{code to print}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
error(char *message, int code=1)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

To be called in case of an error. Print an error message and an
integer value that represents error code. Releases some system
resources. This is the required way of aborting the program execution.
This operation is local.

\end{desc}

\apih{LOCATE}{Locate the processor containing a specified element of a global array}

\begin{capi}
\begin{ccode}
int NGA_Locate(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{subscript[ndim]}{element subscript}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_locate(g_a, i, j, owner)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{i, j}{element subscript}
\outarg{integer}{owner}{process id}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_locate(g_a, subscript, owner)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{subscript}{element subscript}
\outarg{integer}{owner}{process id}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::locate(int subscript[]) const
int GlobalArray::locate(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{subscript[ndim]}{element subscript}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
locate(int g_a, subscript)
   g_a (int)                             - the array handle
   subscript (1D array-like of integers) - len(subscript) should be ndim
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Return in owner the GA compute process ID that `owns' the data. If any
element of subscript[] is out of bounds ``-1" is returned.
This operation is local.

\end{desc}

\apih{LOCATE REGION}{Locate a region of a global array}

\begin{capi}
\begin{ccode}
int NGA_Locate_region(int g_a, int lo[], int hi[], int map[], int procs[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{ndim}{number of dimensions of the global array}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\outarg{}{map[][2*ndim]}{array with mapping information}
\outarg{}{procs[nproc]}{list of processes that own a part of array section}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_locate_region(g_a, ilo, ihi, jlo, jhi, map, np)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, ilo, ihi, jlo, jhi}{TODO}
\outarg{integer}{map(5,*)}{TODO}
\outarg{integer}{np}{TODO}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_locate_region(g_a, lo, hi, map, proclist, np)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{lo(ndim),hi(ndim)}{region(patch) specifications}
\outarg{integer}{map(2*ndim,*)}{patch ownership array}
\outarg{integer}{proclist(np)}{list of processes}
\outarg{integer}{np}{number of processes}
\inarg{}{map(1:ndim,i)}{contains lower bound dimensions for part owned by process proclist(i)}
\inarg{}{map(ndim+1:2*ndim,i)}{contains upper bound dimensions for part owned by process proclist(i)}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::locateRegion(int lo[], int hi[], int map[], int procs[]) const;
int GlobalArray::locateRegion(int64_t lo[], int64_t hi[], int64_t map[], int procs[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{array of starting indices for array section}
\inarg{}{hi[ndim]}{array of ending indices for array section}
\outarg{}{map[][2*ndim]}{array with mapping information}
\outarg{}{procs[nproc]}{list of processes that own a part of selection}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
locate_region(int g_a, lo, hi)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Return the list of the GA processes ID that `own' the data. Parts of the
specified patch might be actually `owned' by several processes. If lo/hi
are out of bounds ``0" is returned, otherwise the return value is equal to
the number of processes that hold the data.

\begin{verbatim}
     map[i][0:ndim-1]         - lo[i]
     map[i][ndim:2*ndim-1]    - hi[i]
     procs[i]                 - processor id that owns data in patch
                                lo[i]:hi[i]
\end{verbatim}

This operation is local.

\end{desc}

\apih{INQUIRE}{Inquire the type and size of a global array}

\begin{capi}
\begin{ccode}
void NGA_Inquire(int g_a, int *type, int *ndim, int dims[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{type}{data type}
\outarg{}{ndim}{number of dimensions}
\outarg{}{dims}{array of dimensions}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_inquire(g_a, type, dim1, dim2)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\outarg{integer}{type}{}
\outarg{integer}{dim1}{}
\outarg{integer}{dim2}{}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_inquire(g_a, type, ndim, dims)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{integer}{type}{data type id}
\outarg{integer}{ndim}{number of dimensions}
\outarg{integer}{dims(ndim)}{array of dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::inquire(int *type, int *ndim, int dims[]) const
void GlobalArray::inquire(int *type, int *ndim, int64_t dims[]) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{type}{data type}
\outarg{}{ndim}{number of dimensions}
\outarg{}{dims}{array of dimensions}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
 inquire(int g_a)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Returns data type and dimensions of the array.
This operation is local.

\apih{INQUIRE MEMORY}{Inquire the memory used by global arrays on the calling processor}

\begin{capi}
\begin{ccode}
size_t GA_Inquire_memory()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_inquire_memory()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
size_t GAServices::inquireMemory()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
inquire_memory()
\end{pycode}
\end{pyapi}


Returns amount of memory (in bytes) used in the allocated global arrays on
the calling processor.
This operation is local.

\end{desc}

\apih{INQUIRE NAME}{Inquire a global array's name}

\begin{capi}
\begin{ccode}
char* GA_Inquire_name(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_inquire_name(g_a, array_name)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\outarg{character*(*)}{array_name}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
char* GlobalArray::inquireName() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
inquire_name(int g_a)
   g_a (int) - the array handle
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns the name of an array represented by the handle g_a.
This operation is local.

\end{desc}

\apih{NDIM}{Inquire the number of dimensions in a global array}

\begin{capi}
\begin{ccode}
int GA_Ndim(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_ndim(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::ndim() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
ndim(int g_a)
   g_a (int) - the array handle
\end{pycode}
\end{pyapi}


\local

\begin{desc}

Returns the number of dimensions in array represented by the handle g_a.
This operation is local.

\end{desc}

\apih{NBLOCK}{Inquire the number of blocks along each dimension of a global array}

\begin{capi}
\begin{ccode}
void GA_Nblock(int g_a, int nblock[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{nblock[ndim]}{number of partitions for each dimension}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_nblock(g_a, nblock)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{integer}{nblock[ndim]}{number of partitions for each dimension}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nblock(int nblock[]) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{nblock[ndim]}{number of partitions for each dimension}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nblock(int g_a)
   g_a (int)             - array handle
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Given a distribution of an array represented by the handle g_a,
returns the number of partitions of each array dimension. This
operation is local.
\end{desc}

\apih{MEMORY AVAIL}{Inquire the memory available on the invoking processor to allocate global arrays}

\begin{capi}
\begin{ccode}
size_t GA_Memory_avail()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_memory_avail()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::memoryAvailable() ;
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
memory_avail()
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns amount of memory (in bytes) left for allocation of new global
arrays on the calling processor.

Note: If GA_uses_ma returns true, then GA_Memory_avail returns the lesser
of the amount available under the GA limit and the amount available from MA
(according to ma_inquire_avail operation). If no GA limit has been set, it
returns what MA says is available.

If ( ! GA_Uses_ma() \&\ \&\ ! GA_Memory_limited() ) returns $< 0$, indicating
that the bound on currently available memory cannot be determined.
This operation is local.
\end{desc}

\apih{USES MA}{Check whether GA uses MA}

\begin{capi}
\begin{ccode}
int GA_Uses_ma()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_uses_ma()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::usesMA()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
uses_ma()
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns ``1" if memory in global arrays comes from the Memory Allocator (MA).
``0" means that memory comes from another source, for example System V shared
memory is used.
This operation is local.

TODO
\end{desc}

\apih{MEMORY LIMITED}{Check whether memory available to GA's runtime is limited}

\begin{capi}
\begin{ccode}
int GA_Memory_limited()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_memory_limited()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::memoryLimited()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
memory_limited()
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Indicates if limit is set on memory usage in Global Arrays on the calling processor.
``1" means ``yes", ``0" means ``no".
This operation is local.

Returns:
True for "yes", False for "no"
\end{desc}

\apih{PROC TOPOLOGY}{Inquire the linear location of a processor in the processor topology employed by a global array}

\begin{capi}
\begin{ccode}
void NGA_Proc_topology(int g_a, int proc, int coordinates[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{ndim}{number of array dimensions}
\inarg{}{proc}{process id}
\outarg{}{coordinates[ndim]}{coordinates in processor grid}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_proc_topology(g_a, proc, prow, pcol)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{proc}{}
\outarg{integer}{prow, pcol}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::procTopology(int proc, int coord[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{proc}{process id}
\outarg{}{coord[ndim]}{coordinates in processor grid}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
proc_topology(int g_a, int proc)
\end{pycode}
\end{pyapi}


\local

\begin{desc}

Based on the distribution of an array associated with handle g_a,
determines coordinates of the specified processor in the virtual
processor grid corresponding to the distribution of array g_a. The
numbering starts from 0. The values of -1 means that the processor
doesn't ``own" any section of the array represented by g_a.

This operation is local.
\end{desc}

\apih{PRINT FILE}{Print the contents of a global array to a file}

\begin{capi}
\begin{ccode}
void GA_Print_file(FILE *file, int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{file}{file pointer}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printFile(FILE *file) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{file}{file pointer}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_file(int g_a, file)
   file (file-like) - file-like object which must implement fileno(),
                      or a string
   g_a (int)        - the array handle
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Prints an entire array to a file.

This is a collective operation.
\end{desc}

\apih{PRINT PATCH}{Print a patch of a global array to a file}

\begin{capi}
\begin{ccode}
void NGA_Print_patch(int g_a, int lo[],int hi[],int pretty)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{lo[],hi[]}{coordinates of the patch}
\inarg{}{pretty}{formatting flag}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_print_patch(g_a,ilo,ihi,jlo,jhi,pretty)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ilo,ihi,jlo,jhi}{}
\inarg{integer}{pretty}{}
\end{funcargs}
\end{f2dapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printPatch(int* lo, int* hi, int pretty) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{low coordinates of the patch}
\inarg{}{hi}{high coordinates of the patch}
\inarg{}{pretty}{formatting flag}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_patch(int g_a, lo=None, hi=None, int pretty=True)
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Prints a patch of g_a array to the standard output. If the variable
pretty has the value 0 then output is printed in a dense fashion. If
pretty has the value 1 then output is formatted and rows/columns are labeled.

This is a collective operation.
\end{desc}

\apih{PRINT}{Print the contents of a global array}

\begin{capi}
\begin{ccode}
void GA_Print(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::print() const
\end{cxxcode}
\end{cxxapi}


\gcoll

\begin{desc}

Prints an entire array to the standard output.

This is a collective operation.
\end{desc}

\apih{PRINT STATS}{Print GA runtime statistics}

\begin{capi}
\begin{ccode}
void GA_Print_stats()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print_stats()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::printStats()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_stats()
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This non-collective (MIMD) operation prints information about:
\begin{itemize}
    \item Number of calls to the GA create/duplicate, destroy, get,
     put, scatter, gather, and read_and_inc operations
    \item Total amount of data moved in the GA primitive operations
    \item Amount of data moved in GA primitive operations to logicaly
     remote locations
    \item Maximum memory consumption in global arrays, and
    \item Number of requests serviced in the interrupt-driven implementations
     by the calling process.
\end{itemize}

This operation is local.
\end{desc}

\apih{PRINT DISTRIBUTION}{Print the distribution of a global array}

\begin{capi}
\begin{ccode}
void GA_Print_distribution(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print_distribution(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printDistribution() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_distribution(int g_a)
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Prints the array distribution.

This is a collective operation.
\end{desc}

\apih{CHECK HANDLE}{Check whether a GA handle is valid}

\begin{capi}
\begin{ccode}
void GA_Check_handle(int g_a, char* string)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{string}{message string}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_check_handle(g_a, string)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{character(*)*}{string}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::checkHandle(char* string) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{string}{message}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
check_handle(int g_a, char *message)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Check that the global array handle g_a is valid ... if not, call
ga_error with the string provided and some more info.
This operation is local.
\end{desc}

\apih{INIT FENCE}{Initialize tracing of completion of data movement operations}

\begin{capi}
\begin{ccode}
void GA_Init_fence()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_init_fence()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::initFence()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
init_fence()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

Initializes tracing of the completion status of data movement operations.
This operation is local.
\end{desc}

\apih{FENCE}{Fence all GA data movement operations initiated by the calling process}

\begin{capi}
\begin{ccode}
void GA_Fence()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_fence()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::fence()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
fence()
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

Blocks the calling process until all the data transfers corresponding to GA
operations called after ga_init_fence complete. For example, since ga_put
might return before the data reaches the final destination, ga_init_fence
and ga_fence allow the process to wait until the data tranfer is fully completed:
\begin{verbatim}
        ga_init_fence();
        ga_put(g_a, ...);
        ga_fence();
\end{verbatim}

ga_fence must be called after ga_init_fence. A barrier, ga_sync, assures the
completion of all data transfers and implicitly cancels all outstanding
ga_init_fence calls. ga_init_fence and ga_fence must be used in pairs, multiple
calls to ga_fence require the same number of corresponding ga_init_fence calls.
ga_init_fence/ga_fence pairs can be nested.

ga_fence works for multiple GA operations. For example:
\begin{verbatim}
        ga_init_fence();
        ga_put(g_a, ...);
        ga_scatter(g_a, ...);
        ga_put(g_b, ...);
        ga_fence();
\end{verbatim}

The calling process will be blocked until data movements initiated by two calls
to ga_put and one ga_scatter complete.
\end{desc}

\apih{CREATE MUTEXES}{Create mutexes}

\begin{capi}
\begin{ccode}
int GA_Create_mutexes(int number)
\end{ccode}
\begin{funcargs}
\inarg{}{number}{number of mutexes in mutex array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_create_mutexes(number)
\end{fcode}
\begin{funcargs}
\inarg{integer}{number}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::createMutexes(int number)
\end{cxxcode}
\begin{funcargs}
\inarg{}{number}{of mutexes in mutex array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_mutexes(int number)
   number (int) - the number of mutexes to create
\end{pycode}
\end{pyapi}

\wcoll
\begin{desc}

Creates a set containing the number of mutexes. Returns 0 if the operation
succeeded or 1 if it has failed. Mutex is a simple synchronization object
used to protect Critical Sections. Only one set of mutexes can exist at a
time. An array of mutexes can be created and destroyed as many times as needed.

Mutexes are numbered: 0, ..., number-1.

This is a collective operation.

Returns:
True on success, False on failure

\end{desc}

\apih{DESTROY MUTEXES}{Destroy mutexes}

\begin{capi}
\begin{ccode}
int GA_Destroy_mutexes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_destroy_mutexes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::destroyMutexes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
destroy_mutexes()
\end{pycode}
\end{pyapi}

\wcoll
\begin{desc}

Destroys the set of mutexes created with ga_create_mutexes. Returns 0 if
the operation succeeded or 1 when failed.

This is a collective operation.
\end{desc}

\apih{LOCK}{Lock a specific mutex}

\begin{capi}
\begin{ccode}
void GA_Lock(int mutex)
\end{ccode}
\begin{funcargs}
\inarg{}{mutex}{mutex object id}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_lock(mutex)
\end{fcode}
\begin{funcargs}
\inarg{integer}{mutex}{}
\end{funcargs}
   ! mutex id
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::lock(int mutex)
\end{cxxcode}
\begin{funcargs}
\inarg{}{mutex}{mutex object id}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
lock(int mutex)
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

Locks a mutex object identified by the mutex number. It is a fatal
error for a process to attempt to lock a mutex which was already
locked by this process.
\end{desc}

\apih{UNLOCK}{Unlock a mutex}

\begin{capi}
\begin{ccode}
void GA_Unlock(int mutex)
\end{ccode}
\begin{funcargs}
\inarg{}{mutex}{mutex object id}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_unlock(mutex)
\end{fcode}
\begin{funcargs}
\inarg{integer}{mutex}{}
\end{funcargs}
   ! mutex id
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::unlock(int mutex)
\end{cxxcode}
\begin{funcargs}
\inarg{}{mutex}{mutex object id}
\end{funcargs}
\end{cxxapi}
\ncoll

\begin{desc}

Unlocks a mutex object identified by the mutex number. It is a fatal
error for a process to attempt to unlock a mutex which has not been
locked by this process.
\end{desc}

\apih{NODEID}{The GA rank of the invoking process}

\begin{capi}
\begin{ccode}
int GA_Nodeid()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_nodeid()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::nodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nodeid()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

Returns the GA process id (0, ..., ga_Nnodes()-1) of the requesting compute process.
This operation is local.
\end{desc}

\apih{NNODES}{Total number of GA ranks}

\begin{capi}
\begin{ccode}
int GA_Nnodes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_nnodes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::nodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nnodes()
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns the number of the GA compute (user) processes.
This operation is local.
\end{desc}

\apih{GEMM}{Matrix multiplication of global arrays}

\begin{capi}
\begin{ccode}
void GA_Dgemm(char ta, char tb, int m, int n, int k, double alpha,
              int g_a, int g_b, double beta, int g_c)
void GA_Sgemm(char ta, char tb, int m, int n, int k, float alpha,
              int g_a, int g_b, float beta, int g_c)
void GA_Zgemm(char ta, char tb, int m, int n, int k, double complex alpha,
              int g_a, int g_b, double complex beta, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{, g_b, }{handles to input arrays}
\outarg{}{g_c}{handles to output array}
\inarg{}{ta, tb}{transpose operators}
\inarg{}{m}{number of rows of op(A) and of matrix  C}
\inarg{}{n}{number of columns of op(B) and of matrix  C}
\inarg{}{k}{number of columns of op(A) and rows of matrix op(B)}
\inarg{}{alpha, beta}{scale factors}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine GA_Dgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c)
subroutine GA_Sgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c)
subroutine GA_Zgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{handles to input arrays}
\outarg{integer}{g_c}{handle to output array}
\inarg{character(1)}{ta, tb}{transpose operators}
\inarg{integer}{m}{number of rows of op(A) and of matrix  C}
\inarg{integer}{n}{number of columns of op(B) and of matrix  C}
\inarg{integer}{k}{number of columns of op(A) and rows of matrix op(B)}
\inarg{double precision/double complex/real}{alpha, beta}{scale factors}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::dgemm(char ta, char tb, int m, int n, int k,
                        double alpha, const GlobalArray *g_a, const
                        GlobalArray *g_b, double beta) const
void GlobalArray::dgemm(char ta, char tb, int64_t m, int64_t n, int64_t k,
                        double alpha, const GlobalArray *g_a, const
                        GlobalArray *g_b, double beta) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{ta}{transpose operators}
\inarg{}{tb}{transpose operators}
\inarg{}{m}{number of rows of op(A) and of matrix C}
\inarg{}{n}{number of columns of op(B) and of matrix C}
\inarg{}{k}{number of columns of op(A) and rows of matrix op(B)}
\inarg{}{alpha}{scale factors}
\inarg{}{g_a}{input arrays}
\inarg{}{g_b}{input arrays}
\inarg{}{beta}{scale factors}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
gemm(int ta, int tb, int64_t m, int64_t n, int64_t k, alpha, int g_a,
int g_b, beta, int g_c)
   ta (bool)       - transpose operator
   tb (bool)       - transpose operator
   m (int)         - number of rows of op(A) and of matrix C
   n (int)         - number of columns of op(B) and of matrix C
   k (int)         - number of columns of op(A) and rows of matrix op(B)
   alpha (object)  - scale factor
   g_a (int)       - handle to input array
   g_b (int)       - handle to input array
   beta (object)   - scale factor
   g_c (int)       - handle to output array
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Performs one of the matrix-matrix operations:
\[
      C := alpha*op( A )*op( B ) + beta*C,
\]

where op( X ) is one of
\begin{verbatim}
      op( X ) = X   or   op( X ) = X',
\end{verbatim}

alpha and beta are scalars, and A, B, and C are matrices, with op( A )
an m by k matrix, op( B ) a k by n matrix and C an m by n matrix.

On entry, transa specifies the form of op( A ) to be used in the matrix
multiplication as follows:
\begin{verbatim}
           ta = `N' or `n', op( A ) = A.
           ta = `T' or `t', op( A ) = A'.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{COPY PATCH}{Copy a patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Copy_patch(char trans, int g_a, int alo[], int ahi[],
                    int g_b, int blo[], int bhi[])
\end{ccode}
\begin{funcargs}
\inarg{}{trans}{transpose operator}
\inarg{}{g_a, g_b}{array handles}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_copy_patch(trans, g_a, ailo, aihi, ajlo, ajhi,
                         g_b, bilo, bihi, bjlo, bjhi)
\end{fcode}
\begin{funcargs}
\inarg{character}{trans}{transpose operator}
\inarg{integer}{g_a, g_b}{}
\inarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}
\inarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_copy_patch(trans, g_a, alo, ahi, g_b, blo, bhi)
\end{fcode}
\begin{funcargs}
\inarg{character}{trans}{transpose operator}
\inarg{integer}{g_a, g_b}{}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::copyPatch(char trans, const GlobalArray* ga, int alo[],
                            int ahi[], int blo[], int bhi[]) const
void GlobalArray::copyPatch(char trans, const GlobalArray* ga, int64_talo[],
                            int64_t ahi[], int64_t blo[], int64_t bhi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{trans}{use transpose operator}
\inarg{}{ga}{global array}
\inarg{}{alo}{ga patch coordinates}
\inarg{}{ahi}{ga patch coordinates}
\inarg{}{blo}{this GlobalArray's patch coordinates}
\inarg{}{bhi}{this GlobalArray's patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Copies elements in a patch of one array into another one. The patches of
arrays may be of different shapes but must have the same number of elements.
Patches must be non-overlapping (if g_a=g_b).
\begin{verbatim}
    trans = `N' or `n' means that the transpose operator should
             not be applied.
    trans = `T' or `t' means that transpose operator should be applied.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{DOT PATCH}{Dot product of patches of global arrays}

\begin{capi}
\begin{ccode}
double NGA_Ddot_patch (int g_a, char ta, int alo[], int ahi[],
                       int g_b, char tb, int blo[], int bhi[])
long   NGA_Idot_patch (int g_a, char ta, int alo[], int ahi[],
                       int g_b, char tb, int blo[], int bhi[])
double complex NGA_Zdot_patch (int g_a, char ta, int alo[], int ahi[],
                              int g_b, char tb, int blo[], int bhi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\inarg{}{ta, tb}{transpose flags}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
double precision function ga_ddot_patch (g_a, ta, ailo, aihi, ajlo, ajhi,
                                         g_b, tb, bilo, bihi, bjlo, bjhi)
double complex function ga_zdot_patch (g_a, ta, ailo, aihi, ajlo, ajhi,
                                       g_b, tb, bilo, bihi, bjlo, bjhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{}
\inarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}
\inarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}
\inarg{character*1}{ta, tb}{transpose flags}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
double precision function nga_ddot_patch (g_a, ta, alo, ahi,
                                          g_b, tb, bio, bhi)
double complex function nga_zdot_patch (g_a, ta, alo, ahi,
                                        g_b, tb, blo, bhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{}
\funcarg{integer}{ndim}{number of dimensions}{NA}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}
\inarg{character*1}{ta, tb}{transpose flags}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
double GlobalArray::ddotPatch(char ta, int alo[], int ahi[],
                              const GlobalArray * g_a, char tb, int blo[],
                              int bhi[]) const
double GlobalArray::ddotPatch(char ta, int64_t alo[], int64_t ahi[],
                              const GlobalArray * g_a, char tb,
                              int64_t blo[], int64_t bhi[]) const
float GlobalArray::fdotPatch(char ta, int alo[], int ahi[],
                             const GlobalArray * g_a, char tb, int blo[],
                             int bhi[]) const
float GlobalArray::fdotPatch(char ta, int64_t alo[], int64_t ahi[],
                             const GlobalArray * g_a, char tb, int64_t blo[],
                             int64_t bhi[]) const
double complex GlobalArray::zdotPatch(char ta, int alo[], int ahi[],
                                     const GlobalArray * g_a, char tb,
                                     int blo[], int bhi[]) const
double complex GlobalArray::zdotPatch(char ta, int64_t alo[], int64_t ahi[],
                                     const GlobalArray * g_a, char tb,
                                     int64_t blo[], int64_t bhi[]) const
long GlobalArray::idotPatch(char ta, int alo[], int ahi[],
                           const GlobalArray * g_a, char tb, int blo[],
                           int bhi[]) const
long GlobalArray::idotPatch(char ta, int64_t alo[], int64_t ahi[],
                            const GlobalArray * g_a, char tb, int64_t blo[],
                            int64_t bhi[]) const
long GlobalArray::ldotPatch(char ta, int alo[], int ahi[],
                            const GlobalArray * g_a, char tb, int blo[],
                            int bhi[]) const
long GlobalArray::ldotPatch(char ta, int64_t alo[], int64_t ahi[],
                            const GlobalArray * g_a, char tb, int64_t blo[],
                            int64_t bhi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{ta}{transpose flags}
\inarg{}{alo}{g_a patch coordinates}
\inarg{}{ahi}{g_a patch coordinates}
\inarg{}{g_a}{global array}
\inarg{}{tb}{transpose flags}
\inarg{}{blo}{g_b patch coordinates}
\inarg{}{bhi}{g_b patch coordinates}
\end{funcargs}
\end{cxxapi}

\gcoll

\begin{desc}

Computes the element-wise dot product of the two (possibly transposed)
patches which must be of the same type and have the same number of elements.

This is a collective operation.
\end{desc}

\apih{MATMUL PATCH}{Matrix multiplication of patches of global arrays}

\begin{capi}
\begin{ccode}
void GA_Matmul_patch (char transa, char transb, void* alpha, void *beta,
                      int g_a, int ailo, int aihi, int ajlo, int ajhi,
                      int g_b, int bilo, int bihi, int bjlo, int bjhi,
                      int g_c, int cilo, int cihi, int cjlo, int cjhi)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{, g_b, g_c}{array handles}
\inarg{ailo}{, aihi, ajlo, ajhi}{patch of g_a}
\inarg{bilo}{, bihi, bjlo, bjhi}{patch of g_b}
\inarg{cilo}{, cihi, cjlo, cjhi}{patch of g_c}
\inarg{alpha}{, beta}{scale factors}
\inarg{transa}{, transb}{transpose operators}
\end{funcargs}
\end{capi}

\begin{capi}
\begin{ccode}
void NGA_Matmul_patch(char transa, char transb, void* alpha, void *beta,
                      int g_a, int alo[], int ahi[],
                      int g_b, int blo[], int bhi[],
                      int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\inarg{g_a}{, g_b, g_c}{array handles}
\inarg{alo}{, ahi}{patch of g_a}
\inarg{blo}{, bhi}{patch of g_b}
\inarg{clo}{, chi}{patch of g_c}
\inarg{alpha}{, beta}{scale factors}
\inarg{transa}{, transb}{transpose operators}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_matmul_patch (transa, transb, alpha, beta,
                            g_a, ailo, aihi, ajlo, ajhi,
                            g_b, bilo, bihi, bjlo, bjhi,
                            g_c, cilo, cihi, cjlo, cjhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, ailo, aihi, ajlo, ajhi}{patch of g_a}
\inarg{integer}{g_b, bilo, bihi, bjlo, bjhi}{patch of g_b}
\inarg{integer}{g_c, cilo, cihi, cjlo, cjhi}{patch of g_c}
\inarg{double precision/complex}{alpha, beta}{TODO}
\inarg{character*1}{transa, transb}{TODO}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_matmul_patch(transa, transb, alpha, beta,
                            g_a, alo, ahi,
                            g_b, blo, bhi,
                            g_c, clo, chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, alo, ahi}{patch of g_a}
\inarg{integer}{g_b, blo, bhi}{patch of g_b}
\inarg{integer}{g_c, clo, chi}{patch of g_c}
\inarg{double precision/complex}{alpha, beta}{TODO}
\inarg{character*1}{transa, transb}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::matmulPatch(char transa, char transb,
                              void* alpha, void *beta, const GlobalArray *g_a,
                              int ailo, int aihi, int ajlo, int ajhi,
                              const GlobalArray *g_b, int bilo, int bihi,
                              int bjlo, int bjhi, int cilo, int cihi,
                              int cjlo, int cjhi) const;
void GlobalArray::matmulPatch(char transa, char transb,
                              void* alpha, void *beta, const GlobalArray *g_a,
                              int64_t ailo, int64_t aihi, int64_t ajlo,
                              int64_t ajhi, const GlobalArray *g_b, int64_t
                              bilo, int64_t bihi, int64_t bjlo, int64_t bjhi,
                              int64_t cilo, int64_t cihi, int64_t cjlo,
                              int64_t cjhi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{transa}{transpose operators}
\inarg{}{transb}{transpose operators}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{ailo}{patch of g_a}
\inarg{}{aihi}{patch of g_a}
\inarg{}{ajlo}{patch of g_a}
\inarg{}{ajhi}{patch of g_a}
\inarg{}{bilo}{patch of g_b}
\inarg{}{bihi}{patch of g_b}
\inarg{}{bjlo}{patch of g_b}
\inarg{}{bjhi}{patch of g_b}
\inarg{}{cilo}{patch of g_c}
\inarg{}{cihi}{patch of g_c}
\inarg{}{cjlo}{patch of g_c}
\inarg{}{cjhi}{patch of g_c}
\inarg{}{alpha}{scale factors}
\inarg{}{beta}{scale factors}
\end{funcargs}
\end{cxxapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::matmulPatch(char transa, char transb, void* alpha,
                              void *beta,const GlobalArray *g_a,
                              int *alo, int *ahi, const GlobalArray *g_b,
                              int *blo, int *bhi, int *clo, int *chi) const
void GlobalArray::matmulPatch(char transa, char transb, void* alpha,
                              void *beta, const GlobalArray *g_a,
                              int64_t *alo, int64_t *ahi, const GlobalArray
                              *g_b, int64_t *blo, int64_t *bhi,
                              int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{alo}{array of patch of g_a}
\inarg{}{ahi}{array of patch of g_a}
\inarg{}{blo}{array of patch of g_b}
\inarg{}{bhi}{array of patch of g_b}
\inarg{}{clo}{array of patch of g_c}
\inarg{}{chi}{array of patch of g_c}
\inarg{}{alpha}{scale factors}
\inarg{}{beta}{scale factors}
\inarg{}{transa}{transpose operators}
\inarg{}{transb}{transpose operators}
\end{funcargs}
\end{cxxapi}


\begin{pyapi}
\begin{pycode}
matmul_patch(int transa, int transb, alpha, beta, int g_a, alo, ahi,
int g_b, blo, bhi, int g_c, clo, chi)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

ga_matmul_patch is a patch version of ga_dgemm and comes in 2-D and N-D
versions. The 2-D interface performs the operation:
\begin{verbatim}
         C[cilo:cihi,cjlo:cjhi] := alpha* AA[ailo:aihi,ajlo:ajhi] *
                                   BB[bilo:bihi,bjlo:bjhi] ) +
                                   beta*C[cilo:cihi,cjlo:cjhi],
\end{verbatim}

where AA = op(A), BB = op(B), and op(X) is one of
\begin{verbatim}
      op(X) = X   or   op(X) = X',
\end{verbatim}

Valid values for transpose arguments: `n', `N', `t', `T'. It works for both
double and double complex data tape.

nga_matmul_patch is a N-dimensional patch version of ga_dgemm and is similar to
the 2-D interface:
\begin{verbatim}
      C[clo[]:chi[]] := alpha* AA[alo[]:ahi[]] *
                               BB[blo[]:bhi[]] ) + beta*C[clo[]:chi[]],
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{ADD PATCH}{Add patches of global arrays}

\begin{capi}
\begin{ccode}
void NGA_Add_patch(void *alpha, int g_a, int alo[], int ahi[],
                   void *beta,  int g_b, int blo[], int bhi[],
                   int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\inarg{g_a}{, g_b, g_c}{array handles}
\inarg{}{alo[], ahi[]}{patch of g_a}
\inarg{}{blo[], bhi[]}{patch of g_b}
\inarg{}{clo[], chi[]}{patch of g_c}
\inarg{alpha}{, beta}{scale factors}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_add_patch(alpha, g_a, ailo, aihi, ajlo, ajhi,
                        beta,  g_b, bilo, bihi, bjlo, bjhi,
                               g_c, cilo, cihi, cjlo, cjhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b, g_c}{}
\inarg{double precision/complex/integer}{alpha, beta}{TODO}
\inarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}
\inarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}
\inarg{integer}{cilo, cihi, cjlo, cjhi}{g_c patch coordinates}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_add_patch(alpha, g_a, alo, ahi, beta, g_b, blo, bhi
                         g_c, clo, chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b, g_c}{}
\inarg{double precision/complex/integer}{alpha,beta}{TODO}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}
\inarg{integer}{clo(ndim), chi(ndim)}{g_c patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addPatch(void *alpha, const GlobalArray * g_a, int alo[],
                           int ahi[],void *beta, const GlobalArray * g_b,
                           int blo[], int bhi[], int clo[], int chi[]) const
void GlobalArray::addPatch(void *alpha, const GlobalArray * g_a, int64_t alo[],
                           int64_t ahi[], void *beta, const GlobalArray * g_b,
                           int64_t blo[], int64_t bhi[], int64_t clo[],
                           int64_t chi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{alpha}{scale factor}
\inarg{}{g_a}{global array}
\inarg{}{alo}{patch of g_a}
\inarg{}{ahi}{patch of g_a}
\inarg{}{beta}{scale factor}
\inarg{}{g_b}{global array}
\inarg{}{blo}{patch of g_b}
\inarg{}{bhi}{patch of g_b}
\inarg{}{clo}{patch of this GlobalArray}
\inarg{}{chi}{patch of this GlobalArray}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Patches of arrays (which must have the same number of elements) are added
together element-wise.
\begin{verbatim}
c[ ][ ] = alpha * a[ ][ ] + beta * b[ ][ ]
\end{verbatim}
This is a collective operation.
\end{desc}

\apih{FILL PATCH}{Fill a patch of a global array with a specified value}

\begin{capi}
\begin{ccode}
void NGA_Fill_patch(int g_a, int lo[], int hi[], void *val)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handles}
\inarg{}{lo[], hi[]}{patch of g_a}
\inarg{}{val}{value to fill}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_fill_patch(g_a, ailo, aihi, ajlo, ajhi, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{double precision/complex/integer}{s}{TODO}
\inarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_fill_patch (g_a, alo, ahi, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{double precision/complex/integer}{s}{TODO}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::fillPatch (int lo[], int hi[], void *val) const
void GlobalArray::fillPatch (int64_t lo[], int64_t hi[], void *val) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{patch of this GlobalArray}
\inarg{}{hi}{patch of this GlobalArray}
\inarg{}{val}{value to fill}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Fill the patch of g_a with value of `val'

This is a collective operation.
\end{desc}

\apih{ZERO PATCH}{Zero a patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Zero_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handles}
\inarg{}{lo[], hi[]}{patch of g_a}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_zero_patch(g_a, alo, ahi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zeroPatch (int lo[], int hi[]) const
void GlobalArray::zeroPatch (int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{patch of this GlobalArray}
\inarg{}{hi}{patch of this GlobalArray}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}


Set all the elements in the patch to zero.

This is a collective operation.

\end{desc}

\apih{SCALE PATCH}{Scale elements in the patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Scale_patch(int g_a, int lo[], int hi[], void *val)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handles}
\inarg{}{lo[], hi[]}{patch of g_a}
\inarg{}{val}{scale factor}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scale_patch(g_a, ailo, aihi, ajlo, ajhi, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{double precision/complex/integer}{s}{TODO}
\inarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scale_patch(g_a, alo, ahi, s)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{}
\inarg{double precision/complex/integer}{s}{TODO}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scalePatch (int lo[], int hi[], void *val) const;
void GlobalArray::scalePatch (int64_t lo[], int64_t hi[], void *val) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{patch of this GlobalArray}
\inarg{}{hi}{patch of this GlobalArray}
\inarg{}{val}{scale factor}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Scale an array by the factor `val'

This is a collective operation.
\end{desc}

\apih{BRDCST}{Broadcast elements among all processes}

\begin{capi}
\begin{ccode}
void GA_Brdcst(void *buf, int lenbuf, int root)
\end{ccode}
\begin{funcargs}
\inarg{}{lenbuf}{length of buffer (bytes)}
\inoutarg{}{buf[lenbuf]}{data}
\inarg{}{root}{root process}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_brdcst(type, buf, lenbuf, root)
\end{fcode}
\begin{funcargs}
\inarg{integer}{type}{}
\inoutarg{byte}{buf(lenbuf)}{TODO}
\inarg{integer}{lenbuf}{}
\inarg{integer}{root}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::brdcst(void *buf, int lenbuf, int root)
\end{cxxcode}
\begin{funcargs}
\inarg{}{lenbuf}{length of buffer}
\inoutarg{}{buf[lenbuf]}{data}
\inarg{}{root}{root process}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
brdcst(buffer, int root)
   buffer (1D array-like of objects) - the ndarray message
                                       (converted to the appropriate type)
   root (int)                        - the process which is sending
\end{pycode}
\end{pyapi}

\wcoll
\begin{desc}

Broadcast from process root to all other processes a message of length lenbuf.

This is operation is provided only for convenience purposes: it is available
regardless of the message-passing library that GA is running.

If the buffer is not contiguous, an error is raised. This operation is provided
only for convenience purposes: it is available regardless of the message-passing
library that GA is running with.


This is a collective operation.

Returns:
The buffer in case a temporary was passed in.
\end{desc}

\apih{DGOP}{Global operation of double precision elements among all processes}

\begin{capi}
\begin{ccode}
void GA_Dgop(double x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_dgop(type, x, n, op)
\end{fcode}
\begin{funcargs}
\inarg{integer}{type}{}
\inoutarg{double precision}{x(n)}{TODO}
\inarg{character*(*)}{op}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::dgop(double x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}


\wcoll
\begin{desc}

Double Global OPeration.

$X(1:N)$ is a vector present on each process. DGOP `sums' elements of
X accross all nodes using the commutative operator OP. The result is
broadcast to all nodes. Supported operations include `+', `*', `max',
`min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is operation is provided only for convenience purposes: it is available
regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{IGOP}{Inter global operation among all processes}

\begin{capi}
\begin{ccode}
void GA_Igop(long x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_igop(type, x, n, op)
\end{fcode}
\begin{funcargs}
\inarg{integer}{type}{}
\inoutarg{integer}{x(n)}{TODO}
\inarg{character*(*)}{op}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::igop(int x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\wcoll
\begin{desc}

Integer Global OPeration. The integer (more precisely long) version
of ga_dgop described above, also includes the bitwise OR operation.

This is operation is provided only for convenience purposes: it is
available regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{LGOP}{Long global operation among all processes}

\begin{capi}
\begin{ccode}
void GA_Lgop(long x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::lgop(long x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\wcoll

\begin{desc}

Long Global OPeration. The long version of ga_dgop described above,
also includes the bitwise OR operation.

This is operation is provided only for convenience purposes: it is
available regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{CLUSTER NNODES}{Total number of cluster (shared memory) nodes}

\begin{capi}
\begin{ccode}
int GA_Cluster_nnodes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nnodes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNnodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nnodes()
\end{pycode}
\end{pyapi}
\local
\begin{desc}


This functions returns the total number of nodes that the program is running
on. On SMP architectures, this will be less than or equal to the total number
of processors.

This is a  local operation.
\end{desc}

\apih{CLUSTER NODEID}{Cluster node Rank of the invoking process}

\begin{capi}
\begin{ccode}
int GA_Cluster_nodeid()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nodeid()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nodeid(int proc=-1)
   proc (int)    - process ID to lookup
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the node ID of the process. On SMP architectures with more
than one processor per node, several processes may return the same node id.

This is a  local operation.
\end{desc}

\apih{CLUSTER PROC NODEID}{Cluster node rank of a specified process}

\begin{capi}
\begin{ccode}
int GA_Cluster_proc_nodeid(int proc)
\end{ccode}
\begin{funcargs}
\inarg{}{proc}{process id}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_proc_nodeid(proc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{proc}{process id}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterProcNodeid(int iproc)
\end{cxxcode}
\begin{funcargs}
\inarg{}{iproc}{process id}
\end{funcargs}
\end{cxxapi}
\local

\begin{desc}

This function returns the node ID of the specified process proc.
On SMP architectures with more than one processor per node, several
processes may return the same node id.

This is a  local operation.
\end{desc}

\apih{CLUSTER NPROCS}{Number of processes in a given cluster node}

\begin{capi}
\begin{ccode}
int GA_Cluster_nprocs(int inode)
\end{ccode}
\begin{funcargs}
\inarg{}{inode}{node id}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nprocs(inode)
\end{fcode}
\begin{funcargs}
\inarg{integer}{inode}{node id}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNprocs(int inode)
\end{cxxcode}
\begin{funcargs}
\inarg{}{inode}{node id}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nprocs(int inode)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the number of processors available on node inode.

This is a local operation.
\end{desc}

\apih{CLUSTER PROCID}{Rank of a process from a cluster node rank and intra-node rank}

\begin{capi}
\begin{ccode}
int GA_Cluster_procid(int inode, int iproc)
\end{ccode}
\begin{funcargs}
\inarg{}{inode}{node id}
\inarg{}{iproc}{processor id}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_procid(inode,iproc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{inode}{node id}
\inarg{integer}{iproc}{processor id}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterProcid(int inode, int iproc)
\end{cxxcode}
\begin{funcargs}
\inarg{}{inode}{node id}
\inarg{}{iproc}{processor id}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_procid(int inode, int iproc)
\end{pycode}
\end{pyapi}

\local

\begin{desc}

This function returns the processor id associated with node inode and
the local processor ID iproc. If node inode has N processors, then the
value of iproc lies between 0 and N-1.

This is a local operation.
\end{desc}

\apih{DIAG}{Diagonalize a global array}

\begin{capi}
\begin{ccode}
void GA_Diag(int g_a, int g_s, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{Matrix to diagonalize}
\inarg{}{g_s}{Metric}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag(g_a, g_s, g_v, eval)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{Matrix to diagonalize}
\inarg{integer}{g_s}{Metric}
\outarg{integer}{g_v}{Global matrix to return evecs}
\outarg{double precision}{eval(*)}{Local array to return evals}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diag(const GlobalArray *g_s, GlobalArray *g_v, void *eval)
const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_s}{Matrix to diagonalize}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag(int g_a, int g_s, int g_v, evalues=None)
   g_a (int)     - the array handle of the matrix to diagonalize
   g_s (int)     - the array handle of the metric
   g_v (int)     - the array handle to return evecs
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Solve the generalized eigenvalue problem returning all eigenvectors and
values in ascending order. The input matrices are not overwritten or destroyed.

Returns:
All eigen-values as an ndarray in ascending order.

This is a collective operation.
\end{desc}

\apih{DIAG REUSE}{Diagonalize a global array for repeated diagonalizations}

\begin{capi}
\begin{ccode}
void GA_Diag_reuse(int control, int g_a, int g_s, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\inarg{}{control}{Control flag}
\inarg{}{g_a}{Matrix to diagonalize}
\inarg{}{g_s}{Metric}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag_reuse(control, g_a, g_s, g_v, eval)
\end{fcode}
\begin{funcargs}
\inarg{integer}{control}{Control flag}
\inarg{integer}{g_a}{Matrix to diagonalize}
\inarg{integer}{g_s}{Metric}
\outarg{integer}{g_v}{Global matrix to return evecs}
\outarg{double precision}{eval(*)}{Local array to return evals}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diagReuse(int control, const GlobalArray *g_s,
                            GlobalArray *g_v, void *eval) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{control}{Control flag}
\inarg{}{g_s}{Matrix to diagonalize}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag_reuse(int control, int g_a, int g_s, int g_v, evalues=None)
   control (int) - 0 indicates first call to the eigensolver; >0
   consecutive calls (reuses factored g_s); <0 only erases factorized g_s;
   g_v and eval unchanged (should be called after previous use if another
   eigenproblem, i.e., different g_a and g_s, is to be solved)
   g_a (int)     - the array handle of the matrix to diagonalize
   g_s (int)     - the array handle of the metric
   g_v (int)     - the array handle to return evecs
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solve the generalized eigenvalue problem returning all eigenvectors and
values in ascending order. Recommended for REPEATED calls if g_s is unchanged.
Values of the control flag:
\begin{verbatim}
          value       action/purpose
            0          indicates first call to the eigensolver
           >0          consecutive calls (reuses factored g_s)
           <0          only erases factorized g_s; g_v and eval unchanged
                       (should be called after previous use if another
                        eigenproblem, i.e., different g_a and g_s, is to
                        be solved)
\end{verbatim}

The input matrices are not destroyed.

Returns:
All eigen-values as an ndarray in ascending order.

This is a collective operation.
\end{desc}

\apih{DIAG STD}{Standard diagonalization of a global array}

\begin{capi}
\begin{ccode}
void GA_Diag_std(int g_a, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{Matrix to diagonalize}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag_std(g_a, g_v, eval)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{Matrix to diagonalize}
\outarg{integer}{g_v}{Global matrix to return evecs}
\outarg{double precision}{eval(*)}{Local array to return evals}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diagStd(GlobalArray *g_v, void *eval) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{g_v}{Global matrix to return evecs}
\outarg{}{eval}{Local array to return evals}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag_std(int g_a, int g_v, evalues=None)
   g_a (int)     - the array handle of the matrix to diagonalize
   g_v (int)     - the array handle to return evecs
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solve the standard (non-generalized) eigenvalue problem returning all
eigenvectors and values in the ascending order. The input matrix is
neither overwritten nor destroyed.

Returns:
all eigenvectors via the g_v global array, and eigenvalues as an
ndarray in ascending order

This is a collective operation.
\end{desc}

\apih{LLT SOLVE}{Cholesky factorization of a global array}

\begin{capi}
\begin{ccode}
int GA_Llt_solve(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{coefficient matrix}
\outarg{}{g_b}{rhs/solution matrix}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_llt_solve(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{coefficient matrix}
\inoutarg{integer}{g_b}{rhs/solution matrix}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::lltSolve(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{coefficient matrix}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
llt_solve(int g_a, int g_b)
   g_a (int)     - the coefficient matrix
   g_b (int)     - the rhs/solution matrix
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solves a system of linear equations
\begin{verbatim}
            A * X = B
\end{verbatim}

using the Cholesky factorization of an NxN double precision symmetric
positive definite matrix A (represented by handle g_a). On successful
exit B will contain the solution X.

It returns:
\begin{verbatim}
         = 0 : successful exit
         > 0 : the leading minor of this order is not positive
               definite and the factorization could
               not be completed.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{LU SOLVE}{LU decomposition of a global array}

\begin{capi}
\begin{ccode}
void GA_Lu_solve(char trans, int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{trans}{transpose or not transpose}
\inarg{}{g_a}{coefficient matrix}
\outarg{}{g_b}{rhs/solution matrix}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_lu_solve(trans, g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{character}{trans}{transpose or not transpose}
\inarg{integer}{g_a}{coefficient matrix}
\inoutarg{integer}{g_b}{rhs/solution matrix}
\end{funcargs}

   trans = `N' or `n' means that the transpose operator should not be applied.
   trans = `T' or `t' means that the transpose operator should be applied.
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::luSolve(char trans, const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{trans}{transpose or not transpose}
\inarg{}{g_a}{coefficient matrix}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
lu_solve(int g_a, int g_b, int trans=False)
   g_a (int)     - the array handle for the coefficient matrix
   g_b (int)     - the array handle for the solution matrix
   trans (bool)  - transpose (True) or not transpose (False)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}


Solve the system of linear equations op(A)X = B based on the LU factorization.

op(A) = A or A' depending on the parameter trans:
\begin{verbatim}
     trans = `N' or `n' means that the transpose operator should not be applied.
     trans = `T' or `t' means that the transpose operator should be applied.
\end{verbatim}

Matrix A is a general real matrix. Matrix B contains possibly multiple rhs vectors.
The array associated with the handle g_b is overwritten by the solution matrix X.

This is a collective operation.
\end{desc}

\apih{SOLVE}{Solve a system of linear equations}

\begin{capi}
\begin{ccode}
int GA_solve(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{coefficient matrix}
\outarg{}{g_b}{rhs/solution matrix}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_solve(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{coefficient matrix}
\inoutarg{integer}{g_b}{rhs/solution matrix}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::solve(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{coefficient matrix}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
solve(int g_a, int g_b)
\end{pycode}
\end{pyapi}


\gcoll

\begin{desc}


Solves a system of linear equations
\begin{verbatim}
            A * X = B
\end{verbatim}

It first will call the Cholesky factorization routine and, if sucessfully, will solve the system with the Cholesky solver. If Cholesky will be not be able to factorize A, then it will call the LU factorization routine and will solve the system with forward/backward substitution. On exit B will contain the solution X.

It returns
\begin{verbatim}
         = 0 : Cholesky factoriztion was succesful
         > 0 : the leading minor of this order
               is not positive definite, Cholesky factorization
               could not be completed and LU factoriztion was used
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SPD INVERT}{Invert a symmetric positive definite matrix a global array}

\begin{capi}
\begin{ccode}
int GA_Spd_invert(int g_a)
\end{ccode}
\begin{funcargs}
\inoutarg{}{g_a}{matrix}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_spd_invert(g_a)
\end{fcode}
\begin{funcargs}
\inoutarg{integer}{g_a}{matrix}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::spdInvert() const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{coefficient matrix}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
spd_invert(int g_a)
\end{pycode}
\end{pyapi}


\gcoll

\begin{desc}


It computes the inverse of a double precision using the Cholesky
factorization of a NxN double precision symmetric positive definite
matrix A stored in the global array represented by g_a. On successful
exit, A will contain the inverse.

It returns
\begin{verbatim}
         = 0 : successful exit
         > 0 : the leading minor of this order is not positive
               definite and the factorization could not be completed
         < 0 : it returns the index i of the (i,i)
               element of the factor L/U that is zero and
               the inverse could not be computed
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SELECT ELEM}{select an element in a global returned by the chosen operation (eg., min, max, etc.)}

\begin{capi}
\begin{ccode}

void NGA_Select_elem(int g_a, char *op, void* val, int index[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle Control}
\inarg{}{op}{operator {`min',`max'}}
\outarg{}{val}{address where value should be stored}
\outarg{}{index[ndim]}{array index for the selected element}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_select_elem(g_a, op, val, index)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle Control}
\inarg{character(*)*}{op}{operator {`min',`max'}}
\outarg{}{val}{address where selected value should be stored}
\outarg{}{index[ndim]}{array index for the selected element}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::selectElem(char *op, void* val, int index[]) const
void GlobalArray::selectElem(char *op, void* val, int64_t index[]) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{op}{operator {"min","max"}}
\outarg{}{val}{address where value should be stored}
\outarg{}{index[ndim]}{array index for the selected element}
\end{funcargs}
\end{cxxapi}


\begin{pyapi}
\begin{pycode}
select_elem(int g_a, char *op)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Returns the value and index for an element that is selected by the
specified operator in a global array corresponding to g_a handle.
This is a collective operation.
\end{desc}

\apih{SUMMARIZE}{Print summary information on a global array}

\begin{capi}
\begin{ccode}
void GA_Summarize(int verbose)
\end{ccode}
\begin{funcargs}
\inarg{}{verbose}{If true print distribution info}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_summarize(verbose)
\end{fcode}
\begin{funcargs}
\inarg{logical}{verbose}{If true print distribution info}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::summarize(int verbose) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{verbose}{If true print distribution info}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
summarize(int verbose)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Prints info about allocated arrays.
\end{desc}

\apih{SYMMETRIZE}{Symmetrize a global array}

\begin{capi}
\begin{ccode}
void GA_Symmetrize(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{}{}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_symmetrize(g_a)
\end{fcode}
\begin{funcargs}
\inoutarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::symmetrize() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
symmetrize(int g_a)
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Symmetrizes matrix A represented with handle g_a: A:= .5 * (A+A').

This is a collective operation.
\end{desc}

\apih{TRANSPOSE}{Transpose a global array}

\begin{capi}
\begin{ccode}
void GA_Transpose(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{original matrix}
\outarg{}{g_b}{solution matrix}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_transpose(g_a, g_b)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{remains unchanged}
\outarg{integer}{g_b}{}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::transpose(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{assign transpose to this GlobalArray}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
transpose(int g_a, int g_b)
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}


Transposes a matrix: B = A', where A and B are represented by handles g_a and g_b.

This is a collective operation.
\end{desc}

\apih{ABS VALUE}{Convert a global array to contain absolute values of its elements}

\begin{capi}
\begin{ccode}
void GA_Abs_value(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_abs_value(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::absValue() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
abs_value(int g_a, lo=None, hi=None)
   g_a (int) - the array handle
   lo (1D array-like) - lower bound patch coordinates, inclusive
   hi (1D array-like) - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}



\gcoll

\begin{desc}

Take the element-wise absolute value of the array.
This is a collective operation.
\end{desc}

\apih{ABS VALUE PATCH}{Convert a patch of a global array to have absolute values of its elements}

\begin{capi}
\begin{ccode}
void GA_Abs_value_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{lo[], hi[]}{g_a patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_abs_value_patch(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{lo(ndim), hi(ndim)}{g_a patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::absValuePatch(int *lo, int *hi) const
void GlobalArray::absValuePatch(int64_t *lo, int64_t *hi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{lower corner patch coordinates}
\inarg{}{hi}{upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Take the element-wise absolute value of the patch.
This is a collective operation.
\end{desc}

\apih{ADD CONSTANT}{Add a constant to all elements in a global array}

\begin{capi}
\begin{ccode}
void GA_Add_constant(int g_a, void *alpha)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{double/complex/int/long/float*}{alpha}{added value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_constant(g_a,  alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{double/complex/integer/float}{alpha}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addConstant(void* alpha) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{alpha}{double/complex/int/long/float constant to be added}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add_constant(int g_a, alpha, lo=None, hi=None)
   g_a (int)                      - the array handle
   alpha (object)                 - the constant to add (converted to
                                    appropriate type)
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Add the constant pointed by alpha to each element of the array.
This is a collective operation.
\end{desc}

\apih{ADD CONSTANT PATCH}{Add a constant to all elements in a global array patch}

\begin{capi}
\begin{ccode}
void GA_Add_constant_patch(int g_a, int lo[], int hi[], void *alpha)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{lo[], hi[]}{patch coordinates}
\inarg{double/complex/int/long/float*}{alpha}{added value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_constant_patch(g_a, lo, hi, alpha)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{lo(ndim), hi(ndim)}{patch coordinates}
\inarg{double/complex/integer/float}{alpha}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addConstantPatch(int *lo, int *hi, void *alpha) const
void GlobalArray::addConstantPatch(int64_t *lo, int64_t *hi, void *alpha) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{lower corner patch coordinates}
\inarg{}{hi}{upper corner patch coordinates}
\inarg{}{alpha}{double/complex/int/long/float constant to be added}
\end{funcargs}
\end{cxxapi}



\gcoll

\begin{desc}

Add the constant pointed by alpha to each element of the patch.
This is a collective operation.
\end{desc}

\apih{RECIP}{Translate a global array to contain reciprocal of its elements}

\begin{capi}
\begin{ccode}
void GA_Recip(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_recip(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::recip() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
recip(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Take the element-wise reciprocal of the array.
This is a collective operation.
\end{desc}

\apih{RECIP PATCH}{Translate a global array patch to contain reciprocal of its elements}

\begin{capi}
\begin{ccode}
void GA_Recip_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{}{lo[], hi[]}{patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_recip_patch(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{lo(ndim), hi(ndim)}{patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::recipPatch(int *lo, int *hi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{lower corner patch coordinates}
\inarg{}{hi}{upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Take element-wise reciprocal of the patch.
This is a collective operation.
\end{desc}

\apih{ELEM MULTIPLY}{Element-wise multiplication of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_multiply(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_multiply(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMultiply(const GlobalArray * g_a, const
GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{GlobalArray}
\inarg{}{g_b}{GlobalArray}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_multiply(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b,
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b,
                                     exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of g_c,
                                     inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of g_c,
                                     exclusive
\end{pycode}
\end{pyapi}


\gcoll

\begin{desc}

Computes the element-wise product of the two arrays
which must be of the same types and same number of
elements. For two-dimensional arrays,

\begin{verbatim}
        c(i, j)  = a(i,j)*b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM MULTIPLY PATCH}{Element-wise multiplication of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_multiply_patch(int g_a, int alo[], int ahi[], int g_b, int blo[],
                            int bhi[], int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\outarg{}{clo[], chi[]}{g_c patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_multiply_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, clo,
chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}
\inarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMultiplyPatch(const GlobalArray * g_a, int *alo,
                                    int *ahi, const GlobalArray * g_b,
                                    int *blo, int *bhi, int *clo,
                                    int *chi) const
void GlobalArray::elemMultiplyPatch(const GlobalArray * g_a, int64_t *alo,
                                    int64_t *ahi, const GlobalArray * g_b,
                                    int64_t *blo, int64_t *bhi,
                                    int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\inarg{}{clo}{g_c lower corner patch coordinates}
\inarg{}{chi}{g_c upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise product of the two patches
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j)  = a(i,j)*b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM DIVIDE}{Element-wise division of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_divide(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_divide(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemDivide(const GlobalArray * g_a, const GlobalArray
                             * g_b) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{GlobalArray}
\inarg{}{g_b}{GlobalArray}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_divide(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b,
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b,
                                     exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of g_c,
                                     inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of g_c,
                                     exclusive
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the element-wise quotient of the two arrays
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j) = a(i,j)/b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
If one of the elements of array g_b is zero, the quotient
for the element of g_c will be set to GA_NEGATIVE_INFINITY.

This is a collective operation.
\end{desc}

\apih{ELEM DIVIDE PATCH}{Element-wise division of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_divide_patch(int g_a, int alo[], int ahi[], int g_b,
                          int blo[], int bhi[], int g_c, int clo[],
                          int chi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\outarg{}{clo[], chi[]}{g_c patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_divide_patch(g_a, alo, ahi, g_b, blo, bhi, g_c,
                                clo, chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}
\inarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemDividePatch(const GlobalArray * g_a, int *alo,
                                  int *ahi, const GlobalArray * g_b,
                                  int *blo, int *bhi, int *clo,
                                  int *chi) const
void GlobalArray::elemDividePatch(const GlobalArray * g_a, int64_t *alo,
                                  int64_t *ahi, const GlobalArray * g_b,
                                  int64_t *blo, int64_t *bhi,
                                  int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\inarg{}{clo}{g_c lower corner patch coordinates}
\inarg{}{chi}{g_c upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise quotient of the two patches
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j)  = a(i,j)/b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM MAXIMUM}{Element-wise maximum of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_maximum(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_maximum(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMaximum(const GlobalArray * g_a, const
                              GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_maximum(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b,
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b,
                                     exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of g_c,
                                     inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of g_c,
                                     exclusive
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the element-wise maximum of the two arrays
which must be of the same types and same number of
elements. For two dimensional arrays,
\begin{verbatim}
    c(i,j)  = max{a(i,j), b(i,j)}
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM MAXIMUM PATCH}{Element-wise maximum of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_maximum_patch(int g_a, int alo[], int ahi[], int g_b,
                           int blo[], int bhi[], int g_c, int clo[],
                           int chi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\outarg{}{clo[], chi[]}{g_c patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_maximum_patch(g_a, alo, ahi, g_b, blo, bhi, g_c,
                                 clo, chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}
\inarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}
\inarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMaximumPatch(const GlobalArray * g_a, int *alo, int *ahi,
                                   const GlobalArray * g_b, int *blo, int *bhi,
                                   int *clo, int *chi) const
void GlobalArray::elemMaximumPatch(const GlobalArray * g_a, int64_t *alo,
                                   int64_t *ahi, const GlobalArray * g_b,
                                   int64_t *blo, int64_t *bhi,
                                   int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\inarg{}{clo}{g_c lower corner patch coordinates}
\inarg{}{chi}{g_c upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise maximum of the two patches
which must be of the same types and same number of
elements. For two-dimensional noncomplex arrays,
\begin{verbatim}
        c(i,j)  = max{a(i,j), b(i,j)}
\end{verbatim}

If the data type is complex, then
\begin{verbatim}
        c(i,j).real = max{ |a(i,j)|, |b(i,j)| } while c(i,j).image = 0.
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM MINIMUM}{Element-wise minimum of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_minimum(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_minimum(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a, g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMinimum(const GlobalArray * g_a, const
                              GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise minimum of the two arrays
which must be of the same types and same number of
elements. For two dimensional arrays,
\begin{verbatim}
        c(i,j)  = min{a(i,j), b(i,j)}
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM MINIMUM PATCH}{Element-wise minimum of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_minimum_patch(int g_a, int alo[], int ahi[], int g_b,
                           int blo[], int bhi[], int g_c, int clo[],
                           int chi[])
\end{ccode}
\begin{funcargs}
\inarg{}{g_a, g_b}{array handles}
\outarg{}{g_c}{array handle}
\inarg{}{alo[], ahi[]}{g_a patch coordinates}
\inarg{}{blo[], bhi[]}{g_b patch coordinates}
\outarg{}{clo[], chi[]}{g_c patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_minimum_patch(g_a, alo, ahi, g_b, blo, bhi, g_c,
                                 clo, chi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b}{array handles}
\outarg{integer}{g_c}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim),ahi(ndim)}{g_a patch dimensions}
\inarg{integer}{blo(ndim),bhi(ndim)}{g_b patch dimensions}
\inarg{integer}{clo(ndim),chi(ndim)}{g_c patch dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMinimumPatch(const GlobalArray * g_a, int *alo, int *ahi,
                                   const GlobalArray * g_b, int *blo, int *bhi,
                                   int *clo, int *chi) const
void GlobalArray::elemMinimumPatch(const GlobalArray * g_a, int64_t *alo,
                                   int64_t *ahi, const GlobalArray * g_b,
                                   int64_t *blo, int64_t *bhi,
                                   int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\inarg{}{clo}{g_c lower corner patch coordinates}
\inarg{}{chi}{g_c upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise minimum of the two patches
which must be of the same types and same number of
elements. For two-dimensional of noncomplex arrays,
\begin{verbatim}
        c(i,j)  = min{a(i,j), b(i,j)}
\end{verbatim}

If the data type is complex, then
\begin{verbatim}
        c(i,j).real = min{ |a(i,j)|, |b(i,j)| } while c(i,j).image = 0.
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{SHIFT DIAGONAL}{Add specified constant to diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Shift_diagonal(int g_a, void *c)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{double/complex/int/long/float}{c}{shift value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_shift_diagonal(g_a, c)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{double/complex/integer/float}{c}{TODO}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::shiftDiagonal(void *c) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{c}{double/complex/int/long/float constant to add}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
shift_diagoal(int g_a, value=None)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Adds this constant to the diagonal elements of the matrix.
This is a collective operation.
\end{desc}

\apih{SET DIAGONAL}{Set the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Set_diagonal(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_v}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_diagonal(g_a, g_v)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_v}{array handles}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setDiagonal(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_v}{global array containing diagonal values}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_diagonal(int g_a, int g_v)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Sets the diagonal elements of this matrix g_a with the elements of the vector g_v.
This is a collective operation.
\end{desc}

\apih{ZERO DIAGONAL}{Zero the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Zero_diagonal(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_zero_diagonal(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zeroDiagonal() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
zero_diagonal(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Sets the diagonal elements of this matrix g_a with zeros.
This is a collective operation.
\end{desc}

\apih{ADD DIAGONAL}{Add to the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Add_diagonal(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_v}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_diagonal(g_a, g_v)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_v}{array handles}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addDiagonal(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_v}{global array containing diagonal elements to be added}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add_diagonal(int g_a, int g_v)
   g_a (int)     - the array handle
   g_v (int)     - the vector handle
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Adds the elements of the vector g_v to the diagonal of this matrix g_a.
This is a collective operation.
\end{desc}

\apih{GET DIAG}{Copy diagonal elements of a global array into another global array}

\begin{capi}
\begin{ccode}
void GA_Get_diag(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_v}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get_diag(g_a, g_v)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{g_v}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::getDiagonal(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array containing diagonal elements}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_diag(int g_a, int g_v)
   g_a (int)        - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Inserts the diagonal elements of this matrix g_a into the vector g_v.
This is a collective operation.
\end{desc}

\apih{SCALE ROWS}{Scale the rows of a global array with elements in another global array}

\begin{capi}
\begin{ccode}
void GA_Scale_rows(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_v}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scale_rows(g_a, g_v)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_v}{array handles}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scaleRows(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_v}{global array containing scale factors}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
g_a (int)                 - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales the rows of this matrix g_a using the vector g_v.
This is a collective operation.
\end{desc}

\apih{SCALE COLS}{Scale columns of a global array with elements in another gobal array}

\begin{capi}
\begin{ccode}
void GA_Scale_cols(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_v}{array handles}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_scale_cols(g_a, g_v)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_v}{array handles}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scaleCols(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_v}{global array containing scale factors}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scale_cols(int g_a, int g_v)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales the columns of this matrix g_a using the vector g_v.
This is a collective operation.
\end{desc}

\apih{NORM1}{Compute a global array's 1-norm}

\begin{capi}
\begin{ccode}
void GA_norm1(int g_a, double *nm)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{nm}{matrix/vector 1-norm value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_norm1(g_a, nm)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{double precision}{nm}{matrix/vector 1-norm value}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::norm1(double *nm) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{nm}{matrix/vector 1-norm value}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
norm1(int g_a)
   g_a (int)                   - the array handle
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the 1-norm of the matrix or vector g_a.
This is a collective operation.
\end{desc}

\apih{NORM INFINITY}{Compute a global array's infinite norm}

\begin{capi}
\begin{ccode}
void GA_Norm_infinity(int g_a, double *nm)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\outarg{}{nm}{matrix/vector infinity-norm value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_norm_infinity(g_a, nm)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\outarg{double precision}{nm}{matrix/vector infinity-norm value}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::normInfinity(double *nm) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{nm}{matrix/vector infinity-norm value}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
norm_infinity(int g_a)
   g_a (int)           - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the infinity-norm of the matrix or vector g_a.
Returns: the 1-norm of the matrix or vector g_a.

This is a collective operation.
\end{desc}

\apih{MEDIAN}{Compute a global arrays median}

\begin{capi}
\begin{ccode}
void GA_Median(int g_a, int g_b, int g_c, int g_m)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_b,g_c}{array handles}
\outarg{}{g_m}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_median(g_a, g_b, g_c, g_m)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b,g_c}{array handles}
\outarg{integer}{g_m}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::median(const GlobalArray * g_a, const GlobalArray * g_b,
                         const GlobalArray * g_c) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{g_c}{global array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
median(int g_a, int g_b, int g_c, int g_m, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None, mlo=None, mhi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   g_m (int)                       - the array handle for the result
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b,
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b,
                                     exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of g_c,
                                     inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of g_c,
                                     exclusive
   mlo (1D array-like of integers) - lower bound patch coordinates of g_m,
                                     inclusive
   mhi (1D array-like of integers) - higher bound patch coordinates of g_m,
                                     exclusive
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the componentwise Median of three arrays g_a, g_b, and g_c, and
stores the result in this array g_m.  The result (m) may replace one of
the input arrays (a/b/c).
This is a collective operation.
\end{desc}

\apih{MEDIAN PATCH}{Compute a global array patch's median}

\begin{capi}
\begin{ccode}
void GA_Median_patch(int g_a, int alo[], int ahi[], int g_b, int blo[],
                     int bhi[], int g_c, int clo[], int chi[], int g_m,
                     int mlo[], int mhi[])
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_b,g_c}{array handles}
\outarg{}{g_m}{array handle}
\inarg{}{alo[],ahi[]}{g_a patch coordinates}
\inarg{}{blo[],bhi[]}{g_b patch coordinates}
\inarg{}{clo[],chi[]}{g_c patch coordinates}
\outarg{}{mlo[],mhi[]}{g_m patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_median_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, clo, chi, g_m, mlo, mhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b,g_c}{array handles}
\outarg{integer}{g_m}{array handle}
\inarg{integer}{ndim}{number of dimensions}
\inarg{integer}{alo(ndim),ahi(ndim)}{g_a patch dimensions}
\inarg{integer}{blo(ndim),bhi(ndim)}{g_b patch dimensions}
\inarg{integer}{clo(ndim),chi(ndim)}{g_c patch dimensions}
\inarg{integer}{mlo(ndim),mhi(ndim)}{g_m patch dimensions}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::medianPatch(const GlobalArray *g_a, int *alo, int *ahi,
                              const GlobalArray *g_b, int *blo, int *bhi,
                              const GlobalArray *g_c, int *clo, int *chi,
                              int *mlo, int *mhi) const;
void GlobalArray::medianPatch(const GlobalArray *g_a, int64_t *alo,
                              int64_t *ahi, const GlobalArray *g_b,
                              int64_t *blo, int64_t *bhi, const
                              GlobalArray *g_c, int64_t *clo,
                              int64_t *chi, int64_t *mlo, int64_t *mhi)
                              const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_a}{global array}
\inarg{}{g_b}{global array}
\inarg{}{g_c}{global array}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\inarg{}{clo}{g_c lower corner patch coordinates}
\inarg{}{chi}{g_c upper corner patch coordinates}
\inarg{}{mlo}{g_m lower corner patch coordinates}
\inarg{}{mhi}{g_m upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the componentwise Median of three patches g_a, g_b, and g_c,
and stores the result in this patch g_m.  The result (m) may replace
one of the input patches (a/b/c).
This is a collective operation.
\end{desc}

\apih{STEP MAX}{Compute a global array's step max}

\begin{capi}
\begin{ccode}
void GA_Step_max(int g_a, int g_b, double *step)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_b}{array handles where g_b is step direction}
\outarg{}{step}{maximum step size}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_step_max(g_a, g_b, step)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b}{array handles}
\outarg{double precision}{step}{the maximum step}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stepMax(const GlobalArray * g_b, double *step) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_b}{global array where g_b is the step direction}
\outarg{}{step}{the maximum step}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
step_max(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Calculates the largest multiple of a vector g_b that can be added to
this vector g_a while keeping each element of this vector non-negative.
This is a collective operation.
\end{desc}

\apih{STEP MAX2}{Compute a global array's step max2}

\begin{capi}
\begin{ccode}
void GA_Step_max2(int g_xx, int g_vv, int g_xxll, int g_xxuu, double * step2)
\end{ccode}
\begin{funcargs}
\inarg{}{g_xx}{array handle}
\inarg{}{g_vv}{step direction array handle}
\inarg{}{g_xxll}{lower bounds array handle}
\inarg{}{g_xxuu}{upper bounds array handle}
\outarg{}{step2}{maximum step size}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_step_max2(g_xx, g_vv, g_xxll, g_xxuu, step2)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_xx,g_vv,g_xxll,g_xxuu}{array handles}
\outarg{double precision}{step2}{the maximum step size}
\inarg{}{g_vv}{the step direction}
\inarg{}{g_xxll}{lower bounds}
\inarg{}{g_xxuu}{upper bounds}
\end{funcargs}
\end{fapi}
\gcoll

\begin{desc}

Calculates the largest step size that should be used in a projected bound line search.
This is a collective operation.
\end{desc}

\apih{STEP MAX PATCH}{Compute a global array patch's step max}

\begin{capi}
\begin{ccode}
void GA_Step_max_patch(int g_a, int alo[], int ahi[], int g_b, blo[], bhi[], double *step)
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_b}{array handles where g_b is step direction}
\outarg{}{step}{the maximum step}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_step_max_patch(g_a, alo, ahi, g_b, blo, bhi, step)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b}{array handles where g_b is step direction}
\inarg{integer}{alo,ahi,blo,bhi}{patch coordinates of g_a and g_b}
\outarg{double precision}{step}{the maximum step}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stepMaxPatch(int *alo, int *ahi, const GlobalArray *g_b,
                               int *blo, int *bhi, double *step) const
void GlobalArray::stepMaxPatch(int64_t *alo, int64_t *ahi, const GlobalArray
                               *g_b, int64_t *blo, int64_t *bhi,
                               double *step) const
\end{cxxcode}
\begin{funcargs}
\inarg{}{g_b}{global array representing step direction}
\inarg{}{alo}{g_a lower corner patch coordinates}
\inarg{}{ahi}{g_a upper corner patch coordinates}
\inarg{}{blo}{g_b lower corner patch coordinates}
\inarg{}{bhi}{g_b upper corner patch coordinates}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Calculates the largest multiple of a vector g_b that can be added to this vector g_a while keeping each element of this vector non-negative.
This is a collective operation.
\end{desc}

\apih{PGROUP GET DEFAULT}{Set default GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_default()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_default()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup* PGroup::getDefault()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_default()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function will return a handle to the default processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation.
\end{desc}

\apih{PGROUP GET MIRROR}{Get the mirrored processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_mirror()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_mirror()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup * PGroup::getMirror()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_mirror()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function will return a handle to the mirrored processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation.
\end{desc}

\apih{PGROUP GET WORLD}{Get the world processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_world()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_world()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup * PGroup::getWorld()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_world()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function will return a handle to the world processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation.
\end{desc}

\apih{PGROUP SYNC}{Synchronize processes in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_sync(int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_sync(p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::sync()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_sync(int pgroup)
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

This operation executes a synchronization group across the processors in the processor group specified by p_handle. Nodes outside this group are unaffected.

This is a collective operation on the processor group specified by p_handle.
\end{desc}

\apih{PGROUP BRDCST}{Broadcast elements among processes in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_brdcst(int p_handle, void* buf, int lenbuf, int root)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\inoutarg{}{buf}{pointer to buffer containing data}
\inarg{}{lenbuf}{length of data (in bytes)}
\inarg{}{root}{processor sending message}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_brdcst(p_handle, type, buf, lenbuf, root)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\inarg{integer}{type}{message index}
\inoutarg{byte}{buf(lenbuf)}{local message buffer}
\inarg{integer}{lenbuf}{length of message}
\inarg{integer}{root}{processor sending message}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::brdcst(void* buf, int lenbuf, int root)
\end{cxxcode}
\begin{funcargs}
\inoutarg{}{buf}{pointer to buffer containing data}
\inarg{}{lenbuf}{length of data (in bytes)}
\inarg{}{root}{processor sending message}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_brdcst(int pgroup, ndarray buffer, int root)
   pgroup (int)        - processor group handle
   buffer (array-like) - the message
   root (int)          - the process which is sending
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Broadcast data from processor specified by root to all other processors in the processor group specified by p_handle. The length of the message in bytes is specified by lenbuf. The initial and broadcasted data can be found in the buffer specified by the pointer buf.

If the buffer is not contiguous, an error is raised. This operation is provided only for convenience purposes: it is available regardless of the message-passing library that GA is running with.


This is a collective operation on the processor group specified by p_handle.

Returns:
The buffer in case a temporary was passed in.
\end{desc}

\apih{PGROUP DGOP}{Double global operation with a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_dgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\inoutarg{}{buf}{buffer containing data}
\inarg{}{n}{number of elements in x}
\inarg{}{op}{operation to be performed}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_dgop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\inarg{integer}{type}{message index}
\inoutarg{double precision}{buf(n)}{double precision array}
\inarg{integer}{n}{number elements in array}
\inarg{character*(*)}{op}{operation on data}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(double *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a double precision array present on each processor in the processor group p_handle. The GA_Pgroup_dgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processor in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle.
\end{desc}

\apih{PGROUP IGOP}{Integer global operation within a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_igop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\inoutarg{}{buf}{buffer containing data}
\inarg{}{n}{number of elements in x}
\inarg{}{op}{operation to be performed}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_igop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\inarg{integer}{type}{message index}
\inoutarg{integer}{buf(n)}{integer array}
\inarg{integer}{n}{number elements in array}
\inarg{character*(*)}{op}{operation on data}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(int *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is an integer array present on each processor in the processor group p_handle. The GA_Pgroup_igop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle.
\end{desc}

\apih{PGROUP LGOP}{Long global operation in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_lgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\inoutarg{}{buf}{buffer containing data}
\inarg{}{n}{number of elements in x}
\inarg{}{op}{operation to be performed}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(long *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a long integer array present on each processor in the processor group p_handle. The GA_Pgroup_lgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle.
\end{desc}

\apih{PGROUP FGOP}{Floating point global operation in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_lgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\inoutarg{}{buf}{buffer containing data}
\inarg{}{n}{number of elements in x}
\inarg{}{op}{operation to be performed}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_sgop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\inarg{integer}{type}{message index}
\inoutarg{real}{buf(n)}{single precision array}
\inarg{integer}{n}{number elements in array}
\inarg{character*(*)}{op}{operation on data}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(float *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\inarg{}{n}{number of elements}
\inoutarg{}{x[n]}{array of elements}
\inarg{}{op}{operator}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a single precision array present on each processor in the processor group p_handle. The GA_Pgroup_fgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle.
\end{desc}

\apih{PGROUP NNODES}{Number of GA ranks in a processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_nnodes(int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_nnodes(p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int PGroup::nodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_nnodes(int pgroup)
   pgroup (int)                  - the group handle
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function returns the number of processors contained in the group specified by p_handle.

Returns the number of processors contained in the group specified by
pgroup.

This is a local operation.
\end{desc}

\apih{PGROUP NODEID}{GA rank of invoking process in a processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_nodeid(int p_handle)
\end{ccode}
\begin{funcargs}
\inarg{}{p_handle}{processor group handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_nodeid(p_handle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{p_handle}{processor group handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int PGroup::nodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_nodeid(int pgroup)
   pgroup (int)                  - the group handle
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This function returns the relative index of the processor in the processor group specified by p_handle. This index will generally differ from the absolute processor index returned by GA_Nodeid if the processor group is not the world group.

Returns the relative index of the processor in the processor group
specified by pgroup.

This is a local operation.

\end{desc}

\apih{MERGE MIRRORED}{Merge a mirrored global array}

\begin{capi}
\begin{ccode}
int GA_Merge_mirrored(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handles}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_merge_mirrored(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::mergeMirrored()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
merge_mirrored(int g_a)
   g_a (int)                     - array handle
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

This subroutine merges mirrored arrays by adding the contents of each array across nodes. The result is that each mirrored copy of the array represented by g_a is the sum of the individual arrays before the merge operation. After the merge, all mirrored arrays are equal.

This is a  collective  operation.

\end{desc}

\apih{IS MIRRORED}{Check whether a global array is mirrored}

\begin{capi}
\begin{ccode}
int GA_Is_mirrored(int g_a)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer ga_is_mirrored(g_a)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::isMirrored()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
is_mirrored(int g_a)
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This subroutine checks if the array is a mirrored array or not. Returns 1 if it is a mirrored array, else it returns 0.

This is a local operation.

\end{desc}

\apih{MERGE DISTR PATCH}{Merge a patched of a mirrored global array}

\begin{capi}
\begin{ccode}
int NGA_Merge_distr_patch(int g_a, int alo[], int ahi[], int g_b, int blo[], int bhi[])
\end{ccode}
\begin{funcargs}
\inarg{g_a}{,g_b}{array handles}
\inarg{}{alo[],ahi[]}{g_a patch coordinates}
\inarg{}{blo[],bhi[]}{g_b patch coordinates}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer nga_merge_distr_patch(g_a, alo, ahi, g_b, blo, bhi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a,g_b}{array handles}
\funcarg{integer}{ndim}{number of dimensions of the global array}{NA}
\inarg{integer}{alo(ndim),ahi(ndim)}{g_a patch coordinates}
\inarg{integer}{blo(ndim),bhi(ndim)}{g_b patch coordinates}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::mergeDistrPatch(int alo[], int ahi[], GlobalArray *g_a,
                                  int blo[], int bhi[])
void GlobalArray::mergeDistrPatch(int64_t alo[], int64_t ahi[], GlobalArray *g_a,
                                  int64_t blo[], int64_t bhi[])
\end{cxxcode}
\begin{funcargs}
\inarg{}{alo[ndim]}{patch indices of mirrored array}
\inarg{}{ahi[ndim]}{patch indices of mirrored array}
\inarg{}{blo[ndim]}{patch indices of result array}
\inarg{}{bhi[ndim]}{patch indices of result array}
\outarg{}{g_a}{global array containing result}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
merge_distr_patch(int g_a, alo, ahi, int g_b, blo, bhi)
   g_a (int)                       - array handle
   alo (1D array-like of integers) - g_a patch coordinate
   ahi (1D array-like of integers) - g_a patch coordinate
   g_b (int)                       - array handle
   blo (1D array-like of integers) - g_b patch coordinate
   bhi (1D array-like of integers) - g_b patch coordinate
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

This function merges all copies of a patch of a mirrored array (g_a) into a patch in a distributed array (g_b).

This is a collective operation.
\end{desc}

\apih{NBGET}{Non-blocking get from a global array}

\begin{capi}
\begin{ccode}
void NGA_NbGet(int g_a, int lo[], int hi[], void* buf, int ld[], ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\outarg{}{buf}{pointer to the local buffer array where the data goes}
\inarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{}{nbhandle}{pointer to the non-blocking request handle}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbget(g_a, ilo, ihi, jlo, jhi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ilo,ihi}{starting indices for global array section}
\inarg{integer}{jlo,jhi}{ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data goes}
\inarg{integer}{ld}{leading dimension/stride/extent for buffer array}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbget(g_a, lo, hi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{lo[ndim]}{array of starting indices for global array section}
\inarg{integer}{hi[ndim]}{array of ending indices for global array section}
\outarg{type}{buf}{local buffer array where the data goes}
\inarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbGet(int lo[], int hi[], void *buf, int ld[],
                        GANbhdl *nbhandle)
void GlobalArray::nbGet(int64_t lo[], int64_t hi[], void *buf, int64_t ld[],
                        GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{patch coordinates of block}
\inarg{}{hi[ndim]}{patch coordinates of block}
\inarg{}{buf}{local buffer to receive data}
\inarg{}{ld[ndim-1]}{array of strides for local data}
\outarg{}{nbhandle}{nonblocking handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbget(int g_a, lo=None, hi=None, ndarray buffer=None)
   g_a (int)                      - the array handle
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
   buffer (ndarray)               - Fill this buffer instead of allocating a
                                    new one internally. Must be contiguous and
                                    have same number of elements as patch.
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

A non-blocking version of the blocking get operation. The get operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Copies data from global array section to the local array buffer.

The local array is assumed to be have the same number of dimensions as the global array. Any detected inconsitencies/errors in the input arguments are fatal.

This is a non-blocking one-sided operation.
Returns:
The local array buffer.

\end{desc}

\apih{NBPUT}{Non-blocking put into a global array}

\begin{capi}
\begin{ccode}
void NGA_NbPut(int g_a, int lo[], int hi[], void* buf, int ld[],
ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data is}
\inarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{}{nbhandle}{pointer to the non-blocking request handle}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbput(g_a, ilo, ihi, jlo, jhi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ilo,ihi}{starting indices for global array section}
\inarg{integer}{jlo,jhi}{ending indices for global array section}
\inarg{}{typebuf}{local buffer array where the data is}
\inarg{integer}{ld}{leading dimension/stride/extent for buffer array}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbput(g_a, lo, hi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{lo[ndim]}{array of starting indices for global array section}
\inarg{integer}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{typebuf}{local buffer array where the data is}
\inarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbPut(int lo[], int hi[], void *buf, int ld[],
                        GANbhdl *nbhandle)
void GlobalArray::nbPut(int64_t lo[], int64_t hi[], void *buf, int64_t ld[],
                        GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{patch coordinates of block}
\inarg{}{hi[ndim]}{patch coordinates of block}
\inarg{}{buf}{local buffer to receive data}
\inarg{}{ld[ndim-1]}{array of strides for local data}
\outarg{}{nbhandle}{nonblocking handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbput(int g_a, buffer, lo=None, hi=None)
   g_a (int)                      - the array handle
   buffer (array-like)            - the data to put
   lo (1D array-like of integers) - lower bound patch coordinates,
                                    inclusive
   hi (1D array-like of integers) - higher bound patch coordinates,
                                    exclusive
\end{pycode}
\end{pyapi}

\ncoll

\begin{desc}

A non-blocking version of the blocking put operation. The put operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Copies data from local array buffer to the global array section.

The local array is assumed to be have the same number of dimensions as the global array. Any detected inconsitencies/errors in input arguments are fatal.

This is a non-blocking one-sided operation.
\end{desc}

\apih{NBACC}{Non-blocking accumulate into a global array}

\begin{capi}
\begin{ccode}
void NGA_NbAcc(int g_a, int lo[], int hi[], void* buf, int ld[], void *alpha,
               ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{global array handle}
\inarg{}{lo[ndim]}{array of starting indices for global array section}
\inarg{}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{buf}{pointer to the local buffer array where the data is}
\inarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{double/double complex/long*}{alpha}{scale factor}
\inarg{}{nbhandle}{pointer to the non-blocking request handle}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbacc(g_a, ilo, ihi, jlo, jhi, buf, ld, alpha, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ilo,ihi}{starting indices for global array section}
\inarg{integer}{jlo,jhi}{ending indices for global array section}
\inarg{}{typebuf}{local buffer array where the data is}
\inarg{integer}{ld}{leading dimension/stride/extent for buffer array}
\inarg{double precision/complex}{alpha}{scale factor}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbacc(g_a, lo, hi, buf, ld, alpha, nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{global array handle}
\inarg{integer}{ndim}{number of dimensions of the global array}
\inarg{integer}{lo[ndim]}{array of starting indices for global array section}
\inarg{integer}{hi[ndim]}{array of ending indices for global array section}
\inarg{}{typebuf}{local buffer array where the data is}
\inarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}
\inarg{double precision/complex}{alpha}{scale factor}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbAcc(int lo[], int hi[], void *buf, int ld[], void *alpha,
                        GANbhdl *nbhandle)
void GlobalArray::nbAcc(int64_t lo[], int64_t hi[], void *buf, int64_t ld[],
                        void *alpha, GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo[ndim]}{patch coordinates of block}
\inarg{}{hi[ndim]}{patch coordinates of block}
\inarg{}{buf}{local buffer to receive data}
\inarg{}{ld[ndim-1]}{array of strides for local data}
\inarg{alpha}{}{multiplier for data before adding to existing results}
\outarg{}{nbhandle}{nonblocking handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbacc(int g_a, buffer, lo=None, hi=None, alpha=None)
   g_a (int)                      - the array handle
   buffer (array-like)            - must be contiguous and have same number
                                    of elements as patch
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
   alpha (object)                 - multiplier (converted to the appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

A non-blocking version of the blocking accumulate operation. The accumulate operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Non-blocking version of ga.acc.

The accumulate operation can be completed locally by making a call to the ga.nbwait() routine.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as the global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a non-blocking one-sided operation.
\end{desc}

\apih{NBWAIT}{Wait for a non-blocking GA operation}

\begin{capi}
\begin{ccode}
void NGA_NbWait(ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\inarg{}{nbhandle}{pointer to the non-blocking request handle}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_nbwait(nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{fapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbwait(nbhandle)
\end{fcode}
\begin{funcargs}
\inarg{integer}{nbhandle}{non-blocking request handle}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbWait(GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\inarg{}{nbhandle}{nonblocking handle}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbwait(ga_nbhdl_t nbhandle)
\end{pycode}
\end{pyapi}

\ncoll

\begin{desc}

This function completes a non-blocking one-sided operation locally. Waiting on a nonblocking put or an accumulate operation assures that data was injected into the network and the user buffer can now be reused. Completing a get operation assures data has arrived into the user memory and is ready for use. The wait operation ensures only local completion.

Unlike their blocking counterparts, the nonblocking operations are not ordered with respect to the destination. Performance being one reason, the other reason is that by ensuring ordering we incur additional and possibly unnecessary overhead on applications that do not require their operations to be ordered. For cases where ordering is necessary, it can be done by calling a fence operation. The fence operation is provided to the user to confirm remote completion if needed.
\end{desc}

\apih{WTIME}{Return time in seconds}

\begin{capi}
\begin{ccode}
double GA_Wtime()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
double precision function ga_wtime()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
double GlobalArray::wtime()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
wtime()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function returns a wall (or elapsed) time on the calling processor. Returns time in seconds representing elapsed wall-clock time since an arbitrary time in the past. Example:

\begin{verbatim}
double starttime, endtime;
starttime = GA_Wtime();
.... code snippet to be timed ....
endtime   = GA_Wtime();
printf(\"Time taken = \%lf secondsn\", endtime-starttime);
\end{verbatim}

This is a local operation.
This function is only available in release 4.1 or greater.
\end{desc}

\apih{SET DEBUG}{Set GA debug flag}

\begin{capi}
\begin{ccode}
void GA_Set_debug(int dbg)
\end{ccode}
\begin{funcargs}
\inarg{}{dbg}{value to set internal flag}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_debug(flag)
\end{fcode}
\begin{funcargs}
\inarg{logical}{flag}{value to set internal debug flag}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::setDebug(int dbg);
\end{cxxcode}
\begin{funcargs}
\inarg{}{dbg}{value to set internal flag}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_debug(int debug)
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function sets an internal flag in the GA library to either true or false. The value of this flag can be recovered at any time using the GA_Get_debug function. The flag is set to false when the the GA library is initialized. This can be useful in a number of debugging situations, especially when examining the behavior of routines that are called in multiple locations in a code.

This is a local operation.
\end{desc}

\apih{GET DEBUG}{Retrieve value of GA debug flag}

\begin{capi}
\begin{ccode}
int GA_Get_debug()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_get_debug()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::getDebug()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_debug()
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function returns the value of an internal flag in the GA library whose value can be set using the GA_Set_debug subroutine.

This is a local operation.
\end{desc}

\apih{PATCH ENUM}{Enumerate a global array patch}

\begin{capi}
\begin{ccode}
void GA_Patch_enum(int g_a, int lo, int hi, void *start, void *inc)
\end{ccode}
\begin{funcargs}
\inarg{}{g_a}{array handle}
\inarg{lo}{,hi}{patch coordinates}
\inarg{}{start}{starting value of enumeration}
\inarg{}{inc}{increment value}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_patch_enum(g_a, lo, hi, start, inc)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_a}{array handle}
\inarg{integer}{lo,hi}{low and high values of array patch}
\inarg{integer/double precision/complex}{start}{starting value of enumeration}
\inarg{integer/double precision/complex}{inc}{increment value}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::patchEnum(int lo, int hi, void *istart, void *inc)
void GlobalArray::patchEnum(int64_t lo, int64_t hi, void *start, void *inc)
\end{cxxcode}
\begin{funcargs}
\inarg{}{lo}{coordinate interval to enumerate}
\inarg{}{hi}{coordinate interval to enumerate}
\inarg{}{istart}{starting value of enumeration}
\inarg{}{inc}{increment value}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

This subroutine enumerates the values of an array between elements lo and hi starting with the value start and incrementing each subsequent value by inc. This operation is only applicable to 1-dimensional arrays. An example of its use is shown below:

\begin{verbatim}
GA_Patch_enum(g_a, 1, n, 7, 2);

g_a:  7  9 11 13 15 17 19 21 23 ...
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SCAN ADD}{Scan add (fixme)}

\begin{capi}
\begin{ccode}
void GA_Scan_add(int g_src, int g_dest, int g_mask, int lo, int hi, int excl)
\end{ccode}
\begin{funcargs}
\inarg{}{g_src}{handle for source array}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{lo}{,hi}{low and high values of range on which operation is performed}
\inarg{}{excl}{value to signify if masked values are included in in add}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scan_add(g_src, g_dest, g_mask, lo, hi, excl)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_src}{handle for source array}
\outarg{integer}{g_dest}{handle for destination array}
\inarg{integer}{g_mask}{handle for integer array representing a bit mask}
\inarg{integer}{lo,hi}{low and high values of range on which operation is performed}
\inarg{integer}{excl}{value to signify if masked values are included in add}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scanAdd(const GlobalArray *g_dest, const GlobalArray *g_mask,
                          int lo, int hi, int excl) const
void GlobalArray::scanAdd(const GlobalArray *g_dest, const GlobalArray *g_mask,
                          int64_t lo, int64_t hi, int excl) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{}{lo}{low and high values of range on which operation is performed}
\inarg{}{hi}{low and high values of range on which operation is performed}
\inarg{}{excl}{value to signify if masked values are included in in add}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scan_add(int g_src, int g_dst, int g_msk, lo=None, hi=None, int excl=False)
   g_src (int)                    - handle for source arrray
   g_dst (int)                    - handle for destination array
   g_msk (int)                    - handle for integer array representing mask
   lo (1D array-like of integers) - low value of range on which operation is
                                    performed
   hi (1D array-like of integers) - hi value of range on which operation is
                                    performed
   excl (bool)                    - whether the first value is set to 0 (see above)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This operation will add successive elements in a source vector g_src and put the results in a destination vector g_dest. The addition will restart based on the values of the integer mask vector g_mask. The scan is performed within the range specified by the integer values lo and hi. Note that this operation can only be applied to 1-dimensional arrays. The excl flag determines whether the sum starts with the value in the source vector corresponding to the location of a 1 in the mask vector (excl=0) or whether the first value is set equal to 0 (excl=1). Some examples of this operation are given below.

\begin{verbatim}
GA_Scan_add(g_src, g_dest, g_mask, 1, n, 0);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  3  6 10 16 21  7 15  9 19 30 12 25 39 15 16 33

GA_Scan_add(g_src, g_dest, g_mask, 1, n, 1);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   0  1  3  6 10 15  0  7  0  9 19  0 12 25  0  0 16
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SCAN COPY}{Scan copy (fixme)}

\begin{capi}
\begin{ccode}
void GA_Scan_copy(int g_src, int g_dest, int g_mask, int lo, int hi)
\end{ccode}
\begin{funcargs}
\inarg{}{g_src}{handle for source array}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{lo}{,hi}{low and high values of range on which operation is performed}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scan_copy(g_src, g_dest, g_mask, lo, hi)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_src}{handle for source array}
\outarg{integer}{g_dest}{handle for destination array}
\inarg{integer}{g_mask}{handle for integer array representing a bit mask}
\inarg{integer}{lo,hi}{low and high values of range on which operation is performed}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scanCopy(const GlobalArray *g_dest, const GlobalArray *g_mask,
                           int lo, int hi) const
void GlobalArray::scanCopy(const GlobalArray *g_dest, const GlobalArray *g_mask,
                           int64_t lo, int64_t hi) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{}{lo}{low values of range on which operation is performed}
\inarg{}{hi}{high values of range on which operation is performed}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scan_copy(int g_src, int g_dst, int g_msk, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This subroutine does a segmented scan-copy of values in the source array g_src into a destination array g_dest with segments defined by values in the integer mask array g_mask. The scan-copy operation is only applied to the range between the lo and hi indices. This operation is restriced to 1-dimensional arrays. The resulting destination array will consist of segments of consecutive elements with the same value. An example is shown below.

\begin{verbatim}
GA_Scan_copy(g_src, g_dest, g_mask, 1, n);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  1  1  1  1  1  7  7  9  9  9 12 12 12 15 16 16

This is  a collective operation.
\end{verbatim}

\end{desc}

\apih{PACK}{Pack (fixme)}

\begin{capi}
\begin{ccode}
void GA_Pack(int g_src, int g_dest, int g_mask, int lo, int hi,
             int *icount)
\end{ccode}
\begin{funcargs}
\inarg{}{g_src}{handle for source array}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{lo}{,hi}{low and high values of range on which operation is performed}
\outarg{}{icount}{number of values in compressed array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pack(g_src, g_dest, g_mask, lo, hi, icount)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_src}{handle for source array}
\outarg{integer}{g_dest}{handle for destination array}
\inarg{integer}{g_mask}{handle for integer array representing a bit mask}
\inarg{integer}{lo,hi}{low and high values of range on which operation is performed}
\outarg{integer}{icount}{number of values in compressed array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::pack(const GlobalArray *g_dest, const GlobalArray *g_mask,
                       int lo, int hi, int *icount) const
void GlobalArray::pack(const GlobalArray *g_dest, const GlobalArray *g_mask,
                       int64_t lo, int64_t hi, int64_t *icount) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{g_dest}{destination array}
\inarg{}{g_mask}{mask array}
\inarg{}{lo}{coordinate interval to pack}
\inarg{}{hi}{coordinate interval to pack}
\outarg{}{icount}{number of packed elements}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pack(int g_src, int g_dst, int g_msk, lo=None, hi=None)
   g_src (int)                    - handle for source arrray
   g_dst (int)                    - handle for destination array
   g_msk (int)                    - handle for integer array representing mask
   lo (1D array-like of integers) - low value of range on which operation
                                    is performed
   hi (1D array-like of integers) - hi value of range on which operation
                                    is performed
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

The pack subroutine is designed to compress the values in the source vector g_src into a smaller destination array g_dest based on the values in an integer mask array g_mask. The values lo and hi denote the range of elements that should be compressed and icount is a variable that on output lists the number of values placed in the compressed array. This operation is the complement of the GA_Unpack operation. An example is shown below

\begin{verbatim}
GA_Pack(g_src, g_dest, g_mask, 1, n, \&icount);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  7  9 12 15 16
icount:   6
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{UNPACK}{Unpack (fixme)}

\begin{capi}
\begin{ccode}
void GA_Unpack(int g_src, int g_dest, int g_mask, int lo, int hi,
               int *icount)
\end{ccode}
\begin{funcargs}
\inarg{}{g_src}{handle for source array}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{lo}{,hi}{low and high values of range on which operation is performed}
\outarg{}{icount}{number of values in uncompressed array}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_unpack(g_src, g_dest, g_mask, lo, hi, icount)
\end{fcode}
\begin{funcargs}
\inarg{integer}{g_src}{handle for source array}
\outarg{integer}{g_dest}{handle for destination array}
\inarg{integer}{g_mask}{handle for integer array representing a bit mask}
\inarg{integer}{lo,hi}{low and high values of range on which operation is performed}
\outarg{integer}{icount}{number of values in uncompressed array}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::unpack(GlobalArray *g_dest, GlobalArray *g_mask,
                         int lo, int hi, int *icount) const
void GlobalArray::unpack(GlobalArray *g_dest, GlobalArray *g_mask,
                         int64_t lo, int64_t hi, int64_t *icount) const
\end{cxxcode}
\begin{funcargs}
\outarg{}{g_dest}{handle for destination array}
\inarg{}{g_mask}{handle for integer array representing mask}
\inarg{}{lo}{low value of range on which operation is performed}
\inarg{}{hi}{high value of range on which operation is performed}
\outarg{}{icount}{number of values in uncompressed array}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
unpack(int g_src, int g_dst, int g_msk, lo=None, hi=None)
   g_src (int)                    - handle for source arrray
   g_dst (int)                    - handle for destination array
   g_msk (int)                    - handle for integer array representing mask
   lo (1D array-like of integers) - low value of range on which operation
                                    is performed
   hi (1D array-like of integers) - hi value of range on which operation
                                    is performed
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

The unpack subroutine is designed to expand the values in the source vector g_src into a larger destination array g_dest based on the values in an integer mask array g_mask. The values lo and hi denote the range of elements that should be compressed and icount is a variable that on output lists the number of values placed in the uncompressed array. This operation is the complement of the GA_Pack operation. An example is shown below.

\begin{verbatim}
 GA_Unpack(g_src, g_dest, g_mask, 1, n, \&icount);

g_src:    1  7  9 12 15 16
g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_dest:   1  0  0  0  0  0  7  0  9  0  0 12  0  0 15 16  0
icount:   6
\end{verbatim}

This is a collective operation.

\end{desc}

\end{document}
