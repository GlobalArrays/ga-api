\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{color}
\usepackage{alltt}
\usepackage{underscore}
\usepackage{environ}
\usepackage{graphicx}
\include{preamble}

\begin{document}

\apih{INITIALIZE}{Initialize GA}


\begin{capi}
\begin{ccode}
void NGA_Initialize()
void GA_Initialize()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_initialize()
subroutine ga_initialize()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GA::Initialize(int argc, char *argv[], size_t limit)
\end{cxxcode}
\begin{funcargs}
\funcarg{int}{argc}{number of command line arguments}{input}
\funcarg{char**}{argv} {command line arguments}{input}
\funcarg{size_t}{limit}{amount of memory in bytes per process}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
from ga4py import ga
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Allocate and initialize internal data structures in Global Arrays.

This is a collective operation.

\end{desc}


\apih{INITIALIZE_LTD}{Initialize GA with memory limit}

\begin{capi}
\begin{ccode}
void GA_Initialize_ltd(size_t limit)
\end{ccode}
\begin{funcargs}
\funcarg{}{limit}{amount of memory in bytes per process}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_initialize_ltd(limit)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{limit}{amount of memory in bytes per process}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GA::Initialize(int argc, char *argv[], unsigned long heapSize,
               unsigned long stackSize, int type, size_t limit = 0)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{argc}{number of command line arguments}{input}
\funcarg{}{argv}{command line arguments}{input}
\funcarg{}{limit}{amount of memory in bytes per process}{input}
\funcarg{}{heapSize}{all of the dynamically allocated local memory}{input}
\funcarg{}{stackSize}{all of the dynamically allocated local memory}{input}
\funcarg{}{type}{data type}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
initialize_ltd(size_t limit)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Allocate and initialize internal data structures and set the limit for memory
used in Global Arrays. The limit is per process: it is the amount of memory
that the given processor can contribute to collective allocation of Global
Arrays. It does not include temporary storage that GA might be allocating (and
releasing) during execution of a particular operation.

$*limit < 0$ means ``allow unlimited memory usage" in which case this operation
is equivalent to GA_initialize.

This is a collective operation.

\end{desc}


\apih{PGROUP_CREATE}{Create a GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_create(int *list, int size)
\end{ccode}
\begin{funcargs}
\funcarg{}{list[size]}{list of processor IDs in group}{input}
\funcarg{size}{}{number of processors in group}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_create(list, size)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{size}{number of processors in group}{input}
\funcarg{integer}{list(size)}{list of processors in processor group}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
PGroup::PGroup(int *plist, int size)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{size}{number of processors in group}{input}
\funcarg{}{plist[size]}{list of processor IDs in group}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_create(list)
\end{pycode}
\end{pyapi}

\dcoll

\begin{desc}

  This command is used to create a processor group. At present, it
  must be invoked by all processors in the current default processor
  group. The list of processors use the indexing scheme of the default
  processor group. If the default processor group is the world group,
  then these indices are the usual processor indices. This function
  returns a process group handle that can be used to reference this
  group by other functions.

  This is a collective operation on the default processor group.

\end{desc}


\apih{PGROUP_DESTROY}{Destroy a GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_destroy(int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_pgroup_destroy(p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
PGroup::~PGroup()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_destroy(int pgroup)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

  This command is used to free up a processor group handle. It returns
  0 if the processor group handle was not previously active.

  This is a collective operation on the default processor group.

\end{desc}

\apih{PGROUP_SET_DEFAULT}{Set a default GA processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_set_default(int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_set_default(p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static void PGroup::setDefault(PGroup *p_handle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_set_default(int pgroup)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

  This function can be used to reset the default processor group on a
  collection of processors. All processors in the group referenced by
  p_handle must make a call to this function. Any standard global
  array call that is made after resetting the default processor group
  will be restricted to processors in that group. Global arrays that
  are created after resetting the default processor group will only be
  defined on that group and global operations, such as GA_Sync or
  GA_Igop, and will be restricted to processors in that group. The
  GA_Pgroup_set_default call can be used to rapidly convert large
  applications, written with GA, into routines that run on processor
  groups.

  The default processor group can be overridden by using GA calls that
  require an explicit group handle as one of the arguments.

  This is a collective operation on the group represented by the
  handle p_handle.

\end{desc}

\apih{CREATE}{Create a global array}

\begin{capi}
\begin{ccode}
int NGA_Create(int type, int ndim, int dims[], char *array_name, int chunk[])
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_create(type, dim1, dim2, array_name, chunk1, chunk2, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{MA type}{input}
\funcarg{integer}{dim1,dim2}{array (dim1,dim2) as in FORTRAN}{input}
\funcarg{integer}{chunk1,chunk2}{minimum size that dimensions should be chunked up into}{input}
\funcarg{integer}{g_a}{handle for future references}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_create(type, ndim, dims, array_name, chunk, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray* createGA(int type, int ndim, int dims[],
                                   char *arrayname, int chunk[])
GlobalArray * GAServices::createGA(int type, int ndim, int dims[],
                                   char *arrayname, int chunk[])
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type(MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create(int gtype, dims, char *name='', chunk=None, int pgroup=-1)
   gtype (int)                      - the type of the array
   dims (1D array-like of integers) - shape of the array
   name (string)                    - the name of the array
   chunk (1D array-like of integers)- see above
   pgroup (int)                     - create array only as part of
                                      this processor group
\end{pycode}
\end{pyapi}

\dcoll

\begin{desc}

  Creates an ndim-dimensional array using the regular distribution
  model and returns an integer handle representing the array.

  The array can be distributed evenly or not. The control over the
  distribution is accomplished by specifying chunk (block) size for
  all or some of array dimensions. For example, for a 2-dimensional
  array, setting chunk[0]=dim[0] gives distribution by vertical strips
  (chunk[0]*dims[0]); setting chunk[1]=dim[1] gives distribution by
  horizontal strips (chunk[1]*dims[1]). Actual chunks will be modified
  so that they are at least the size of the minimum and each process
  has either zero or one chunk. Specifying chunk[i] as less than 1 will cause
  that dimension to be distributed evenly.

  As a convenience, when chunk is specified as NULL, the entire array
  is distributed evenly.

  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.
\end{desc}


\apih{CREATE_CONFIG}{Create a GA with a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_config(int type, int ndim, int dims[], char *array_name,
                      int chunk[], int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{}{p_handle}{processor list handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_config(type, ndim, dims, array_name, chunk,
                                   p_handle, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], char *arrayname,
                         int chunk[],PGroup* p_handle)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], char *arrayname,
                         int64_t chunk[], PGroup* p_handle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type(MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{cxxapi}

\dcoll

\begin{desc}

  Creates an ndim-dimensional array using the regular distribution
  model but with an explicitly specified processor list handle and
  returns an integer handle representing the array.

  This call is essentially the same as the NGA_Create call, except for
  the processor list handle p_handle. It can be used to create
  mirrored arrays.

  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.

\end{desc}

\apih{CREATE_GHOSTS}{Create a GA with ghost cells}

\begin{capi}
\begin{ccode}
int NGA_Create_ghosts(int type, int ndim, int dims[], int width[],
                      char *array_name, int chunk[])
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts(type, ndim, dims, width, array_name, 
                                   chunk, g_a) 
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{width(ndim)}{array of ghost cell widths}{input}
\funcarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname, int chunk[], char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], int64_t width[],
                         char *arrayname, int64_t chunk[], char ghosts)
GlobalArray * GAServices::createGA_Ghosts(int type, int ndim, int dims[],
                                          int width[], char *array_name, 
                                          int chunk[])
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, in order to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_ghosts(int gtype, dims, width, char *name='', chunk=None, int pgroup=-1)
   gtype (int)                       - the type of the array
   dims (1D array-like of integers)  - shape of the array
   width (1D array-like of integers) - ghost cell widths
   name (string)                     - the name of the array
   chunk (1D array-like of integers) - see above
   pgroup (int)                      - create array only as part of this 
                                       processor group
\end{pycode}
\end{pyapi}

\dcoll

\begin{desc}

  Creates an ndim-dimensional array with a layer of ghost cells around
  the visible data on each processor using the regular distribution
  model and returns an integer handle representing the array.

  The array can be distributed evenly or not evenly. The control over
  the distribution is accomplished by specifying chunk (block) size
  for all or some of the array dimensions. For example, for a
  2-dimensional array, setting chunk(1)=dim(1) gives distribution by
  vertical strips (chunk(1)*dims(1)); setting chunk(2)=dim(2) gives
  distribution by horizontal strips (chunk(2)*dims(2)). Actual chunks
  will be modified so that they are at least the size of the minimum
  and each process has either zero or one chunk. Specifying chunk(i)
  as \textless 1 will cause that dimension (i-th) to be distributed evenly. The
  width of the ghost cell layer in each dimension is specified using
  the array width(). The local data of the global array residing on
  each processor will have a layer width[n] ghosts cells wide on
  either side of the visible data along the dimension n.

  Return value: a non-zero array handle means the call was successful.

  This is a collective operation.
\end{desc}


\apih{CREATE_GHOSTS_CONFIG}{Create a GA with ghost cells and specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_ghosts_config(int type, int ndim, int dims[], 
                             int width[], char *array_name, int chunk[], 
                             int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{}{p_handle}{processor list handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_config(type, ndim, dims, width, array_name, chunk, p_handle, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{width(ndim)}{array of ghost cell widths}{input}
\funcarg{integer}{chunk(ndim)}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[], 
                         char *arrayname, int chunk[], PGroup* p_handle, 
                         char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], int64_t width[],
                         char *arrayname, int64_t chunk[], PGroup* p_handle, 
                         char ghosts)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{chunk[ndim]}{array of chunks, each element specifies minimum size that given dimensions should be chunked up into}{input}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}{input}
\end{funcargs}
\end{cxxapi}

\dcoll

\begin{desc}

  Creates an ndim-dimensional array with a layer of ghost cells around
  the visible data on each processor using the regular distribution
  model and an explicitly specified processor list and returns an
  integer handle representing the array.

  This call is essentially the same as the NGA_Create_ghosts call,
  except for the processor list handle p_handle. It can be used to
  create mirrored arrays.

  Return value: a non-zero array handle means the call was successful.
  This is a collective operation.

\end{desc}

\apih{CREATE_IRREG}{Create an irregular-distributed GA}

\begin{capi}
\begin{ccode}
int NGA_Create_irreg(int type, int ndim, int dims[], char *array_name, 
                     int block[], int map[])
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims}{array of dimension values}{input}
\funcarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{map[s]}{starting index for for each block; the size s is a sum all elements of nblock array}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_create_irreg(type, dim1, dim2, array_name, map1, 
                                 nblock1, map2, nblock2, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{MA type}{input}
\funcarg{integer}{dim1,dim2}{array (dim1,dim2) as in FORTRAN}{input}
\funcarg{integer}{nblock1}{no. of blocks first dimension is divided into}{input}
\funcarg{integer}{nblock2}{no. of blocks second dimension is divided into}{input}
\funcarg{integer}{map1(*)}{ilo for each block}{input}
\funcarg{integer}{map2(*)}{jlo for each block}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_create_irreg(type, ndim, dims, array_name, map, 
                                  nblock, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}{input}
\funcarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray * GAServices::createGA(int type, int ndim, int dims[], 
                                   char *arrayname,
                                   int block[], int maps[])
GlobalArray::GlobalArray(int type, int ndim, int dims[], char *arrayname,
                         int block[],int maps[]);
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], 
                         char *arrayname, int64_t block[], 
                         int64_t maps[])
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims}{array of dimension values}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{block[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{maps[s]}{starting index for for each block; the size s is a sum all elements of nblock array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_irreg(int gtype, dims, block, map, char *name='', int pgroup=-1)
   gtype (int)                       - the type of the array
   dims (1D array-like of integers)  - shape of the array
   block (1D array-like of integers) - the number of blocks each dimension 
                                       is divided into
   map (1D array-like of integers)   - starting index for each block 
                                       len(map) == sum of all elements of 
                                       nblock array
   name (string)                     - the name of the array
   pgroup (int)                      - create array only as part of this 
                                       processor group
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  Creates an array by following the user-specified distribution and
  returns an integer handle representing the array.

  The distribution is specified as a Cartesian product of
  distributions for each dimension. The array indices start at 0. 
For example, Figure \ref{crirreg} demonstrates the distribution of a
2-dimensional 8x10 array on 6 (or more) processors.  

nblock[2]=\{3,2\}, the size of the map array is s=5 and the array map contains the following 

  elements map=\{0,2,6, 0, 5\}. The distribution is nonuniform because
  P1 and P4 get 20 elements each and processors P0, P2, P3, and P5 only
  10 elements each.


\begin{figure}
\includegraphics{CrIrreg}
\centering
\caption{Creating an Irregular Array}
\label{crirreg}
\end{figure}
 
  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.

\end{desc}


\apih{CREATE_IRREG_CONFIG}{Create an irregular-distributed GA with a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_irreg_config(int type, int ndim, int dims[], 
                            char *array_name, int block[], int map[], 
                            int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims}{array of dimension values}{input}
\funcarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{map[s]}{starting index for for each block; the size s is a sum all elements of nblock array}{input}
\end{funcargs}
   p_handle               - processor list handle
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_irreg_config(type, ndim, dims, array_name, map, nblock, p_handle, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}{input}
\funcarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::  GlobalArray(int type, int ndim, int dims[], 
                           char *arrayname, int block[],
                           int maps[], PGroup* p_handle)
GlobalArray::  GlobalArray(int type, int ndim, int64_t dims[], 
                           char *arrayname,
                           int64_t block[], int64_t maps[], 
                           PGroup* p_handle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{MA data type (MT_F_DBL,MT_F_INT,MT_F_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims}{array of dimension values}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{block[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{maps[s]}{starting index for for each block; the size s is a sum all elements of nblock array}{input}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{cxxapi}
\dcoll

\begin{desc}

  Creates an array by following the user-specified distribution and an
  explicitly specified processor list handle and returns an integer
  handle representing the array.

  This call is essentially the same as the NGA_Create_irreg call,
  except for the processor list handle p_handle. It can be used to
  create mirrored arrays.

  Return value: a non-zero array handle means the call was succesful.
  This is a collective operation.

\end{desc}


\apih{CREATE_GHOST_IRREG}{Create an irregular-distributed GA with ghost cells}

\begin{capi}
\begin{ccode}
int NGA_Create_ghost_irreg(int type, int ndim, int dims[], width[],
                           char *array_name, nblock[], map[])
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{map[s]}{starting index for for each block; the size     s is a sum of all elements of nblock array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_irreg(type, ndim, dims, width, array_name, map, nblock, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{width(ndim)}{array of ghost cell widths}{input}
\funcarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}{input}
\funcarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[], 
                         char *arrayname,
                         int block[], int maps[], char ghosts);
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], 
                         int64_t width[], char *arrayname,
                         int64_t block[], int64_t maps[], char ghosts)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{block[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{maps[s]}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_ghosts_irreg(int gtype, dims, width, block, map, char *name='', 
int pgroup=-1)
   gtype (int)                       - the type of the array
   dims (1D array-like of integers)  - shape of the array
   width (1D array-like of integers) - ghost cell widths
   block (1D array-like of integers) - number of blocks each dimension is 
                                       divided into
   map (1D array-like of integers)   - starting index for each block 
                                       len(map) == sum of all elements of 
                                       nblock array
   name (string)                     - the name of the array
   pgroup (int)                      - create array only as part of this 
                                       processor group
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  Creates an array with ghost cells by following the user-specified
  distribution and returns an integer handle representing the array.

  The distribution is specified as a Cartesian product of
  distributions for each dimension. 

Figure \ref{crghostir} demonstrates distribution of a 2-dimensional
array 8x10 on 6 (or more) processors. 

nblock(2)=\{3,2\}, the size of map array is s=5 and the array map contains
the following elements map=\{1,3,7, 1, 6\}. The distribution is nonuniform
because, P1 and P4 get 20 elements each and processors P0, P2, P3, and P5
only 10 elements each.
 
The array width[] is used to control the width of the ghost cell boundary
around the visible data on each processor. The local data of the Global Array
residing on each processor will have a layer width[n] ghosts cells wide on
either side of the visible data along the dimension n. 

Return value: a non-zero array handle means the call was succesful. 

This is a collective operation.

\begin{figure}
\includegraphics{CrGhostIr}
\centering
\caption{Creating an Array with Ghost Cells}
\label{crghostir}
\end{figure}

\end{desc}


\apih{CREATE_GHOSTS_IRREG_CONFIG}{Create an irregular-distributed GA with ghost cells and a specific configuration}

\begin{capi}
\begin{ccode}
int NGA_Create_ghost_irreg_config(int type, int ndim, int dims[],
                                  width [], char*array_name,nblock[],
                                  map[], int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a unique character string}{input}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{nblock[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{map[s]}{starting index for for each block; the size     s is a sum of all elements of nblock array}{input}
\funcarg{}{p_handle}{processor list handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_create_ghosts_irreg_config(type, ndim, 
                                                dims, width, array_name, 
                                                map, nblock, 
                                                p_handle, g_a)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a unique character string}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions}{input}
\funcarg{integer}{width(ndim)}{array of ghost cell widths}{input}
\funcarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}{input}
\funcarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{g_a}{integer handle for future references}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(int type, int ndim, int dims[], int width[],
                         char *arrayname, int block[], int maps[],
                         PGroup* p_handle, char ghosts)
GlobalArray::GlobalArray(int type, int ndim, int64_t dims[], 
                         int64_t width[], char *arrayname, 
                         int64_t block[], int64_t maps[],
                         PGroup* p_handle,char ghosts)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type (MT_DBL,MT_INT,MT_DCPL)}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{dims[ndim]}{array of dimensions}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\funcarg{}{arrayname}{a unique character string}{input}
\funcarg{}{block[ndim]}{no. of blocks each dimension is divided into}{input}
\funcarg{}{maps[s]}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{ghosts}{this is a dummy parameter: added to increase the number of arguments, inorder to avoid the conflicts among constructors. (ghosts = 'g' or 'G')}{input}
\end{funcargs}
\end{cxxapi}
\dcoll

\begin{desc}

Creates an array with ghost cells by following the user-specified distribution
and returns integer handle representing the array.

This call is essentially the same as the NGA_Create_ghosts_irreg call, except
for the processor list handle p_handle. It can be used to create mirrored arrays.

Return value: a non-zero array handle means the call was succesful.

This is a collective operation.

\end{desc}


\apih{CREATE_HANDLE}{Create a handle to a global array}

\begin{capi}
\begin{ccode}
int GA_Create_handle()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_create_handle()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_handle()
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

  This function returns a Global Array handle that can then be used to
  create a new Global Array. This is part of a new API for creating
  Global Arrays that is designed to replace the old interface built
  around the NGA_Create_xxx calls. The sequence of operations is to
  begin with a call to GA_Greate_handle to get a new array handle. The
  attributes of the array, such as dimension, size, type, etc., can
  then be set using successive calls to the GA_Set_xxx subroutines.
  When all array attributes have been set, the GA_Allocate subroutine
  is called and the Global Array is actually created and memory for it
  is allocated.

  This is a collective operation.

\end{desc}


\apih{SET_ARRAY_NAME}{Set the array name for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_array_name (int g_a, char *name)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{name}{array name}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_array_name(g_a, name)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{character*(*)}{name}{a unique character string}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setArrayName(char *name) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{name}{array name}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_array_name(int g_a, char *name)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to assign a unique character string name
  to a Global Array handle that was obtained using the
  GA_Create_handle function.

  This is a collective operation.

\end{desc}


\apih{SET_DATA}{Set the data properties for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_data (int g_a, int ndim, int dims[], int type)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{dimension of global array}{input}
\funcarg{}{dims[]}{dimensions of global array}{input}
\funcarg{}{type}{data type of global array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_data (g_a, ndim, dims, type)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{dimension of array}{input}
\funcarg{integer}{dims(ndim)}{array dimensions}{input}
\funcarg{integer}{type}{data type (MT_DBL,MT_INT,etc.)}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setData(int ndim, int dims[], int type) const
void GlobalArray::setData(int ndim, int64_t dims[], int type) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{ndim}{dimension of global array}{input}
\funcarg{}{dims}{dimensions of global array}{input}
\funcarg{}{type}{data type of global array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_data(int g_a, dims, int type)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the array dimension, the coordinate
  dimensions, and the data type assigned to a Global Array handle
  obtained using the GA_Create_handle function.

  This is a collective operation.
\end{desc}


\apih{SET_IRREG_DISTR}{Specify irregular distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_irreg_distr(int g_a, int mapc[], int nblock[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{mapc[s]}{starting index for each block; the size s is the sum of all elements of the array nblock}{input}
\funcarg{}{nblock[ndim]}{number of blocks that each dimension is divided into}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_irreg_distr(g_a, mapc, nblock)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{map(s)}{starting index for for each block; the size s is a sum of all elements of nblock array}{input}
\funcarg{integer}{nblock(ndim)}{no. of blocks each dimension is divided into}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setIrregDistr(int mapc[], int nblock[]) const
void GlobalArray::setIrregDistr(int64_t mapc[], int64_t nblock[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{mapc[s]}{starting index for each block; the size s is the sum of all elements of the array nblock}{input}
\funcarg{}{nblock[ndim]}{number of blocks that each dimension is divided into}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_irreg_distr(int g_a, mapc, nblock)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to partition the array data among the
  individual processors for a global array handle obtained using the
  GA_Create_handle function.

  The distribution is specified as a Cartesian product of
  distributions for each dimension. For example, the following figure
  demonstrates distribution of a 2-dimensional array 8x10 on 6 (or
  more) processors. nblock(2)=\{3,2\}, the size of mapc array is s=5 and
  array mapc contains the following elements mapc=\{1, 3, 7, 1, 6\}. The
  distribution is nonuniform because, P1 and P4 get 20 elements each
  and processors P0, P2, P3, and P5 only 10 elements each.

  The array width() is used to control the width of the ghost cell
  boundary around the visible data on each processor. The local data
  of the global array residing on each processor will have a layer
  width(n) ghosts cells wide on either side of the visible data along
  the dimension n.
An example is shown in Figure \ref{setirregdist}.

\begin{figure}
\centering
\includegraphics{SetIrregDist}
\caption{Set an Irregular Distribution}
\label{setirregdist}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{SET_PGROUP}{Set the processor group for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_pgroup(int g_a, int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_pgroup(g_a, p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setPGroup(PGroup *pHandle) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{pHandle}{processor group handle}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_pgroup(int g_a, int pgroup)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the processor configuration
  assigned to a global array handle that was obtained using the
  GA_Create_handle function. It can be used to create mirrored arrays
  by using the mirrored array processor configuration in this function
  call. It can also be used to create an array on a processor group by
  using a processor group handle in this call.

  This is a collective operation.

\end{desc}

\apih{SET_RESTRICTED}{Specify a GA handle to be allocated on a subset of processors}

\begin{capi}
\begin{ccode}
void GA_Set_restricted(int g_a, int list[], int nproc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{list[nproc]}{list of processor IDs that contain data}{input}
\funcarg{}{nproc}{number of processors that contain data}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_restricted(g_a, list, nproc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{list(nproc)}{list of processor IDs that contain data}{input}
\funcarg{integer}{nproc}{number of processors that contain data}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setRestricted(int list[], int nprocs) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{list}{list of processors that should contain data}{input}
\funcarg{}{nprocs}{number of processors in list}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

  This function restricts data in the global array g_a to only the
  nproc processors listed in the array list. The value of nproc must be
  less than or equal to the number of available processors. If this
  call is used in conjunction with GA_Set_irreg_distr, then the
  decomposition in the GA_Set_irreg_distr call must be done assuming
  that the number of processors is nproc. The data that ordinarily
  would be mapped to process 0 is mapped to the process in list[0],
  the data that would be mapped to process 1 will be mapped to
  list[1], etc. This can be used to remap the data distribution to
  different processors, even if nproc equals the number of available
  processors.

This is a collective operation.

\end{desc}

\apih{SET_RESTRICTED_RANGE}{Specify a GA handle to be created on a subset (as a range) of processors}

\begin{capi}
\begin{ccode}
void GA_set_restricted_range(int g_a, int lo_proc, int hi_proc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{lo_proc}{range of processors (inclusive) that contain data}{input}
\funcarg{}{hi_proc}{range of processors (inclusive) that contain data}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_restricted_range(g_a, lo_proc, hi_proc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{lo_proc}{range of processors (inclusive) that contain data}{input}
\funcarg{integer}{hi_proc}{range of processors (inclusive) that contain data}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setRestrictedRange(int lo_proc, int hi_proc) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo_proc}{low end of processor range}{input}
\funcarg{}{hi_proc}{high end of processor range}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

  This function restricts data in the global array to the range of
  processors beginning with lo_proc and ending with hi_proc. Both
  lo_proc and hi_proc must be less than or equal to the total number
  of processors minus one (e.g., in the range [0,N-1], where N is the
  total number of processors) and lo_proc must be less than or equal
  to hi_proc. If lo_proc = 0 and hi_proc = N-1 then this call has no
  effect on the data distribution.

  This is a collective operation.

\end{desc}

\apih{SET_GHOSTS}{Specify ghost cells for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_ghosts(int g_a, int width[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_ghosts(g_a, width)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{width(ndim)}{array of ghost cell widths}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setGhosts(int width[]) const
void GlobalArray::setGhosts(int64_t width[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{width[ndim]}{array of ghost cell widths}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_ghosts(int g_a, width)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to set the ghost cell widths for a global
  array handle that was obtained using the GA_Create_handle function.
  The ghosts cells widths indicate how many ghost cells are used to
  pad the locally held array data along each dimension. The padding
  can be set independently for each coordinate dimension.

  This is a collective operation.

\end{desc}

\apih{SET_CHUNK}{Specify chunk size for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_chunk(int g_a, int chunk[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{chunk[]}{array of chunk widths}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_chunk(g_a, chunk)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{chunk(ndim)}{array of chunk widths}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setChunk(int chunk[]) const
void GlobalArray::setChunk(int64_t chunk[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{chunk}{array of chunk widths}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_chunk(int g_a, chunk)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function is used to set the chunk array for a global array
  handle that was obtained using the GA_Create_handle function. The
  chunk array is used to determine the minimum number of array
  elements assigned to each processor along each coordinate direction.

  This is a collective operation.

\end{desc}

\apih{SET_BLOCK_CYCLIC}{Specify round-robin distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_block_cyclic(int g_a, int dims[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{dims[]}{array of block dimensions}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_block_cyclic(g_a, dims)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{dims(ndim)}{array of block dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setBlockCyclic(int dims[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{dims}{array of block dimensions}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_block_cyclic(int g_a, dims)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This subroutine is used to create a global array with a simple
  block-cyclic data distribution. The array is broken up into blocks
  of size dims and each block is numbered sequentially using a column
  major indexing scheme. The blocks are then assigned in a simple
  round-robin fashion to processors. 

Figure \ref{stblkcy} illustrates an array containing 25 blocks distributed on 4 processors.

  Blocks at the edge of the array may be smaller than the block size
  specified in dims. In the example below, blocks
  4, 9, 14, 19, 20, 21, 22, 23, and 24 might be smaller than the remaining
  blocks. Most global array operations are insensitive to whether or
  not a block-cyclic data distribution is used, although performance
  may be slower in some cases if the global array is using a
  block-cyclic data distribution. Individual data blocks can be
  accessesed using the block-cyclic access functions.

\begin{figure}
\centering
\includegraphics{StBlkCy}
\caption{Set Block Cyclic Data Distribution}
\label{stblkcy}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{SET_BLOCK_CYCLIC_PROC_GRID}{Specify block-cyclic processor distribution for a GA handle}

\begin{capi}
\begin{ccode}
void GA_Set_block_cyclic(int g_a, int dims[], int proc_grid[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{dims[]}{array of block dimensions}{input}
\funcarg{}{proc_grid[]}{processor grid dimensions}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_block_cyclic_proc_grid(g_a, dims, proc_grid)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{dims(ndim)}{array of block dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setBlockCyclicProcGrid(int dims[], int proc_grid[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{dims}{array of block dimensions}{input}
\funcarg{}{proc_grid}{processor grid dimensions}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_block_cyclic_proc_grid(int g_a, block, proc_grid)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This subroutine is used to create a global array with a
  SCALAPACK-type block cyclic data distribution. The user specifies
  the dimensions of the processor grid in the array proc_grid. The
  product of the processor grid dimensions must equal the number of
  total number of processors and the number of dimensions in the
  processor grid must be the same as the number of dimensions in the
  global array. The data blocks are mapped onto the processor grid in
  a cyclic manner along each of the processor grid axes. 

Figure \ref{setblkcyprocgrid} illustrates an array consisting of 25 data blocks distributed on 6 processors.

The 6 processors are configured in a 3 by 2 processor grid. Blocks at
the edge of the array may be smaller than the block size specified in dims.
Most global array operations  are insensitive to whether or not a block-cyclic
data distribution is used, although performance may be slower in some cases
if the global array is using a block-cyclic data distribution. Individual data
blocks can be accessesed using the block-cyclic access functions.

\begin{figure}
\centering
\includegraphics{SetBlkCyProcGrid}
\caption{Creating a SCALAPACK-type Block Cyclic Data Distribution}
\label{setblkcyprocgrid}
\end{figure}

  This is a collective operation.

\end{desc}

\apih{ALLOCATE}{Allocate the array specified by a GA handle}

\begin{capi}
\begin{ccode}
int GA_Allocate(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{ga}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_allocate(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::allocate() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
allocate(int g_a)
   g_a (int)                         - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function allocates the memory for the global array handle
  originally obtained using the GA_Create_handle function. At a
  minimum, the GA_Set_data function must be called before the memory
  is allocated. Other GA_Set_xxx functions can also be called before
  invoking this function.

Returns True if allocation of g_a was successful.

  This is a collective operation.

\end{desc}

\apih{UPDATE_GHOSTS}{Update ghost cells}

\begin{capi}
\begin{ccode}
void GA_Update_ghosts(int g_a)
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_update_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::updateGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
update_ghosts(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This call updates the ghost cell regions on each processor with the
  corresponding neighbor data from other processors. The operation
  assumes that all data is wrapped around using periodic boundary data
  so that ghost cell data that goes beyound an array boundary is
  wrapped around to the other end of the array. The GA_Update_ghosts
  call contains two GA_Sync calls before and after the actual update
  operation. For some applications these calls may be unecessary, if
  so they can be removed using the GA_Mask_sync subroutine. 
This is a collective operation.

\end{desc}

\apih{UPDATE_GHOST_DIR}{Update ghost cells along a specific direction}

\begin{capi}
\begin{ccode}
int NGA_Update_ghost_dir(int g_a, int dimension, int idir, int cflag)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{dimension}{array dimension that is to be updated}{input}
\funcarg{}{idir}{direction of update (+/- 1)}{input}
\funcarg{}{cflag}{flag (0/1) to include corners in update}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function nga_update_ghost_dir(g_a,dimension,idir,cflag)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{dimension}{array dimension that is to be updated}{input}
\funcarg{integer}{idir}{direction of update (+/-1)}{input}
\funcarg{logical}{cflag}{flag to include corners in update}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::updateGhostDir(int dimension, int idir, int cflag) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{dimension}{array dimension that is to be updated}{input}
\funcarg{}{idir}{direction of update (+/- 1)}{input}
\funcarg{}{cflag}{flag (0/1) to include corners in update}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
update_ghost_dir(int g_a, int dimension, int dir, int flag)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

  This function can be used to update the ghost cells along individual
  directions. It is designed for algorithms that can overlap updates
  with computation. The variable dimension indicates which coordinate
  direction is to be updated (e.g. dimension = 1 would correspond to
  the y axis in a two or three dimensional system), the variable idir
  can take the values +/-1 and indicates whether the side that is to
  be updated lies in the positive or negative direction, and cflag
  indicates whether or not the corners on the side being updated are
  to be included in the update. The following calls would be
  equivalent to a call to GA_Update_ghosts for a 2-dimensional system:

\begin{verbatim}
     status = NGA_Update_ghost_dir(g_a,0,-1,1);
     status = NGA_Update_ghost_dir(g_a,0,1,1);
     status = NGA_Update_ghost_dir(g_a,1,-1,0);
     status = NGA_Update_ghost_dir(g_a,1,1,0);
\end{verbatim}

         The variable cflag is set equal to 1 (or non-zero) in the
         first two calls so that the corner ghost cells are update, it
         is set equal to 0 in the second two calls to avoid redundant
         updates of the corners. Note that updating the ghosts cells
         using several independent calls to the nga_update_ghost_dir
         functions is generally not as efficient as using
         GA_Update_ghosts unless the individual calls can be
         effectively overlapped with computation.

This is a  collective operation.

\end{desc}

\apih{HAS_GHOSTS}{Check whether a GA has ghost cells}

\begin{capi}
\begin{ccode}
int GA_Has_ghosts(int g_a)
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_has_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::hasGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
has_ghosts(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This function returns 1 if the global array has some dimensions for
which the ghost cell width is greater than zero, it returns 0 otherwise.
This is a collective operation.

\end{desc}

\apih{ACCESS_GHOSTS}{Access the ghost cells allocated locally on a GA}

\begin{capi}
\begin{ccode}
void NGA_Access_ghosts(int g_a, int dims[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{dims[ndim]}{array of dimensions of local patch, including ghost cells}{output}
\funcarg{}{ptr}{returns an index corresponding to the origin the global array patch held locally on the processor}{output}
\funcarg{}{ld[ndim-1]}{physical dimenstions of the local array patch, including ghost cells}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_ghosts(g_a, dims, index, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{dims(ndim)}{array of dimensions of local patch, including ghost cells}{output}
\funcarg{integer}{index}{returns an index corresponding to the origin the global array patch held locally on the processor}{output}
\funcarg{integer}{ld(ndim)}{physical dimenstions of the local array patch, including ghost cells}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessGhosts(int dims[], void *ptr, int ld[]) const
void GlobalArray::accessGhosts(int64_t dims[], void *ptr, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{dims[ndim]}{array of dimensions of local patch, including ghost cells}{output}
\funcarg{}{ptr}{returns an index corresponding to the origin the global array patch held locally on the processor}{output}
\funcarg{}{ld[ndim-1]}{physical dimensions of the local array patch, including ghost cells}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_ghosts(int g_a)
\end{pycode}
\begin{funcargs}
\funcarg{g_a}{(int)}{the array handle}{input}
\end{funcargs}
\end{pyapi}
\local

\begin{desc}

  Provides access to the local patch of the global array. Returns
  leading dimension ld and and pointer for the data.  This routine
  will provide access to the ghost cell data residing on each
  processor. Calls to NGA_Access_ghosts should normally follow a call
  to NGA_Distribution that returns coordinates of the visible data
  patch associated with a processor. You need to make sure that the
  coordinates of the patch are valid (test values returned from
  NGA_Distribution).

You can only access local data.
This is a local operation.

\end{desc}

\apih{ACCESS_GHOST_ELEMENT}{Access a specific ghost element locally allocated on a GA}

\begin{capi}
\begin{ccode}
void NGA_Access_ghost_element(int g_a, void *ptr, int subscript[],
                              int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{index}{index pointing to location of element indexed by subscript[]}{output}
\funcarg{}{subscript[ndim]}{array of integers that index desired element}{input}
\funcarg{}{ld[ndim-1]}{array of strides for local data patch.  These include ghost cell widths.}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_ghost_element(g_a, index, subscript, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{index}{index pointing to location of element indexed by subscript()}{output}
\funcarg{integer}{subscript(ndim)}{array of integers that index desired element}{input}
\funcarg{integer}{ld(ndim-1)}{array of strides for local data patch. These include ghost cell widths.}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessGhostElement(void *ptr, int subscript[],
int ld[]) const
void GlobalArray::accessGhostElement(void *ptr, int64_t subscript[],
int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{ptr}{index pointing to location of element indexed by subscript[]}{output}
\funcarg{}{subscript[ndim]}{array of integers that index desired element}{input}
\funcarg{}{ld[ndim-1]}{array of strides for local data patch.  These include ghost cell widths.}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_ghost_element(int g_a, subscript, ld)
   g_a (int)                             - the array handle
   subscript (1D array-like of integers) - index of the desired element
\end{pycode}
\end{pyapi}
\local

\begin{desc}

  This function can be used to return a pointer to any data element in
  the locally held portion of the global array and can be used to
  directly access ghost cell data. The array subscript refers to the
  local index of the element relative to the origin of the local patch
  (which is assumed to be indexed by (0,0,...)). 

  This is a local operation.

\end{desc}

\apih{TOTAL_BLOCKS}{Number of blocks allocated when using block-cyclic distribution}

\begin{capi}
\begin{ccode}
int GA_Total_blocks(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_total_blocks(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::totalBlocks() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
total_blocks(int g_a)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the total number of blocks contained in a global array
with a block-cyclic data distribution. This is a local operation.

\end{desc}

\apih{GET_BLOCK_INFO}{Information on block-cyclic information for a GA}

\begin{capi}
\begin{ccode}
void GA_Get_block_info(int g_a, int num_blocks[], int block_dims[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{num_blocks[ndim]}{number of blocks along each axis}{output}
\funcarg{}{block_dims[ndim]}{dimensions of block}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get_block_info(g_a, num_blocks, block_dims)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{num_blocks(ndim)}{number of blocks along each axis}{output}
\funcarg{integer}{block_dims(ndim)}{dimensions of block}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::getBlockInfo(int num_blocks[], int block_dims[])
\end{cxxcode}
\begin{funcargs}
\funcarg{}{num_blocks[ndim]}{array containing number of blocks along each coordinate direction}{output}
\funcarg{}{block_dims[ndim]}{array containing block dimensions}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_block_info(int g_a)
   g_a (int)            - the array handle
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This subroutine returns information about the block-cyclic distribution
associated with global array g_a. The number of blocks along each of the
array axes are returned in the array num_blocks and the dimensions of the
individual blocks, specified in the GA_Set_block_cyclic or 
GA_Set_block_cyclic_proc_grid subroutines, are returned in block_dims. 
This is a local function.

\end{desc}

\apih{DUPLICATE}{Duplicate a GA}

\begin{capi}
\begin{ccode}
int GA_Duplicate(int g_a, char* array_name)
\end{ccode}
\begin{funcargs}
\funcarg{}{array_name}{a character string}{input}
\funcarg{}{g_a}{integer handle for reference array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_duplicate(g_a, g_b, array_name)
\end{fcode}
\begin{funcargs}
\funcarg{character*(*)}{array_name}{a character string}{input}
\funcarg{integer}{g_a}{Integer handle for reference array}{input}
\funcarg{integer}{g_b}{Integer handle for new array}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::GlobalArray(const GlobalArray &g_a, char *arrayname)
GlobalArray::GlobalArray(const GlobalArray &g_a)
GlobalArray * GAServices::createGA(const GlobalArray *g_b, char *arrayname)
GlobalArray * GAServices::createGA(const GlobalArray &g_b)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_b}{integer handle for reference array}{input}
\funcarg{}{arrayname}{a character string}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
duplicate(int g_a, char *name='')
   g_a (int)     - the array handle
   name (string) - the new name of the created array
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Creates a new array by applying all the properties of another existing array. 
It returns an array handle.

Return value: a non-zero array handle means the call was succesful.
This is a collective operation.

\end{desc}

\apih{DESTROY}{Destroy a global array}

\begin{capi}
\begin{ccode}
void GA_Destroy(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_destroy(g_a)  
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GlobalArray::~GlobalArray()
void GlobalArray::destroy()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
destroy(int g_a)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Deallocates the array and frees any associated resources.

This is a collective operation.

\end{desc}

\apih{TERMINATE}{Terminate GA}

\begin{capi}
\begin{ccode}
void GA_Terminate()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_terminate()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GA::Terminate()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
terminate()
\end{pycode}
\end{pyapi}
\wcoll

\begin{desc}

Delete all active arrays and destroy internal data structures.

This is a collective operation.

\end{desc}

\apih{SYNC}{Synchronize all processes in the default processor group}

\begin{capi}
\begin{ccode}
void GA_Sync()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_sync()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::sync()
GA::sync()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
sync()
\end{pycode}
\end{pyapi}
\dcoll

\begin{desc}

Synchronize processes (a barrier) and ensure that all GA operations completed.

This is a collective operation.

\end{desc}

\apih{MASK_SYNC}{Mask GA synchronization operations}

\begin{capi}
\begin{ccode}
void GA_Mask_sync(int first,int last)
\end{ccode}
\begin{funcargs}
\funcarg{}{first}{mask (0/1) for prior internal synchronization}{input}
\funcarg{}{last}{mask (0/1) for post internal synchronization}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_mask_sync(first,last)
\end{fcode}
\begin{funcargs}
\funcarg{logical}{first}{mask for prior internal synchronization}{input}
\funcarg{logical}{last}{mask for post internal synchronization}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::maskSync(int first, int last)
void GA::maskSync(int first, int last)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{first}{masks the sync at the begining of the collective call}{input}
\funcarg{}{last}{masks the sync at the end of the collective call}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
mask_sync(int first, int last)
   first (bool)   - mask for prior internal synchronization
   last (bool)    - mask for post internal synchronization
\end{pycode}
\end{pyapi}
\dcoll
\begin{desc}

This subroutine can be used to remove synchronization calls from around collective
operations. Setting the parameter first = .false. removes the synchronization prior
to the collective operation, setting last = .false. removes the synchronization call
after the collective operation. This call is applicable to all collective operations. 
It most be invoked before each collective operation.
This is a  collective operation.

\end{desc}

\apih{ZERO}{Zero a global array}

\begin{capi}
\begin{ccode}
void GA_Zero(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_zero(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zero() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
zero(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Sets value of all elements in the array to zero.

This is a collective operation.

\end{desc}

\apih{FILL}{Fill a global array with a specific value}

\begin{capi}
\begin{ccode}
void GA_Fill(int g_a, void *value)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_fill(g_a, s)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double precision/complex/integer}{s}{fill value}{input} 
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::fill(void *value) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type.}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
fill(int g_a, value, lo=None, hi=None)
   g_a (int)                      - the array handle
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Assign a single value to all elements in the array.

This is a collective operation.

\end{desc}

\apih{DOT}{Dot product of two global arrays}

\begin{capi}
\begin{ccode}
int GA_Idot(int g_a, int g_b) 
long GA_Ldot(int g_a, int g_b) 
float GA_Fdot(int g_a, int g_b) 
double GA_Ddot(int g_a, int g_b) 
double complex GA_Zdot(int g_a, int g_b) 
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{g_b}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
double precision function ga_ddot(g_a, g_b)
double complex function ga_zdot(g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{g_b}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::idot(const GlobalArray * g_a) const
long GlobalArray::ldot(const GlobalArray * g_a) const
float GlobalArray::fdot(const GlobalArray * g_a) const
double GlobalArray::ddot(const GlobalArray * g_a) const
double complex GlobalArray::zdot(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{GlobalArray}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
dot(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None, int ta=False, 
int tb=False)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   alo (1D array-like of integers) - lower bound patch coordinates of g_a, 
                                     inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a, 
                                     exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of g_b, 
                                     inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b, 
                                     exclusive
   ta (bool)                       - whether the transpose operator should 
                                     be applied to g_a True=applied
   tb (bool)                       - whether the transpose operator should 
                                     be applied to g_b True=applied
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the element-wise dot product of the two arrays which must be of the same types and same number of elements.
      Return value = SUM_ij a(i,j)*b(i,j)


This is a collective operation.

\end{desc}

\apih{SCALE}{Scale a global array with a specified value}

\begin{capi}
\begin{ccode}
void GA_Scale(int g_a, void *value)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{value}{pointer to the value of appropriate type (double/double complex/long) that matches array type}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scale(g_a, s) 
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double precision/complex/integer}{s}{TODO}{input} 
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scale(void *value) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{value}{pointer to the value of appropriate type}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scale(int g_a, value, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales an array by the constant s. Note that the library is unable to detect
errors when the pointed value is of a different type than the array.

This is a collective operation.

\end{desc}

\apih{ADD}{Add corresponding values in two global arrays}

\begin{capi}
\begin{ccode}
void GA_Add(void *alpha, int g_a, void* beta, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{g_b}{array handle}{input}
\funcarg{}{g_c}{array handle}{input}
\funcarg{double/complex/int*}{alpha}{scale factor}{input}
\funcarg{double/complex/int*}{beta}{scale factor}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add(alpha, g_a, beta, g_b, g_c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{g_b}{array handle}{input}
\funcarg{integer}{g_c}{array handle}{input}
\funcarg{double precision/complex/integer}{alpha,beta}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::add(void *alpha, const GlobalArray * g_a, void *beta, const GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{alpha}{scale factor}{input}
\funcarg{}{g_a}{array}{input}
\funcarg{}{beta}{scale factor}{input}
\funcarg{}{g_b}{array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add(int g_a, int g_b, int g_c, alpha=None, beta=None, alo=None, ahi=None,
blo=None, bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle
   g_b (int)                       - the array handle
   g_c (int)                       - the array handle
   alpha (object)                  - multiplier (converted to appropriate type)
   beta (object)                   - multiplier (converted to appropriate type)
   alo (1D array-like of integers) - lower bound patch coordinates of 
                                     g_a, inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of 
                                     g_a, exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of 
                                     g_b, inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of 
                                     g_b, exclusive
   clo (1D array-like of integers) - lower bound patch coordinates of 
                                     g_c, inclusive
   chi (1D array-like of integers) - higher bound patch coordinates of 
                                     g_c, exclusive
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

The arrays (which must be the same shape and identically aligned)
are added together element-wise.

\begin{verbatim}
        c = alpha * a  +  beta * b;
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).

This is a collective operation.

\end{desc}

\apih{COPY}{Copy a global array}

\begin{capi}
\begin{ccode}
void GA_Copy(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{g_b}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_copy(g_a, g_b) 
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{g_b}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::copy(const GlobalArray *g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{GlobalArray to copy}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
copy(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None, int trans=False)
   g_a (int)                       - the array handle copying from
   g_b (int)                       - the array handle copying to
   alo (1D array-like of integers) - lower bound patch coordinates of 
                                     g_a, inclusive
   ahi (1D array-like of integers) - higher bound patch coordinates of 
                                     g_a, exclusive
   blo (1D array-like of integers) - lower bound patch coordinates of 
                                     g_b, inclusive
   bhi (1D array-like of integers) - higher bound patch coordinates of 
                                     g_b, exclusive
   trans (bool)                    - whether the transpose operator should
                                     be applied True=applied
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Copies elements in array represented by g_a into the array represented by g_b.
The arrays must be the same type, shape, and identically aligned.

For patch operations, the patches of arrays may be of different shapes but must
have the same number of elements. Patches must be nonoverlapping (if g_a=g_b). 
Transposes are allowed for patch operations.

This is a collective operation.

\end{desc}

\apih{SET_MEMORY_LIMIT}{Limit the internal memory used by the GA runtime}

\begin{capi}
\begin{ccode}
void GA_Set_memory_limit(size_t limit)
\end{ccode}
\begin{funcargs}
\funcarg{}{limit}{the amount of memory in bytes per process}{input}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_set_memory_limit(limit)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{limit}{the amount of memory in bytes per process}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setMemoryLimit(size_t limit);
\end{cxxcode}
\begin{funcargs}
\funcarg{}{limit}{the amount of memory in bytes per process}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_memory_limit(size_t limit)
   limit (size_t) - the amount of memory in bytes per process 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Sets the amount of memory to be used (in bytes) per process

This is a local operation.

\end{desc}

\apih{GET}{Get data from a global array}

\begin{capi}
\begin{ccode}
void NGA_Get(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get(g_a, ilo, ihi, jlo, jhi, buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\funcarg{double precision/complex/integer}{buf}{TODO}{output}
\funcarg{integer}{ld}{leading dimension}{input}
\end{funcargs}
\end{fapi}

\begin{fapi}
\begin{fcode}
subroutine nga_get(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data goes  to}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\end{funcargs}
\end{fapi}

\begin{pyapi}
\begin{pycode}
get(int g_a, lo=None, hi=None, ndarray buffer=None) 
   g_a (int)                      - the array handle 
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive 
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive 
   buffer, ,                      - an ndarray of the appropriate type,
                                    large enough to hold lo,hi 
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

  Copies data from global array section to the local array buffer. The
  local array is assumed to be have the same number of dimensions as
  the global array. Any detected inconsistencies or errors in the input
  arguments are fatal.

Example: For the ga_get operation transfering data from the [10:14, 0:4]
section of 2-dimensional 15x10 global array into a local buffer 5x10
array we have: 

\begin{verbatim}
lo={10,0,} hi={14,4}, ld={10}  
\end{verbatim}

Figure \ref{get} shows the GET operation.

\begin{figure}
\centering
\includegraphics{GET}
\caption{Copying Data from 2-dimensional 15x10 Global Array into Local Buffer 5x10 Array}
\label{get}
\end{figure}

This is a one-sided operation.
Return: The local array buffer.

 \end{desc}

\apih{PERIODIC_GET}{Get, with periodic boundary conditions, data from a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_get(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_get(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data goes  to}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicGet(int lo[], int hi[], void* buf, int ld[]) const
void GlobalArray::periodicGet(int64_t lo[], int64_t hi[], void* buf,
								  int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
periodic_get(int g_a, lo, hi, buffer, alpha=None)
   g_a (int)                      - the array handle 
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive 
   hi (array-like of integers)    - higher bound patch coordinates, exclusive 
   buffer (array-like)            - must be contiguous and have same number of
                                    elements as patch
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

Same as nga_get except the indices can extend beyond the array boundary/dimensions
in which case the library wraps them around.
This is a one-sided operation.
The local array is assumed to be have the same number of dimensions as the
global array. Any detected inconsitencies/errors in the input arguments are fatal.

Returns: The local Array buffer. 

\end{desc}

\apih{STRIDED_GET}{Get strided data from a global array }

\begin{capi}
\begin{ccode}
void NGA_Strided_get(int g_a, int lo[], int hi[], int skip[], 
                     void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_get(g_a, lo, hi, skip, buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{integer}{skip(ndim)}{array of strides for each dimension}{input}
\funcarg{type}{buf}{local buffer array where the data comes from}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedGet(int lo[], int hi[], int skip[],
                             void *buf, int ld[]) const
void GlobalArray::stridedGet(int64_t lo[], int64_t hi[], int64_t skip[],
                             void *buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for glob array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to local buffer array where data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{cxxapi}
\begin{pyapi}
\begin{pycode}
strided_get(int g_a, lo=None, hi=None, skip=None, ndarray buffer=None) 
   g_a (int)                        - the array handle 
   lo (1D array-like of integers)   - lower bound patch coordinates, inclusive 
   hi (1D array-like of integers)   - higher bound patch coordinates, exclusive 
   skip (1D array-like of integers) - strides for each dimension 
   buffer (ndarray)                 - an ndarray of the appropriate type,
                                      large enough to hold lo,hi 
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

This operation is the same as NGA_Get, except that the values corresponding to
dimension n in buf correspond to every skip[n] values of the global array g_a. 
This is a one-sided operation.
The local array is assumed to be have the same number of dimensions as the 
global array. Any detected inconsitencies/errors in the input arguments are fatal.

Returns: The local array buffer. 
\end{desc}

\apih{PUT}{Put data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Put(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{output}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data is}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_put(g_a, ilo, ihi, jlo, jhi, buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\funcarg{double precision/complex/integer}{buf}{TODO}{output}
\funcarg{integer}{ld}{}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_put(g_a, lo, hi, buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data comes from}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::put(int lo[], int hi[], void *buf, int ld[]) const
void GlobalArray::put(int64_t lo[], int64_t hi[], void *buf, 
                      int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data is}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
put(int g_a, buffer, lo=None, hi=None)
    g_a (int)                      - the array handle
    buffer (array-like)            - the data to put
    lo (1D array-like of integers) - lower bound patch coordinates, inclusive
    hi (array-like of integers)    - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Copies data from the local array buffer to the global array section. The
local array is assumed to have the same number of dimensions as the global array.
Any detected inconsistencies or errors in input arguments are fatal.

This is a one-sided operation.

\end{desc}

\apih{PERIODIC_PUT}{Put, with periodic boundary conditions, data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_put(int g_a, int lo[], int hi[], void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{output}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data is}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_put(g_a, lo, hi,  buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data comes from}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicPut(int lo[], int hi[], void* buf, int ld[]) const
void GlobalArray::periodicPut(int64_t lo[], int64_t hi[], void* buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
periodic_put(int g_a, buffer, lo=None, hi=None)
   g_a (int)                      - the array handle
   buffer (array-like)            - the data to put
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (array-like of integers)    - higher bound patch coordinates, exclusive
\end{pycode}
\end{pyapi}
\ncoll
\begin{desc}

Same as nga_put except the indices can extend beyond the array boundary/dimensions
in which case the library wraps them around.
The indices can extend beyond the array boundary/dimensions in which case the 
libray wraps them around.
Copies data from local array buffer to the global array section.
The local array is assumed to be have the same number of dimensions as the 
global array. Any detected inconsitencies/errors in input arguments are fatal.
This is a one-sided operation.

\end{desc}

\apih{STRIDED_PUT}{Put strided data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Strided_put(int g_a, int lo[], int hi[], int skip[], 
                     void* buf, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_put(g_a, lo, hi, skip, buf, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array}{input}
\funcarg{integer}{skip(ndim)}{array of strides for each dimension}{input}
\funcarg{type}{buf}{local buffer array where the data comes from}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for array section}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedPut(int lo[], int hi[], int skip[],
                             void*buf, int ld[]) const
void GlobalArray::stridedPut(int64_t lo[], int64_t hi[], int64_t skip[],
                             void *buf, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for glob array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to local buffer array where data goes}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
strided_put(int g_a, buffer, lo=None, hi=None, skip=None)
   g_a (int)                        - the array handle
   buffer (array-like)              - the data to put
   lo (1D array-like of integers)   - lower bound patch coordinates, inclusive
   hi (array-like of integers)      - higher bound patch coordinates, exclusive
   skip (1D array-like of integers) - strides for each dimension
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Strided version of put.  This operation is the same as NGA_Put, except 
that the values corresponding to dimension n in buf are copied to every 
skip[n] values of the global array g_a. 

Copies data from local array buffer to the global array section.

The local array is assumed to be have the same number of dimensions as 
the global array. 

Any detected inconsitencies/errors in input arguments are fatal.

This is a one-sided operation.

\end{desc}

\apih{ACC}{Accumulate data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Acc(int g_a, int lo[], int hi[], void* buf, int ld[], 
             void* alpha)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{buf}{pointer to the local buffer array}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{double/double complex/long*}{alpha}{scale factor}{input} 
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_acc(g_a, ilo, ihi, jlo, jhi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\funcarg{double precision/complex}{buf}{TODO}{input}
\funcarg{integer}{ld}{}{input}
\funcarg{double precision/complex}{alpha}{TODO}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_acc(g_a, lo, hi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the local data is}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\funcarg{type}{alpha}{scale argument for accumulate}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::acc(int lo[], int hi[], void *buf,
                      int ld[], void *alpha) const
void GlobalArray::acc(int64_t lo[], int64_t hi[], void *buf,
                      int64_t ld[], void *alpha) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{buf}{pointer to the local buffer array}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{}{alpha}{scale factor (double/double complex/long *)}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
acc(int g_a, buffer, lo=None, hi=None, alpha=None)
   g_a (int)           - the array handle
   buffer (array-like) - must be contiguous and have same number of 
                         elements as patch
   lo (1D array-like)  - lower bound patch coordinates, inclusive
   hi (1D array-like)  - higher bound patch coordinates, exclusive
   alpha (object)      - multiplier (converted to appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

 Combines data from local array buffer with data in the global array section. 
The local array is assumed to be have the same number of dimensions as the 
global array.

If the buffer is not contiguous, a contiguous copy will be made.

    global array section (lo[],hi[]) += *alpha * buffer

This is a one-sided and atomic operation.

\end{desc}

\apih{PERIODIC_ACC}{Accumulate, with periodic boundary conditions, data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Periodic_acc(int g_a, int lo[], int hi[], void* buf, int ld[], 
                      void* alpha)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{buf}{pointer to the local buffer array}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{double/double complex/long*}{alpha}{scale factor}{input} 
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_periodic_acc(g_a, lo, hi, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the local data is}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\funcarg{type}{alpha}{scale argument for accumulate}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::periodicAcc(int lo[], int hi[], void* buf,
                              int ld[], void* alpha) const
void GlobalArray::periodicAcc(int64_t lo[], int64_t hi[], void* buf,
                              int64_t ld[], void* alpha) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{buf}{pointer to the local buffer array}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{}{alpha}{double/double complex/long scale factor}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
   g_a (int)                      - the array handle
   buffer (array-like)            - must be contiguous and have same 
                                    number of elements as patch
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive
   hi (array-like of integers)    - higher bound patch coordinates, exclusive
   alpha (object)                 - multiplier (converted to the
                                    appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Same as nga_acc except the indices can extend beyond the array boundary/dimensions 
in which case the library wraps them around. For Python, this is the periodic 
version of ga.acc.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as the 
global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a one-sided and atomic operation.

\end{desc}

\apih{STRIDED_ACC}{Accumulate strided data into a global array}

\begin{capi}
\begin{ccode}
void NGA_Strided_acc(int g_a, int lo[], int hi[], int skip[], void* buf, 
                     int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{double/DoublComplex/long*}{alpha}{scale factor}{input} 
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_strided_acc(g_a, lo, hi, skip, buf, ld, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo(ndim)}{array of starting indices for global array section}{input}
\funcarg{integer}{hi(ndim)}{array of ending indices for global array section}{input}
\funcarg{integer}{skip(ndim)}{array of strides for each dimension}{input}
\funcarg{type}{buf}{local buffer array where the data comes from}{output}
\funcarg{integer}{ld(ndim-1)}{array specifying leading dimensions for buffer array}{input}
\funcarg{type}{alpha}{scale argument for accumulate}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stridedAcc(int lo[], int hi[], int skip[], void *buf,
                             int ld[], void *alpha) const;
void GlobalArray::stridedAcc(int64_t lo[], int64_t hi[], int64_t skip[], 
                             void *buf, int64_t ld[], void *alpha) const;
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for glob array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{skip[ndim]}{array of strides for each dimension}{input}
\funcarg{}{buf}{pointer to local buffer array where data goes}{input}
\funcarg{}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{}{alpha}{double/DoublComplex/long scale factor}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
strided_acc(int g_a, buffer, lo=None, hi=None, skip=None, alpha=None)
Parameters:
   g_a (int)                      - the array handle
   buffer (array-like)            - must be contiguous and have same number 
                                    of elements as patch
   lo (1D array-like of integers) - lower bound patch coordinates, 
                                    inclusive
   hi (1D array-like of integers) - higher bound patch coordinates, 
                                    exclusive
   alpha (object)                 - multiplier (converted to the 
                                    appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

This operation is the same as NGA_Acc, except that the values corresponding 
to dimension n in buf are accumulated to every skip[n] values of the global array g_a. 

For Python this is the strided version of ga.acc.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as 
the global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a one-sided and atomic operation.


\end{desc}

\apih{DISTRIBUTION}{Inquire data range on a specified processor}

\begin{capi}
\begin{ccode}
void NGA_Distribution(int g_a, int iproc, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{iproc}{process number}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_distribution(g_a, iproc, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{iproc}{process number}{input}
\funcarg{integer}{ilo,ihi,jlo,jhi}{range held by process iproc}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_distribution(g_a, iproc, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{iproc}{process number}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{lo(ndim),hi(ndim)}{range held by process iproc}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::distribution(int me, int* lo, int* hi) const
void GlobalArray::distribution(int me, int64_t* lo, int64_t* hi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{iproc}{process number}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
distribution(int g_a, int iproc=-1)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

If no array elements are owned by process iproc, the range is returned as 
lo[ ]=0 and hi[ ]= -1 for all dimensions.
This operation is local.


Return the distribution given to iproc. If iproc is not specified, then 
ga.nodeid() is used. The range is returned as -1 for lo and -2 for hi if 
no elements are owned by iproc.

\end{desc}

\apih{COMPARE_DISTR}{Compare distribution of two global arrays}

\begin{capi}
\begin{ccode}
int GA_Compare_distr(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_compare_distr(g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::compareDistr(const GlobalArray *g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{GlobalArray to compare against}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
compare_distr(int g_a, int g_b) 
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Compares distributions of two global arrays. Returns 0 if distributions 
are identical and 1 when they are not.

This is a collective operation.

\end{desc}

\apih{ACCESS}{Access data locally allocated for a global array}

\begin{capi}
\begin{ccode}
void NGA_Access(int g_a, int lo[], int hi[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{ptr}{points to location of first element in patch}{output}
\funcarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}{output}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_access(g_a, ilo, ihi, jlo, jhi, index, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\funcarg{integer}{index}{}{output}
\funcarg{integer}{ld}{}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_access(g_a, lo, hi, index, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{lo(ndim),hi(ndim)}{patch specification}{input}
\funcarg{integer}{index}{reference to local data}{output}
\funcarg{integer}{ld(ndim-1)}{array of leading dimensions}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::access(int lo[], int hi[], void *ptr, int ld[]) const
void GlobalArray::access(int64_t lo[], int64_t hi[], void *ptr, int64_t ld[]) cons
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{ptr}{points to location of first element in patch}{output}
\funcarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access(int g_a, lo=None, hi=None) 
   g_a (int)          - the array handle 
   lo (1D array-like) - lower bound patch coordinates, inclusive 
   hi (1D array-like) - higher bound patch coordinates, exclusive 
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Provides access to the specified patch of a global array. Returns array 
of leading dimensions ld and a pointer to the first element in the patch. 
This routine allows to access directly, in place elements in the local 
section of a global array. It useful for writing new GA operations. A call 
to ga_access normally follows a previous call to ga_distribution that 
returns coordinates of the patch associated with a processor. You need 
to make sure that the coordinates of the patch are valid (test values 
returned from ga_distribution).

Each call to ga_access has to be followed by a call to either ga_release 
or ga_release_update. You can access in this fashion only local data. 
Since the data is shared with other processes, you need to consider issues 
of mutual exclusion.
This operation is local.

Note: The entire local data is always accessed, but if a smaller patch is 
requested, an appropriately sliced ndarray is returned.

Returns: ndarray representing local block 

\end{desc}


\begin{fdesc}

  Provides access to the specified patch of array. Returns leading
  dimension ld and and MA-like index for the data. This routine is
  intended for writing new GA operations. Call to ga_access should
  normally follow a call to ga_distribution that returns coordinates
  of the patch associated with a processor. You need to make sure that
  the coordinates of the patch are valid (test values returned from
  ga_distribution).

Your code should include a MA include file, mafdecls.h. 
\begin{verbatim}       
          dbl_mb(index)  - for double precision data
          int_mb(index)  - for integer data
          dcpl_mb(index) - for double complex data
\end{verbatim}

The addressing convention refers the first element \verb|(ilo,jlo)|
of the patch. However, you can only pass that reference to another
subroutine where it could be used like a normal array, see the
following example. This constraint caused by the HP fortran compiler
inability to reference shared memory data properly. The C interface
has no such restrictions.

Example

For a given subroutine:
\begin{verbatim}
          subroutine foo(A,  nrows, ncols lda)
          double precision A(lda,*)
          integer nrows, ncols
             ....
          end
\end{verbatim}
you can reference A(ilo:ihi,jlo:jhi) in the following way:

\begin{verbatim}
          call foo(dbl_mb(index), ihi-ilo+1, jhi-jlo+1, lda)
\end{verbatim}
\end{fdesc}


\apih{ACCESS_BLOCK_SEGMENT}{Access local data for a specific global array block}

\begin{capi}
\begin{ccode}
void NGA_Access_block_segment(int g_a, int proc, void *ptr, int len)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{proc}{processor ID}{input}
\funcarg{}{ptr}{pointer to locally held data}{output}
\funcarg{}{len}{length of data on processor}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block_segment(g_a, proc, index, len)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{proc}{processor ID}{input}
\funcarg{integer}{index}{reference to local data}{output}
\funcarg{integer}{len}{length of data on processor}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlockSegment(int index, void *ptr, int *len) const
void GlobalArray::accessBlockSegment(int index, void *ptr, 
                                     int64_t *len) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index}{processor ID}{input}
\funcarg{}{ptr}{points to location of first element}{output}
\funcarg{}{len}{length of locally held data}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_block(int g_a, int idx) 
Do not use.
   g_a (int)        - the array handle 
   proc (int)       - processor ID 
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This function can be used to gain access to the all the locally held 
data on a particular processor that is associated with a block-cyclic 
distributed array. Once the index has been returned, local data can be 
accessed as described in the documentation for NGA_Access. The parameter 
len is the number of data elements that are held locally. The data  
inside this segment has a lot of additional structure so this function 
is not generally useful to developers. It is primarily used inside the 
GA library to implement other GA routines. Each call to ga_access_block_segment 
should be followed by a call to either NGA_Release_block_segment or 
NGA_Release_update_block_segment.

This is a local operation.

Returns: ndarray representing local block 

\end{desc}

\apih{ACCESS_BLOCK}{Access a block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Access_block(int g_a, int idx, int index, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{idx}{block index}{input}
\funcarg{}{index}{pointer to locally held block}{output}
\funcarg{}{ld[ndim-1]}{array of leading dimensions}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block(g_a, idx, index, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{idx}{block index}{input}
\funcarg{integer}{index}{reference to local data}{output}
\funcarg{integer}{ld(ndim-1)}{array of leading dimensions}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlock(int idx, void *ptr, int ld[]) const
void GlobalArray::accessBlock(int64_t idx, void *ptr, int64_t ld[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{idx}{index of block}{input}
\funcarg{}{ptr}{points to location of first element in patch}{output}
\funcarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
   g_a (int)  - the array handle 
   idx (int)  - the block index 
\end{pycode}
\end{pyapi}

\local

\begin{desc}

This function can be used to gain direct access to the data represented 
by a single block in a global array with a block-cyclic data distribution. 
The index idx is the index of the block in the array assuming that blocks 
are numbered sequentially in a column-major order. A quick way of determining 
whether a block with index idx is held locally on a processor is to calculate 
whether mod(idx,nproc) equals the processor ID, where nproc is the total number 
of processors. Once the index has been returned, local data can be accessed as 
described in the documentation for NGA_Access. Each call to ga_access_block 
should be followed by a call to either NGA_Release_block or NGA_Release_update_block.

This is a local operation.

Returns: ndarray representing local block  
\end{desc}

\apih{ACCESS_BLOCK_GRID}{Access data block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Access_block_grid(int g_a, int subscript[], void *ptr, int ld[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{subscript[ndim]}{subscript of block in array}{input}
\funcarg{}{ptr}{pointer to locally held bloc}{output}
\funcarg{}{ld[ndim-1]}{array of leading dimensions}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_access_block_grid(g_a, subscript, index, ld)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{subscript(ndim)}{subscript of block in array}{input}
\funcarg{integer}{index}{reference to local data}{output}
\funcarg{integer}{ld(ndim-1)}{array of leading dimensions}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::accessBlockGrid(int index[], void *ptr, int ld[]) const
void GlobalArray::accessBlockGrid(int64_t index[], void *ptr, int64_t ld[])
                                  const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index[ndim]}{indices of block in processor grid}{input}
\funcarg{}{ptr}{points to location of first element in patch}{output}
\funcarg{}{ld[ndim-1]}{leading dimensions for the pacth elements}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
access_block_grid(int g_a, subscript) 
   g_a (int)                 - the array handle 
   subscript (1D array-like) - subscript of the block in the array 
\end{pycode}
\end{pyapi}

\local

\begin{desc}

This function can be used to gain direct access to the data represented by 
a single block in a global array with a SCALAPACK block-cyclic data distribution 
that is based on an underlying processor grid. The subscript array contains the 
subscript of the block in the array of blocks. This subscript is based on the 
location of the block in a grid, each of whose dimensions is equal to the number 
of blocks that fit along that dimension. Once the index has been returned, local 
data can be accessed as described in the documentation for NGA_Access. Each call 
to ga_access_block_grid should be followed by a call to either NGA_Release_block_grid 
or NGA_Release_update_block_grid.

This is a local operation.

Returns: ndarray representing local block 

\end{desc}

\apih{RELEASE}{Release access to a global array}

\begin{capi}
\begin{ccode}
void NGA_Release(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_release(g_a, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_release(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{lo(ndim),hi(ndim)}{patch specification}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::release(int lo[], int hi[]) const
void GlobalArray::release(int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release(int g_a, lo=None, hi=None)  
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to a global array when the data was read only.

Your code should look like:
\begin{verbatim}
        NGA_Distribution(g_a, myproc, lo,hi);
        NGA_Access(g_a, lo, hi, \&ptr, ld);
           
             <operate on the data referenced by ptr> 
        GA_Release(g_a, lo, hi);
\end{verbatim}
NOTE: see restrictions specified for ga_access.

This operation is local.

\end{desc}

\apih{RELEASE_UPDATE}{Release access to a global array after an update}

\begin{capi}
\begin{ccode}
void NGA_Release_update(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_release_update(g_a, ilo, ihi, jlo, jhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo, ihi, jlo, jhi}{}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{integer}{lo(ndim),hi(ndim)}{patch specification}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdate(int lo[], int hi[]) const
void GlobalArray::releaseUpdate(int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update(int g_a, lo=None, hi=None)
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the data. It must be used if the data was accessed for writing. 
NOTE: see restrictions specified for ga_access.
This operation is local.
\end{desc}

\apih{RELEASE_BLOCK}{Release access to a block of a global array}

\begin{capi}
\begin{ccode}
void NGA_Release_block(int g_a, int index)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{index}{block index}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block(g_a, index)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{index}{block index}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlock(int index) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index}{block index}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block(int g_a, int index)  
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of data specified by the integer index 
when data was accessed as read only. This is only applicable to 
block-cyclic data distributions created using the simple block-cyclic 
distribution. This is a local operation.

\end{desc}

\apih{RELEASE_UPDATE_BLOCK}{Release after update access to a block in a global array}

\begin{capi}
\begin{ccode}
void NGA_Release_update_block(int g_a, int index)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{index}{block index}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block(g_a, index)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{index}{block index}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlock(int index) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index}{block index}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block(int g_a, int index) 
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Releases access to the block of data specified by the integer index 
when data was accessed in read-write mode. This is only applicable 
to block-cyclic data distributions created using the simple block-cyclic 
distribution. This is a local operation.
\end{desc}

\apih{RELEASE_BLOCK_GRID}{Release access to a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Release_block_grid(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{subscript[ndim]}{indices of block in array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block_grid(g_a, subscript)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{subscript(ndim)}{indices of block in array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlockGrid(int index[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index[ndim]}{indices of block in array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block_grid(int g_a, subscript) 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of data specified by the subscript array 
when data was accessed as read only. This is only applicable to 
block-cyclic data distributions created using the SCALAPACK data 
distribution. This is a local operation.

\end{desc}

\apih{RELEASE_UPDATE_BLOCK_GRID}{Release after update access to a block in a block-cyclic distributed global array}

\begin{capi}
\begin{ccode}
void NGA_Release_update_block_grid(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{subscript[ndim]}{indices of block in array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block_grid(g_a, subscript)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{subscript(ndim)}{indices of block in array}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlockGrid(int index[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{index[ndim]}{indices of block in array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block_grid(int g_a, subscript) 
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Releases access to the block of data specified by the subscript array 
when data was accessed in read-write mode. This is only applicable to 
block-cyclic data distributions created using the SCALAPACK data 
distribution. This is a local operation.

\end{desc}

\apih{RELEASE_BLOCK_SEGMENT}{Release access to a block in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_block_segment(int g_a, int iproc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{iproc}{processor ID}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_block_segment(g_a, iproc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{iproc}{processor ID}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseBlockSegment(int proc) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{proc}{process ID/rank}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_block_segment(int g_a, int iproc) 
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Releases access to the block of locally held data for a block-cyclic 
array, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE_UPDATE_BLOCK_SEGMENT}{Release access to a block of a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_block_segment(int g_a, int iproc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{iproc}{processor ID}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_block_segment(g_a, iproc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{iproc}{processor ID}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateBlockSegment(int proc) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{proc}{process ID/rank}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_block_segment(int g_a, int iproc) 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Releases access to the block of locally held data for a block-cyclic 
array, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE_GHOST_ELEMENT}{Release access to ghost cells in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_ghost_element(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{subscript[ndim]}{element subscript}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_ghost_element(g_a, subscript)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{subscript(ndim)}{element subscript}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseGhostElement(int subscript[]) const
void GlobalArray::releaseGhostElement(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{subscript[ndim]}{indices of element}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_ghost_element(int g_a, subscript)  
\end{pycode}
\end{pyapi} 
\local

\begin{desc}

Releases access to the locally held data for an array with ghost 
elements, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE_UPDATE_GHOST_ELEMENT}{Release after update access to ghost cells in a GA}

\begin{capi}
\begin{ccode}
void NGA_Release_update_ghost_element(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{subscript[ndim]}{element subscript}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_ghost_element(g_a, subscript)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{subscript(ndim)}{element subscript}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateGhostElement(int subscript[]) const
void GlobalArray::releaseUpdateGhostElement(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{subscript[ndim]}{indices of element}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_ghost_element(int g_a, subscript) 
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Releases access to the locally held data for an array with ghost elements, 
when data was accessed in read-write mode. This is a local operation.

\end{desc}

\apih{RELEASE_GHOSTS}{Release access to ghost cells}

\begin{capi}
\begin{ccode}
void NGA_Release_ghosts(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_ghosts(int g_a)  
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Releases access to the locally held block of data containing ghost 
elements, when data was accessed as read-only. This is a local operation.

\end{desc}

\apih{RELEASE_UPDATE_GHOSTS}{Release after access to ghosts}

\begin{capi}
\begin{ccode}
void NGA_Release_update_ghosts(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_release_update_ghosts(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::releaseUpdateGhosts() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
release_update_ghosts(int g_a) 
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Releases access to the locally held block of data containing ghost 
elements, when data was accessed in read-write mode. 
This is a local operation.

\end{desc}

\apih{READ_INC}{Atomically read and increment an element in a global array}

\begin{capi}
\begin{ccode}
long NGA_Read_inc(int g_a, int subscript[], long inc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{subscript[ndim]}{subscript array for the referenced element}{input}
\funcarg{}{inc}{amount element is incremented after read}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
integer function ga_read_inc(g_a, i, j, inc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{i, j, inc}{}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
integer function nga_read_inc(g_a, subscript, inc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{subscript}{(ndim)}{subscript array for the referenced element}{input}
\funcarg{inc}{}{amount element is incremented after read}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
long GlobalArray::readInc(int subscript[], long inc)
long GlobalArray::readInc(int64_t subscript[], long inc)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{subscript[ndim]}{subscript array for the referenced element}{input}
\funcarg{}{inc}{amount element is incremented after read}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
read_inc(int g_a, subscript, long inc=1)  
   g_a (int)       - the array handle 
   subscript (1D array-like of integers) - index for the referenced element 
   inc (long)      - the increment 
\end{pycode}
 \end{pyapi} 

\ncoll
\begin{desc}

Atomically read and increment an element in an integer array.

\begin{verbatim}
   *BEGIN CRITICAL SECTION*
   old_value = a(subscript)
   a(subscript) += inc
   *END CRITICAL SECTION*
   return old_value
\end{verbatim}

This is a one-sided and atomic operation.

\end{desc}

\apih{SCATTER}{Scatter elements into a global array}

\begin{capi}
\begin{ccode}
void NGA_Scatter(int g_a, void *v, int* subsArray[], int n)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray[n][ndim]}{array of subscripts for each element}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scatter(g_a, v, i, j, n)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision}{v(n)}{TODO}{input}
\funcarg{integer}{i(n), j(n), n}{TODO}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scatter(g_a, v, subsArray, n)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{n}{number of elements}{input}
\funcarg{type}{v(n)}{array containing values}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray(ndim,n)}{array of subscripts for each element}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scatter(void *v, int *subsarray[], int n) const
void GlobalArray::scatter(void *v, int64_t *subsarray[], int64_t n) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{subsarray[n][ndim]}{array of subscripts for each element}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scatter(int g_a, values, subsarray)  
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Scatters array elements into a global array. The contents of the input 
arrays (v,subscrArray) are preserved, but their contents might be 
(consistently) shuffled on return. 

For(k=0; k<= n; k++)\{\ a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]... = v[k];\}\ 
 

subsarray will be converted to an ndarray if it is not one already. 
A two-dimensional array is allowed so long as its shape is (n,ndim) 
where n is the number of elements to gather and ndim is the number 
of dimensions of the target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:

for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]

This is a one-sided operation.

\end{desc}

\apih{GATHER}{Gather elements from a global array}

\begin{capi}
\begin{ccode}
void NGA_Gather(int g_a, void *v, int* subsArray[], int n)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray[n][ndim]}{array of subscripts for each element}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_gather(g_a, v, i, j, n)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision}{v(n)}{}{output} 
\funcarg{integer}{i(n), j(n), n}{}{input} 
\end{funcargs}
\end{f2dapi}
\begin{fapi}
\begin{fcode}
subroutine nga_gather(g_a, v, subsArray, n)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{n}{number of elements}{input}
\funcarg{type}{v(n)}{array containing values}{output}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray(ndim,n)}{array of subscripts for each element}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::gather(void *v, int * subsarray[], int n) const
void GlobalArray::gather(void *v, int64_t * subsarray[], int64_t n) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{subsarray[n][ndim]}{array of subscripts for each element}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
gather(int g_a, subsarray, ndarray values=None)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

Gathers array elements from a global array into a local array. The 
contents of the input arrays (v, subscrArray) are preserved, but their 
contents might be (consistently) shuffled on return.
 
 
for (k=0; k<= n; k++)\{\\v[k] = a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]...;\}\    

subsarray will be converted to an ndarray if it is not one already. 
A two-dimensional array is allowed so long as its shape is (n,ndim) 
where n is the number of elements to gather and ndim is the number 
of dimensions of the target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:


for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]



This is a one-sided operation.

\end{desc}

\apih{SCATTER_ACC}{Scatter accumulate elements into a global array}

\begin{capi}
\begin{ccode}
void NGA_Scatter_acc(int g_a, void *v, int* subsArray[], int n, void *alpha)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray[n][ndim]}{array of subscripts for each element}{input}
\funcarg{}{alpha}{multiplicative factor}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scatter_acc(g_a, v, i, j, n, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{n}{number of elements}{input}
\funcarg{type}{v(n)}{array containing value}{input}
\funcarg{integer}{i(n),j(n)}{arrays of indices}{input}
\funcarg{double precision/complex}{alpha}{multiplicative value}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scatter_acc(g_a, v, subsArray, n, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{n}{number of elements}{input}
\funcarg{type}{v(n)}{array containing value}{input}
\funcarg{integer}{ndim}{number of array dimensions}{input}
\funcarg{}{subsArray(ndim,n)}{array of subscripts}{input}
\funcarg{double precision/complex}{alpha}{multiplicative value}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scatterAcc(void *v, int *subsarray[], int n, void *alpha) const
void GlobalArray::scatterAcc(void *v, int64_t *subsarray[], int64_t n, void *alpha) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{v[n]}{array containing values}{input}
\funcarg{}{subsarray[n][ndim]}{array of subscripts for each element}{input}
\funcarg{}{alpha}{multiplicative factor}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scatter_acc(int g_a, values, subsarray, alpha=None) 
\end{pycode}
\end{pyapi}

\ncoll

\begin{desc}

Scatters array elements from a local array into a global array. Adds 
values from the local array to existing values in the global array 
after multiplying by alpha. The contents of the input arrays (v, subscrArray) 
are preserved, but their contents might be (consistently) shuffled on return.
   
for(k=0; k<= n; k++)\{\\v[k] = a[subsArray[k][0]][subsArray[k][1]][subsArray[k][2]]...;\}\   

Like scatter, but adds values to existing values in the global array after 
multiplying by alpha.

subsarray will be converted to an ndarray if it is not one already. A 
two-dimensional array is allowed so long as its shape is (n,ndim) where n is 
the number of elements to gather and ndim is the number of dimensions of the 
target array. Also, subsarray must be contiguous.

For example, if the subsarray were two-dimensional:

for k in range(n):

    v[k] = g_a[subsarray[k,0],subsarray[k,1],subsarray[k,2]...]

For example, if the subsarray were one-dimensional:


for k in range(n):

    base = n*ndim

    v[k] = g_a[subsarray[base+0],subsarray[base+1],subsarray[base+2]...]

This is a one-sided operation.

\end{desc}

\apih{ERROR}{Abort with an error}

\begin{capi}
\begin{ccode}
void GA_Error(char *message, int code)
\end{ccode}
\begin{funcargs}
\funcarg{}{message}{string to print}{input}
\funcarg{}{code}{code to print}{input}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_error(message, code)
\end{fcode}
\begin{funcargs}
\funcarg{character*1}{message(*)}{}{input} 
\funcarg{integer}{code}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::error(const char *message, int code)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{message}{string to print}{input}
\funcarg{}{code}{code to print}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
error(char *message, int code=1)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

To be called in case of an error. Print an error message and an 
integer value that represents error code. Releases some system 
resources. This is the required way of aborting the program execution.
This operation is local.

\end{desc}

\apih{LOCATE}{Locate the processor containing a specified element of a global array}

\begin{capi}
\begin{ccode}
int NGA_Locate(int g_a, int subscript[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{subscript[ndim]}{element subscript}{output}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_locate(g_a, i, j, owner)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{i, j}{element subscript}{input}
\funcarg{integer}{owner}{process id}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_locate(g_a, subscript, owner)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{subscript}{element subscript}{input}
\funcarg{integer}{owner}{process id}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::locate(int subscript[]) const
int GlobalArray::locate(int64_t subscript[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{subscript[ndim]}{element subscript}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
locate(int g_a, subscript)
   g_a (int)                             - the array handle 
   subscript (1D array-like of integers) - len(subscript) should be ndim
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Return in owner the GA compute process ID that `owns' the data. If any 
element of subscript[] is out of bounds ``-1" is returned.
This operation is local.

\end{desc}

\apih{LOCATE_REGION}{Locate a region of a global array}

\begin{capi}
\begin{ccode}
int NGA_Locate_region(int g_a, int lo[], int hi[], int map[], int procs[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{ndim}{number of dimensions of the global array}{input}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{map[][2*ndim]}{array with mapping information}{output}
\funcarg{}{procs[nproc]}{list of processes that own a part of array section}{output}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
logical function ga_locate_region(g_a, ilo, ihi, jlo, jhi, map, np)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, ilo, ihi, jlo, jhi}{TODO}{input}
\funcarg{integer}{map(5,*)}{TODO}{output}
\funcarg{integer}{np}{TODO}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
logical function nga_locate_region(g_a, lo, hi, map, proclist, np)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{lo(ndim),hi(ndim)}{region(patch) specifications}{input}
\funcarg{integer}{map(2*ndim,*)}{patch ownership array}{output} 
\funcarg{integer}{proclist(np)}{list of processes}{output}
\funcarg{integer}{np}{number of processes}{output}
\funcarg{}{map(1:ndim,i)}{contains lower bound dimensions for part owned by process proclist(i)}{input} 
\funcarg{}{map(ndim+1:2*ndim,i)}{contains upper bound dimensions for part owned by process proclist(i)}{input} 
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::locateRegion(int lo[], int hi[], int map[], int procs[]) const;
int GlobalArray::locateRegion(int64_t lo[], int64_t hi[], int64_t map[], int procs[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{array of starting indices for array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for array section}{input}
\funcarg{}{map[][2*ndim]}{array with mapping information}{output}
\funcarg{}{procs[nproc]}{list of processes that own a part of selection}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
locate_region(int g_a, lo, hi) 
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Return the list of the GA processes ID that `own' the data. Parts of the 
specified patch might be actually `owned' by several processes. If lo/hi 
are out of bounds ``0" is returned, otherwise the return value is equal to 
the number of processes that hold the data.
  
\begin{verbatim}
     map[i][0:ndim-1]         - lo[i]
     map[i][ndim:2*ndim-1]    - hi[i]
     procs[i]                 - processor id that owns data in patch 
                                lo[i]:hi[i]
\end{verbatim}

This operation is local.

\end{desc}

\apih{INQUIRE}{Inquire the type and size of a global array}

\begin{capi}
\begin{ccode}
void NGA_Inquire(int g_a, int *type, int *ndim, int dims[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{type}{data type}{output}
\funcarg{}{ndim}{number of dimensions}{output}
\funcarg{}{dims}{array of dimensions}{output}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_inquire(g_a, type, dim1, dim2)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{type}{}{output}
\funcarg{integer}{dim1}{}{output}
\funcarg{integer}{dim2}{}{output}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_inquire(g_a, type, ndim, dims)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{type}{data type id}{output}
\funcarg{integer}{ndim}{number of dimensions}{output}
\funcarg{integer}{dims(ndim)}{array of dimensions}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::inquire(int *type, int *ndim, int dims[]) const
void GlobalArray::inquire(int *type, int *ndim, int64_t dims[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{type}{data type}{output}
\funcarg{}{ndim}{number of dimensions}{output}
\funcarg{}{dims}{array of dimensions}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
 inquire(int g_a) 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Returns data type and dimensions of the array.
This operation is local. 

\apih{INQUIRE_MEMORY}{Inquire the memory used by global arrays on the calling processor}

\begin{capi}
\begin{ccode}
size_t GA_Inquire_memory()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_inquire_memory()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
size_t GAServices::inquireMemory()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
inquire_memory() 
\end{pycode}
\end{pyapi} 


Returns amount of memory (in bytes) used in the allocated global arrays on 
the calling processor.
This operation is local.

\end{desc}

\apih{INQUIRE_NAME}{Inquire a global array's name}

\begin{capi}
\begin{ccode}
char* GA_Inquire_name(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_inquire_name(g_a, array_name)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{character*(*)}{array_name}{}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
char* GlobalArray::inquireName() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
inquire_name(int g_a) 
   g_a (int) - the array handle
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns the name of an array represented by the handle g_a.
This operation is local.

\end{desc}

\apih{NDIM}{Inquire the number of dimensions in a global array}

\begin{capi}
\begin{ccode}
int GA_Ndim(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_ndim(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::ndim() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
ndim(int g_a)  
   g_a (int) - the array handle 
\end{pycode}
\end{pyapi}


\local

\begin{desc}

Returns the number of dimensions in array represented by the handle g_a.
This operation is local.

\end{desc}

\apih{NBLOCK}{Inquire the number of blocks along each dimension of a global array}

\begin{capi}
\begin{ccode}
void GA_Nblock(int g_a, int nblock[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{nblock[ndim]}{number of partitions for each dimension}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_nblock(g_a, nblock)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{nblock[ndim]}{number of partitions for each dimension}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nblock(int nblock[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{nblock[ndim]}{number of partitions for each dimension}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nblock(int g_a) 
   g_a (int)             - array handle
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Given a distribution of an array represented by the handle g_a,
returns the number of partitions of each array dimension. This
operation is local.
\end{desc}

\apih{MEMORY_AVAIL}{Inquire the memory available on the invoking processor to allocate global arrays}

\begin{capi}
\begin{ccode}
size_t GA_Memory_avail()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_memory_avail()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::memoryAvailable() ;
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
memory_avail() 
\end{pycode}
\end{pyapi}

\local

\begin{desc}

Returns amount of memory (in bytes) left for allocation of new global
arrays on the calling processor.

Note: If GA_uses_ma returns true, then GA_Memory_avail returns the lesser 
of the amount available under the GA limit and the amount available from MA 
(according to ma_inquire_avail operation). If no GA limit has been set, it 
returns what MA says is available.

If ( ! GA_Uses_ma() \&\ \&\ ! GA_Memory_limited() ) returns $< 0$, indicating 
that the bound on currently available memory cannot be determined.
This operation is local.
\end{desc}

\apih{USES_MA}{Check whether GA uses MA}

\begin{capi}
\begin{ccode}
int GA_Uses_ma()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_uses_ma()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::usesMA()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
uses_ma()
\end{pycode}
\end{pyapi} 
 
\local

\begin{desc}

Returns ``1" if memory in global arrays comes from the Memory Allocator (MA). 
``0" means that memory comes from another source, for example System V shared 
memory is used.
This operation is local.

TODO
\end{desc}

\apih{MEMORY_LIMITED}{Check whether memory available to GA's runtime is limited}

\begin{capi}
\begin{ccode}
int GA_Memory_limited()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_memory_limited()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::memoryLimited()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
memory_limited() 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

Indicates if limit is set on memory usage in Global Arrays on the calling processor. 
``1" means ``yes", ``0" means ``no".
This operation is local.

Returns: 
True for "yes", False for "no"
\end{desc}

\apih{PROC_TOPOLOGY}{Inquire the linear location of a processor in the processor topology employed by a global array}

\begin{capi}
\begin{ccode}
void NGA_Proc_topology(int g_a, int proc, int coordinates[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{ndim}{number of array dimensions}{input}
\funcarg{}{proc}{process id}{input}
\funcarg{}{coordinates[ndim]}{coordinates in processor grid}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_proc_topology(g_a, proc, prow, pcol)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{proc}{}{input}
\funcarg{integer}{prow, pcol}{}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::procTopology(int proc, int coord[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{proc}{process id}{input}
\funcarg{}{coord[ndim]}{coordinates in processor grid}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
proc_topology(int g_a, int proc)
\end{pycode}
\end{pyapi}
 

\local

\begin{desc}

Based on the distribution of an array associated with handle g_a, 
determines coordinates of the specified processor in the virtual 
processor grid corresponding to the distribution of array g_a. The 
numbering starts from 0. The values of -1 means that the processor 
doesn't ``own" any section of the array represented by g_a.

This operation is local.
\end{desc}

\apih{PRINT_FILE}{Print the contents of a global array to a file}

\begin{capi}
\begin{ccode}
void GA_Print_file(FILE *file, int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{file}{file pointer}{input}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printFile(FILE *file) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{file}{file pointer}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_file(int g_a, file)
   file (file-like) - file-like object which must implement fileno(), 
                      or a string 
   g_a (int)        - the array handle 
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Prints an entire array to a file.

This is a collective operation.
\end{desc}

\apih{PRINT_PATCH}{Print a patch of a global array to a file}

\begin{capi}
\begin{ccode}
void NGA_Print_patch(int g_a, int lo[],int hi[],int pretty)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{lo[],hi[]}{coordinates of the patch}{input}
\funcarg{}{pretty}{formatting flag}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_print_patch(g_a,ilo,ihi,jlo,jhi,pretty)   
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ilo,ihi,jlo,jhi}{}{input}
\funcarg{integer}{pretty}{}{input}
\end{funcargs}
\end{f2dapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printPatch(int* lo, int* hi, int pretty) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{low coordinates of the patch}{input}
\funcarg{}{hi}{high coordinates of the patch}{input}
\funcarg{}{pretty}{formatting flag}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_patch(int g_a, lo=None, hi=None, int pretty=True)
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Prints a patch of g_a array to the standard output. If the variable 
pretty has the value 0 then output is printed in a dense fashion. If 
pretty has the value 1 then output is formatted and rows/columns are labeled.

This is a collective operation.
\end{desc}

\apih{PRINT}{Print the contents of a global array}

\begin{capi}
\begin{ccode}
void GA_Print(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print(g_a)   
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::print() const
\end{cxxcode}
\end{cxxapi}


\gcoll

\begin{desc}

Prints an entire array to the standard output.

This is a collective operation.
\end{desc}

\apih{PRINT_STATS}{Print GA runtime statistics}

\begin{capi}
\begin{ccode}
void GA_Print_stats()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print_stats()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::printStats()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_stats()  
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This non-collective (MIMD) operation prints information about:
\begin{itemize}
    \item Number of calls to the GA create/duplicate, destroy, get, 
     put, scatter, gather, and read_and_inc operations
    \item Total amount of data moved in the GA primitive operations
    \item Amount of data moved in GA primitive operations to logicaly 
     remote locations
    \item Maximum memory consumption in global arrays, and
    \item Number of requests serviced in the interrupt-driven implementations 
     by the calling process.
\end{itemize}

This operation is local.
\end{desc}

\apih{PRINT_DISTRIBUTION}{Print the distribution of a global array}

\begin{capi}
\begin{ccode}
void GA_Print_distribution(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_print_distribution(g_a)   
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::printDistribution() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
print_distribution(int g_a) 
\end{pycode}
\end{pyapi} 

\gcoll
\begin{desc}

Prints the array distribution.

This is a collective operation.
\end{desc}

\apih{CHECK_HANDLE}{Check whether a GA handle is valid}

\begin{capi}
\begin{ccode}
void GA_Check_handle(int g_a, char* string)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{string}{message string}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_check_handle(g_a, string)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{character(*)*}{string}{TODO}{input} 
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::checkHandle(char* string) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{string}{message}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
check_handle(int g_a, char *message)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Check that the global array handle g_a is valid ... if not, call 
ga_error with the string provided and some more info.
This operation is local.
\end{desc}

\apih{INIT_FENCE}{Initialize tracing of completion of data movement operations}

\begin{capi}
\begin{ccode}
void GA_Init_fence()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_init_fence()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::initFence()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
init_fence() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

Initializes tracing of the completion status of data movement operations.
This operation is local.
\end{desc}

\apih{FENCE}{Fence all GA data movement operations initiated by the calling process}

\begin{capi}
\begin{ccode}
void GA_Fence()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_fence()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::fence()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
fence() 
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

Blocks the calling process until all the data transfers corresponding to GA 
operations called after ga_init_fence complete. For example, since ga_put 
might return before the data reaches the final destination, ga_init_fence 
and ga_fence allow the process to wait until the data tranfer is fully completed:
\begin{verbatim}
        ga_init_fence();
        ga_put(g_a, ...);
        ga_fence();
\end{verbatim}

ga_fence must be called after ga_init_fence. A barrier, ga_sync, assures the 
completion of all data transfers and implicitly cancels all outstanding 
ga_init_fence calls. ga_init_fence and ga_fence must be used in pairs, multiple 
calls to ga_fence require the same number of corresponding ga_init_fence calls. 
ga_init_fence/ga_fence pairs can be nested.

ga_fence works for multiple GA operations. For example:
\begin{verbatim}
        ga_init_fence();
        ga_put(g_a, ...);
        ga_scatter(g_a, ...);
        ga_put(g_b, ...);
        ga_fence();
\end{verbatim}

The calling process will be blocked until data movements initiated by two calls 
to ga_put and one ga_scatter complete.
\end{desc}

\apih{CREATE_MUTEXES}{Create mutexes}

\begin{capi}
\begin{ccode}
int GA_Create_mutexes(int number)
\end{ccode}
\begin{funcargs}
\funcarg{}{number}{number of mutexes in mutex array}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_create_mutexes(number)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{number}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::createMutexes(int number)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{number}{of mutexes in mutex array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
create_mutexes(int number)  
   number (int) - the number of mutexes to create 
\end{pycode}
\end{pyapi}

\wcoll
\begin{desc}

Creates a set containing the number of mutexes. Returns 0 if the operation 
succeeded or 1 if it has failed. Mutex is a simple synchronization object 
used to protect Critical Sections. Only one set of mutexes can exist at a 
time. An array of mutexes can be created and destroyed as many times as needed.

Mutexes are numbered: 0, ..., number-1.

This is a collective operation.

Returns: 
True on success, False on failure 

\end{desc}

\apih{DESTROY_MUTEXES}{Destroy mutexes}

\begin{capi}
\begin{ccode}
int GA_Destroy_mutexes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_destroy_mutexes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::destroyMutexes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
destroy_mutexes()  
\end{pycode}
\end{pyapi} 

\wcoll
\begin{desc}

Destroys the set of mutexes created with ga_create_mutexes. Returns 0 if 
the operation succeeded or 1 when failed.

This is a collective operation.
\end{desc}

\apih{LOCK}{Lock a specific mutex}

\begin{capi}
\begin{ccode}
void GA_Lock(int mutex)
\end{ccode}
\begin{funcargs}
\funcarg{}{mutex}{mutex object id}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_lock(mutex)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{mutex}{}{input}
\end{funcargs}
   ! mutex id
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::lock(int mutex)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{mutex}{mutex object id}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
lock(int mutex)  
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

Locks a mutex object identified by the mutex number. It is a fatal 
error for a process to attempt to lock a mutex which was already 
locked by this process.
\end{desc}

\apih{UNLOCK}{Unlock a mutex}

\begin{capi}
\begin{ccode}
void GA_Unlock(int mutex)
\end{ccode}
\begin{funcargs}
\funcarg{}{mutex}{mutex object id}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_unlock(mutex)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{mutex}{}{input}
\end{funcargs}
   ! mutex id
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
GAServices::unlock(int mutex)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{mutex}{mutex object id}{input}
\end{funcargs}
\end{cxxapi}
\ncoll

\begin{desc}

Unlocks a mutex object identified by the mutex number. It is a fatal 
error for a process to attempt to unlock a mutex which has not been 
locked by this process.
\end{desc}

\apih{NODEID}{The GA rank of the invoking process}

\begin{capi}
\begin{ccode}
int GA_Nodeid()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_nodeid()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::nodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nodeid() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

Returns the GA process id (0, ..., ga_Nnodes()-1) of the requesting compute process.
This operation is local.
\end{desc}

\apih{NNODES}{Total number of GA ranks}

\begin{capi}
\begin{ccode}
int GA_Nnodes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_nnodes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::nodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nnodes() 
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

Returns the number of the GA compute (user) processes.
This operation is local.
\end{desc}

\apih{GEMM}{Matrix multiplication of global arrays}

\begin{capi}
\begin{ccode}
void GA_Dgemm(char ta, char tb, int m, int n, int k, double alpha,
              int g_a, int g_b, double beta, int g_c) 
void GA_Sgemm(char ta, char tb, int m, int n, int k, float alpha,
              int g_a, int g_b, float beta, int g_c) 
void GA_Zgemm(char ta, char tb, int m, int n, int k, double complex alpha,
              int g_a, int g_b, double complex beta, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{, g_b, }{handles to input arrays}{input}
\funcarg{}{g_c}{handles to output array}{output}
\funcarg{}{ta, tb}{transpose operators}{input}
\funcarg{}{m}{number of rows of op(A) and of matrix  C}{input}
\funcarg{}{n}{number of columns of op(B) and of matrix  C}{input}
\funcarg{}{k}{number of columns of op(A) and rows of matrix op(B)}{input}
\funcarg{}{alpha, beta}{scale factors}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine GA_Dgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c) 
subroutine GA_Sgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c) 
subroutine GA_Zgemm(ta, tb, m, n, k, alpha, g_a, g_b, beta, g_c) 
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{handles to input arrays}{input}
\funcarg{integer}{g_c}{handle to output array}{output}
\funcarg{character(1)}{ta, tb}{transpose operators}{input} 
\funcarg{integer}{m}{number of rows of op(A) and of matrix  C}{input}
\funcarg{integer}{n}{number of columns of op(B) and of matrix  C}{input}
\funcarg{integer}{k}{number of columns of op(A) and rows of matrix op(B)}{input}
\funcarg{double precision/double complex/real}{alpha, beta}{scale factors}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::dgemm(char ta, char tb, int m, int n, int k,
                        double alpha, const GlobalArray *g_a, const
                        GlobalArray *g_b, double beta) const
void GlobalArray::dgemm(char ta, char tb, int64_t m, int64_t n, int64_t k,
                        double alpha, const GlobalArray *g_a, const 
                        GlobalArray *g_b, double beta) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{ta}{transpose operators}{input}
\funcarg{}{tb}{transpose operators}{input}
\funcarg{}{m}{number of rows of op(A) and of matrix C}{input}
\funcarg{}{n}{number of columns of op(B) and of matrix C}{input}
\funcarg{}{k}{number of columns of op(A) and rows of matrix op(B)}{input}
\funcarg{}{alpha}{scale factors}{input}
\funcarg{}{g_a}{input arrays}{input}
\funcarg{}{g_b}{input arrays}{input}
\funcarg{}{beta}{scale factors}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
gemm(int ta, int tb, int64_t m, int64_t n, int64_t k, alpha, int g_a,
int g_b, beta, int g_c)  
   ta (bool)       - transpose operator 
   tb (bool)       - transpose operator 
   m (int)         - number of rows of op(A) and of matrix C 
   n (int)         - number of columns of op(B) and of matrix C 
   k (int)         - number of columns of op(A) and rows of matrix op(B) 
   alpha (object)  - scale factor 
   g_a (int)       - handle to input array 
   g_b (int)       - handle to input array 
   beta (object)   - scale factor 
   g_c (int)       - handle to output array 
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Performs one of the matrix-matrix operations:
\[
      C := alpha*op( A )*op( B ) + beta*C,
\]

where op( X ) is one of
\begin{verbatim}
      op( X ) = X   or   op( X ) = X',
\end{verbatim}

alpha and beta are scalars, and A, B, and C are matrices, with op( A ) 
an m by k matrix, op( B ) a k by n matrix and C an m by n matrix.

On entry, transa specifies the form of op( A ) to be used in the matrix 
multiplication as follows:
\begin{verbatim}
           ta = `N' or `n', op( A ) = A.
           ta = `T' or `t', op( A ) = A'.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{COPY_PATCH}{Copy a patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Copy_patch(char trans, int g_a, int alo[], int ahi[],
                    int g_b, int blo[], int bhi[]) 
\end{ccode}
\begin{funcargs}
\funcarg{}{trans}{transpose operator}{input}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_copy_patch(trans, g_a, ailo, aihi, ajlo, ajhi,
                         g_b, bilo, bihi, bjlo, bjhi)
\end{fcode}
\begin{funcargs}
\funcarg{character}{trans}{transpose operator}{input}
\funcarg{integer}{g_a, g_b}{}{input}
\funcarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}{input}
\funcarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_copy_patch(trans, g_a, alo, ahi, g_b, blo, bhi) 
\end{fcode}
\begin{funcargs}
\funcarg{character}{trans}{transpose operator}{input}
\funcarg{integer}{g_a, g_b}{}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::copyPatch(char trans, const GlobalArray* ga, int alo[],
                            int ahi[], int blo[], int bhi[]) const
void GlobalArray::copyPatch(char trans, const GlobalArray* ga, int64_talo[],
                            int64_t ahi[], int64_t blo[], int64_t bhi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{trans}{use transpose operator}{input}
\funcarg{}{ga}{global array}{input}
\funcarg{}{alo}{ga patch coordinates}{input}
\funcarg{}{ahi}{ga patch coordinates}{input}
\funcarg{}{blo}{this GlobalArray's patch coordinates}{input}
\funcarg{}{bhi}{this GlobalArray's patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Copies elements in a patch of one array into another one. The patches of 
arrays may be of different shapes but must have the same number of elements. 
Patches must be non-overlapping (if g_a=g_b).
\begin{verbatim}
    trans = `N' or `n' means that the transpose operator should 
             not be applied.
    trans = `T' or `t' means that transpose operator should be applied.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{DOT_PATCH}{Dot product of patches of global arrays}

\begin{capi}
\begin{ccode}
double NGA_Ddot_patch (int g_a, char ta, int alo[], int ahi[],
                       int g_b, char tb, int blo[], int bhi[]) 
long   NGA_Idot_patch (int g_a, char ta, int alo[], int ahi[],
                       int g_b, char tb, int blo[], int bhi[])  
double complex NGA_Zdot_patch (int g_a, char ta, int alo[], int ahi[],
                              int g_b, char tb, int blo[], int bhi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\funcarg{}{ta, tb}{transpose flags}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
double precision function ga_ddot_patch (g_a, ta, ailo, aihi, ajlo, ajhi,
                                         g_b, tb, bilo, bihi, bjlo, bjhi)
double complex function ga_zdot_patch (g_a, ta, ailo, aihi, ajlo, ajhi,
                                       g_b, tb, bilo, bihi, bjlo, bjhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{}{input}
\funcarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}{input}
\funcarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}{input}
\funcarg{character*1}{ta, tb}{transpose flags}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
double precision function nga_ddot_patch (g_a, ta, alo, ahi, 
                                          g_b, tb, bio, bhi)
double complex function nga_zdot_patch (g_a, ta, alo, ahi,
                                        g_b, tb, blo, bhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{}{input}
\funcarg{integer}{ndim}{number of dimensions}{NA}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}{input}
\funcarg{character*1}{ta, tb}{transpose flags}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
double GlobalArray::ddotPatch(char ta, int alo[], int ahi[],
                              const GlobalArray * g_a, char tb, int blo[],
                              int bhi[]) const
double GlobalArray::ddotPatch(char ta, int64_t alo[], int64_t ahi[],
                              const GlobalArray * g_a, char tb, 
                              int64_t blo[], int64_t bhi[]) const
float GlobalArray::fdotPatch(char ta, int alo[], int ahi[],
                             const GlobalArray * g_a, char tb, int blo[],
                             int bhi[]) const
float GlobalArray::fdotPatch(char ta, int64_t alo[], int64_t ahi[],
                             const GlobalArray * g_a, char tb, int64_t blo[],
                             int64_t bhi[]) const
double complex GlobalArray::zdotPatch(char ta, int alo[], int ahi[],
                                     const GlobalArray * g_a, char tb, 
                                     int blo[], int bhi[]) const
double complex GlobalArray::zdotPatch(char ta, int64_t alo[], int64_t ahi[],
                                     const GlobalArray * g_a, char tb, 
                                     int64_t blo[], int64_t bhi[]) const
long GlobalArray::idotPatch(char ta, int alo[], int ahi[],
                           const GlobalArray * g_a, char tb, int blo[],
                           int bhi[]) const
long GlobalArray::idotPatch(char ta, int64_t alo[], int64_t ahi[],
                            const GlobalArray * g_a, char tb, int64_t blo[],
                            int64_t bhi[]) const
long GlobalArray::ldotPatch(char ta, int alo[], int ahi[],
                            const GlobalArray * g_a, char tb, int blo[],
                            int bhi[]) const
long GlobalArray::ldotPatch(char ta, int64_t alo[], int64_t ahi[],
                            const GlobalArray * g_a, char tb, int64_t blo[],
                            int64_t bhi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{ta}{transpose flags}{input}
\funcarg{}{alo}{g_a patch coordinates}{input}
\funcarg{}{ahi}{g_a patch coordinates}{input}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{tb}{transpose flags}{input}
\funcarg{}{blo}{g_b patch coordinates}{input}
\funcarg{}{bhi}{g_b patch coordinates}{input}
\end{funcargs}
\end{cxxapi}

\gcoll

\begin{desc}

Computes the element-wise dot product of the two (possibly transposed) 
patches which must be of the same type and have the same number of elements.

This is a collective operation.
\end{desc}

\apih{MATMUL_PATCH}{Matrix multiplication of patches of global arrays}

\begin{capi}
\begin{ccode}
void GA_Matmul_patch (char transa, char transb, void* alpha, void *beta,
                      int g_a, int ailo, int aihi, int ajlo, int ajhi,
                      int g_b, int bilo, int bihi, int bjlo, int bjhi,
                      int g_c, int cilo, int cihi, int cjlo, int cjhi)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{, g_b, g_c}{array handles}{input}
\funcarg{ailo}{, aihi, ajlo, ajhi}{patch of g_a}{input}
\funcarg{bilo}{, bihi, bjlo, bjhi}{patch of g_b}{input}
\funcarg{cilo}{, cihi, cjlo, cjhi}{patch of g_c}{input}
\funcarg{alpha}{, beta}{scale factors}{input}
\funcarg{transa}{, transb}{transpose operators}{input}
\end{funcargs}
\end{capi}

\begin{capi}
\begin{ccode}
void NGA_Matmul_patch(char transa, char transb, void* alpha, void *beta,
                      int g_a, int alo[], int ahi[],
                      int g_b, int blo[], int bhi[], 
                      int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{, g_b, g_c}{array handles}{input}
\funcarg{alo}{, ahi}{patch of g_a}{input}
\funcarg{blo}{, bhi}{patch of g_b}{input}
\funcarg{clo}{, chi}{patch of g_c}{input}
\funcarg{alpha}{, beta}{scale factors}{input}
\funcarg{transa}{, transb}{transpose operators}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_matmul_patch (transa, transb, alpha, beta,
                            g_a, ailo, aihi, ajlo, ajhi,
                            g_b, bilo, bihi, bjlo, bjhi,
                            g_c, cilo, cihi, cjlo, cjhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, ailo, aihi, ajlo, ajhi}{patch of g_a}{input}
\funcarg{integer}{g_b, bilo, bihi, bjlo, bjhi}{patch of g_b}{input}
\funcarg{integer}{g_c, cilo, cihi, cjlo, cjhi}{patch of g_c}{input}
\funcarg{double precision/complex}{alpha, beta}{TODO}{input}
\funcarg{character*1}{transa, transb}{TODO}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_matmul_patch(transa, transb, alpha, beta,
                            g_a, alo, ahi,
                            g_b, blo, bhi,
                            g_c, clo, chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, alo, ahi}{patch of g_a}{input}
\funcarg{integer}{g_b, blo, bhi}{patch of g_b}{input}
\funcarg{integer}{g_c, clo, chi}{patch of g_c}{input}
\funcarg{double precision/complex}{alpha, beta}{TODO}{input}
\funcarg{character*1}{transa, transb}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::matmulPatch(char transa, char transb,
                              void* alpha, void *beta, const GlobalArray *g_a,
                              int ailo, int aihi, int ajlo, int ajhi,
                              const GlobalArray *g_b, int bilo, int bihi,
                              int bjlo, int bjhi, int cilo, int cihi,
                              int cjlo, int cjhi) const;
void GlobalArray::matmulPatch(char transa, char transb,
                              void* alpha, void *beta, const GlobalArray *g_a,
                              int64_t ailo, int64_t aihi, int64_t ajlo, 
                              int64_t ajhi, const GlobalArray *g_b, int64_t 
                              bilo, int64_t bihi, int64_t bjlo, int64_t bjhi, 
                              int64_t cilo, int64_t cihi, int64_t cjlo, 
                              int64_t cjhi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{transa}{transpose operators}{input}
\funcarg{}{transb}{transpose operators}{input}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{ailo}{patch of g_a}{input}
\funcarg{}{aihi}{patch of g_a}{input}
\funcarg{}{ajlo}{patch of g_a}{input}
\funcarg{}{ajhi}{patch of g_a}{input}
\funcarg{}{bilo}{patch of g_b}{input}
\funcarg{}{bihi}{patch of g_b}{input}
\funcarg{}{bjlo}{patch of g_b}{input}
\funcarg{}{bjhi}{patch of g_b}{input}
\funcarg{}{cilo}{patch of g_c}{input}
\funcarg{}{cihi}{patch of g_c}{input}
\funcarg{}{cjlo}{patch of g_c}{input}
\funcarg{}{cjhi}{patch of g_c}{input}
\funcarg{}{alpha}{scale factors}{input}
\funcarg{}{beta}{scale factors}{input}
\end{funcargs}
\end{cxxapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::matmulPatch(char transa, char transb, void* alpha,
                              void *beta,const GlobalArray *g_a,
                              int *alo, int *ahi, const GlobalArray *g_b,
                              int *blo, int *bhi, int *clo, int *chi) const
void GlobalArray::matmulPatch(char transa, char transb, void* alpha,
                              void *beta, const GlobalArray *g_a,
                              int64_t *alo, int64_t *ahi, const GlobalArray 
                              *g_b, int64_t *blo, int64_t *bhi, 
                              int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{alo}{array of patch of g_a}{input}
\funcarg{}{ahi}{array of patch of g_a}{input}
\funcarg{}{blo}{array of patch of g_b}{input}
\funcarg{}{bhi}{array of patch of g_b}{input}
\funcarg{}{clo}{array of patch of g_c}{input}
\funcarg{}{chi}{array of patch of g_c}{input}
\funcarg{}{alpha}{scale factors}{input}
\funcarg{}{beta}{scale factors}{input}
\funcarg{}{transa}{transpose operators}{input}
\funcarg{}{transb}{transpose operators}{input}
\end{funcargs}
\end{cxxapi}


\begin{pyapi}
\begin{pycode}
matmul_patch(int transa, int transb, alpha, beta, int g_a, alo, ahi,
int g_b, blo, bhi, int g_c, clo, chi)
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

ga_matmul_patch is a patch version of ga_dgemm and comes in 2-D and N-D
versions. The 2-D interface performs the operation:
\begin{verbatim}
         C[cilo:cihi,cjlo:cjhi] := alpha* AA[ailo:aihi,ajlo:ajhi] *
                                   BB[bilo:bihi,bjlo:bjhi] ) + 
                                   beta*C[cilo:cihi,cjlo:cjhi],
\end{verbatim}

where AA = op(A), BB = op(B), and op(X) is one of
\begin{verbatim}
      op(X) = X   or   op(X) = X',
\end{verbatim}

Valid values for transpose arguments: `n', `N', `t', `T'. It works for both 
double and double complex data tape.

nga_matmul_patch is a N-dimensional patch version of ga_dgemm and is similar to
the 2-D interface:
\begin{verbatim}
      C[clo[]:chi[]] := alpha* AA[alo[]:ahi[]] *
                               BB[blo[]:bhi[]] ) + beta*C[clo[]:chi[]],
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{ADD_PATCH}{Add patches of global arrays}

\begin{capi}
\begin{ccode}
void NGA_Add_patch(void *alpha, int g_a, int alo[], int ahi[],
                   void *beta,  int g_b, int blo[], int bhi[],
                   int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{, g_b, g_c}{array handles}{input}
\funcarg{}{alo[], ahi[]}{patch of g_a}{input}
\funcarg{}{blo[], bhi[]}{patch of g_b}{input}
\funcarg{}{clo[], chi[]}{patch of g_c}{input}
\funcarg{alpha}{, beta}{scale factors}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_add_patch(alpha, g_a, ailo, aihi, ajlo, ajhi,
                        beta,  g_b, bilo, bihi, bjlo, bjhi,
                               g_c, cilo, cihi, cjlo, cjhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b, g_c}{}{input}
\funcarg{double precision/complex/integer}{alpha, beta}{TODO}{input}
\funcarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}{input}
\funcarg{integer}{bilo, bihi, bjlo, bjhi}{g_b patch coordinates}{input}
\funcarg{integer}{cilo, cihi, cjlo, cjhi}{g_c patch coordinates}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_add_patch(alpha, g_a, alo, ahi, beta, g_b, blo, bhi
                         g_c, clo, chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b, g_c}{}{input}
\funcarg{double precision/complex/integer}{alpha,beta}{TODO}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch coordinates}{input}
\funcarg{integer}{clo(ndim), chi(ndim)}{g_c patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addPatch(void *alpha, const GlobalArray * g_a, int alo[],
                           int ahi[],void *beta, const GlobalArray * g_b,
                           int blo[], int bhi[], int clo[], int chi[]) const
void GlobalArray::addPatch(void *alpha, const GlobalArray * g_a, int64_t alo[],
                           int64_t ahi[], void *beta, const GlobalArray * g_b,
                           int64_t blo[], int64_t bhi[], int64_t clo[], 
                           int64_t chi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{alpha}{scale factor}{input}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{alo}{patch of g_a}{input}
\funcarg{}{ahi}{patch of g_a}{input}
\funcarg{}{beta}{scale factor}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{blo}{patch of g_b}{input}
\funcarg{}{bhi}{patch of g_b}{input}
\funcarg{}{clo}{patch of this GlobalArray}{input}
\funcarg{}{chi}{patch of this GlobalArray}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Patches of arrays (which must have the same number of elements) are added 
together element-wise.
\begin{verbatim}
c[ ][ ] = alpha * a[ ][ ] + beta * b[ ][ ]
\end{verbatim}
This is a collective operation.
\end{desc}

\apih{FILL_PATCH}{Fill a patch of a global array with a specified value}

\begin{capi}
\begin{ccode}
void NGA_Fill_patch(int g_a, int lo[], int hi[], void *val)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handles}{input}
\funcarg{}{lo[], hi[]}{patch of g_a}{input}
\funcarg{}{val}{value to fill}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_fill_patch(g_a, ailo, aihi, ajlo, ajhi, s)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision/complex/integer}{s}{TODO}{input}
\funcarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_fill_patch (g_a, alo, ahi, s)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision/complex/integer}{s}{TODO}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::fillPatch (int lo[], int hi[], void *val) const
void GlobalArray::fillPatch (int64_t lo[], int64_t hi[], void *val) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{patch of this GlobalArray}{input}
\funcarg{}{hi}{patch of this GlobalArray}{input}
\funcarg{}{val}{value to fill}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Fill the patch of g_a with value of `val'

This is a collective operation.
\end{desc}

\apih{ZERO_PATCH}{Zero a patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Zero_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handles}{input}
\funcarg{}{lo[], hi[]}{patch of g_a}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_zero_patch(g_a, alo, ahi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zeroPatch (int lo[], int hi[]) const
void GlobalArray::zeroPatch (int64_t lo[], int64_t hi[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{patch of this GlobalArray}{input}
\funcarg{}{hi}{patch of this GlobalArray}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}


Set all the elements in the patch to zero.

This is a collective operation.

\end{desc}

\apih{SCALE_PATCH}{Scale elements in the patch of a global array}

\begin{capi}
\begin{ccode}
void NGA_Scale_patch(int g_a, int lo[], int hi[], void *val)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handles}{input}
\funcarg{}{lo[], hi[]}{patch of g_a}{input}
\funcarg{}{val}{scale factor}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_scale_patch(g_a, ailo, aihi, ajlo, ajhi, s)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision/complex/integer}{s}{TODO}{input}
\funcarg{integer}{ailo, aihi, ajlo, ajhi}{g_a patch coordinates}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_scale_patch(g_a, alo, ahi, s)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{}{input}
\funcarg{double precision/complex/integer}{s}{TODO}{input} 
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scalePatch (int lo[], int hi[], void *val) const;
void GlobalArray::scalePatch (int64_t lo[], int64_t hi[], void *val) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{patch of this GlobalArray}{input}
\funcarg{}{hi}{patch of this GlobalArray}{input}
\funcarg{}{val}{scale factor}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Scale an array by the factor `val'

This is a collective operation.
\end{desc}

\apih{BRDCST}{Broadcast elements among all processes}

\begin{capi}
\begin{ccode}
void GA_Brdcst(void *buf, int lenbuf, int root)
\end{ccode}
\begin{funcargs}
\funcarg{}{lenbuf}{length of buffer (bytes)}{input}
\funcarg{}{buf[lenbuf]}{data}{input/output}
\funcarg{}{root}{root process}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_brdcst(type, buf, lenbuf, root)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{type}{}{input}
\funcarg{byte}{buf(lenbuf)}{TODO}{input/output}
\funcarg{integer}{lenbuf}{}{input}
\funcarg{integer}{root}{}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::brdcst(void *buf, int lenbuf, int root)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lenbuf}{length of buffer}{input}
\funcarg{}{buf[lenbuf]}{data}{input/output}
\funcarg{}{root}{root process}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
brdcst(buffer, int root) 
   buffer (1D array-like of objects) - the ndarray message
                                       (converted to the appropriate type) 
   root (int)                        - the process which is sending 
\end{pycode}
\end{pyapi}

\wcoll
\begin{desc}

Broadcast from process root to all other processes a message of length lenbuf.

This is operation is provided only for convenience purposes: it is available 
regardless of the message-passing library that GA is running.

If the buffer is not contiguous, an error is raised. This operation is provided 
only for convenience purposes: it is available regardless of the message-passing 
library that GA is running with.


This is a collective operation.

Returns: 
The buffer in case a temporary was passed in. 
\end{desc}

\apih{DGOP}{Global operation of double precision elements among all processes}

\begin{capi}
\begin{ccode}
void GA_Dgop(double x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_dgop(type, x, n, op)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{type}{}{input}
\funcarg{double precision}{x(n)}{TODO}{input/output}
\funcarg{character*(*)}{op}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::dgop(double x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}


\wcoll
\begin{desc}

Double Global OPeration.

$X(1:N)$ is a vector present on each process. DGOP `sums' elements of 
X accross all nodes using the commutative operator OP. The result is 
broadcast to all nodes. Supported operations include `+', `*', `max', 
`min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is operation is provided only for convenience purposes: it is available 
regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{IGOP}{Inter global operation among all processes}

\begin{capi}
\begin{ccode}
void GA_Igop(long x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_igop(type, x, n, op)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{type}{}{input}
\funcarg{integer}{x(n)}{TODO}{input/output}
\funcarg{character*(*)}{op}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::igop(int x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\wcoll
\begin{desc}

Integer Global OPeration. The integer (more precisely long) version
of ga_dgop described above, also includes the bitwise OR operation.

This is operation is provided only for convenience purposes: it is 
available regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{LGOP}{Long global operation among all processes}

\begin{capi}
\begin{ccode}
void GA_Lgop(long x[], int n, char *op)
\end{ccode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::lgop(long x[], int n, char *op);
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\wcoll

\begin{desc}

Long Global OPeration. The long version of ga_dgop described above, 
also includes the bitwise OR operation.

This is operation is provided only for convenience purposes: it is 
available regardless of the message-passing library that GA is running with.

This is a collective operation.
\end{desc}

\apih{CLUSTER_NNODES}{Total number of cluster (shared memory) nodes}

\begin{capi}
\begin{ccode}
int GA_Cluster_nnodes()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nnodes()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNnodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nnodes() 
\end{pycode}
\end{pyapi}
\local
\begin{desc}


This functions returns the total number of nodes that the program is running 
on. On SMP architectures, this will be less than or equal to the total number 
of processors.

This is a  local operation.
\end{desc}

\apih{CLUSTER_NODEID}{Cluster node Rank of the invoking process}

\begin{capi}
\begin{ccode}
int GA_Cluster_nodeid()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nodeid()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nodeid(int proc=-1)  
   proc (int)    - process ID to lookup
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the node ID of the process. On SMP architectures with more 
than one processor per node, several processes may return the same node id.

This is a  local operation.
\end{desc}

\apih{CLUSTER_PROC_NODEID}{Cluster node rank of a specified process}

\begin{capi}
\begin{ccode}
int GA_Cluster_proc_nodeid(int proc)
\end{ccode}
\begin{funcargs}
\funcarg{}{proc}{process id}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_proc_nodeid(proc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{proc}{process id}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterProcNodeid(int iproc)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{iproc}{process id}{input}
\end{funcargs}
\end{cxxapi}
\local

\begin{desc}

This function returns the node ID of the specified process proc. 
On SMP architectures with more than one processor per node, several 
processes may return the same node id.

This is a  local operation.
\end{desc}

\apih{CLUSTER_NPROCS}{Number of processes in a given cluster node}

\begin{capi}
\begin{ccode}
int GA_Cluster_nprocs(int inode)
\end{ccode}
\begin{funcargs}
\funcarg{}{inode}{node id}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_nprocs(inode)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{inode}{node id}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterNprocs(int inode)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{inode}{node id}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_nprocs(int inode) 
\end{pycode}
\end{pyapi}
\local

\begin{desc}

This function returns the number of processors available on node inode.

This is a local operation.
\end{desc}

\apih{CLUSTER_PROCID}{Rank of a process from a cluster node rank and intra-node rank}

\begin{capi}
\begin{ccode}
int GA_Cluster_procid(int inode, int iproc)
\end{ccode}
\begin{funcargs}
\funcarg{}{inode}{node id}{input}
\funcarg{}{iproc}{processor id}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_cluster_procid(inode,iproc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{inode}{node id}{input}
\funcarg{integer}{iproc}{processor id}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GAServices::clusterProcid(int inode, int iproc)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{inode}{node id}{input}
\funcarg{}{iproc}{processor id}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
cluster_procid(int inode, int iproc)  
\end{pycode}
\end{pyapi} 

\local

\begin{desc}

This function returns the processor id associated with node inode and 
the local processor ID iproc. If node inode has N processors, then the 
value of iproc lies between 0 and N-1.

This is a local operation.
\end{desc}

\apih{DIAG}{Diagonalize a global array}

\begin{capi}
\begin{ccode}
void GA_Diag(int g_a, int g_s, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{Matrix to diagonalize}{input}
\funcarg{}{g_s}{Metric}{input}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag(g_a, g_s, g_v, eval)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{Matrix to diagonalize}{input}
\funcarg{integer}{g_s}{Metric}{input}
\funcarg{integer}{g_v}{Global matrix to return evecs}{output}
\funcarg{double precision}{eval(*)}{Local array to return evals}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diag(const GlobalArray *g_s, GlobalArray *g_v, void *eval)
const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_s}{Matrix to diagonalize}{input}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag(int g_a, int g_s, int g_v, evalues=None) 
   g_a (int)     - the array handle of the matrix to diagonalize 
   g_s (int)     - the array handle of the metric 
   g_v (int)     - the array handle to return evecs 
\end{pycode}
\end{pyapi}

\gcoll
\begin{desc}

Solve the generalized eigenvalue problem returning all eigenvectors and 
values in ascending order. The input matrices are not overwritten or destroyed.

Returns: 
All eigen-values as an ndarray in ascending order. 

This is a collective operation.
\end{desc}

\apih{DIAG_REUSE}{Diagonalize a global array for repeated diagonalizations}

\begin{capi}
\begin{ccode}
void GA_Diag_reuse(int control, int g_a, int g_s, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\funcarg{}{control}{Control flag}{input}
\funcarg{}{g_a}{Matrix to diagonalize}{input}
\funcarg{}{g_s}{Metric}{input}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag_reuse(control, g_a, g_s, g_v, eval)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{control}{Control flag}{input}
\funcarg{integer}{g_a}{Matrix to diagonalize}{input}
\funcarg{integer}{g_s}{Metric}{input}
\funcarg{integer}{g_v}{Global matrix to return evecs}{output}
\funcarg{double precision}{eval(*)}{Local array to return evals}{output}   
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diagReuse(int control, const GlobalArray *g_s,
                            GlobalArray *g_v, void *eval) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{control}{Control flag}{input}
\funcarg{}{g_s}{Matrix to diagonalize}{input}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag_reuse(int control, int g_a, int g_s, int g_v, evalues=None)  
   control (int) - 0 indicates first call to the eigensolver; >0 
   consecutive calls (reuses factored g_s); <0 only erases factorized g_s;
   g_v and eval unchanged (should be called after previous use if another
   eigenproblem, i.e., different g_a and g_s, is to be solved) 
   g_a (int)     - the array handle of the matrix to diagonalize 
   g_s (int)     - the array handle of the metric 
   g_v (int)     - the array handle to return evecs 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solve the generalized eigenvalue problem returning all eigenvectors and 
values in ascending order. Recommended for REPEATED calls if g_s is unchanged. 
Values of the control flag:
\begin{verbatim}
          value       action/purpose
            0          indicates first call to the eigensolver
           >0          consecutive calls (reuses factored g_s)
           <0          only erases factorized g_s; g_v and eval unchanged
                       (should be called after previous use if another
                        eigenproblem, i.e., different g_a and g_s, is to
                        be solved)
\end{verbatim}

The input matrices are not destroyed.

Returns: 
All eigen-values as an ndarray in ascending order. 

This is a collective operation.
\end{desc}

\apih{DIAG_STD}{Standard diagonalization of a global array}

\begin{capi}
\begin{ccode}
void GA_Diag_std(int g_a, int g_v, void *eval)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{Matrix to diagonalize}{input}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_diag_std(g_a, g_v, eval)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{Matrix to diagonalize}{input}
\funcarg{integer}{g_v}{Global matrix to return evecs}{output}
\funcarg{double precision}{eval(*)}{Local array to return evals}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::diagStd(GlobalArray *g_v, void *eval) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_v}{Global matrix to return evecs}{output}
\funcarg{}{eval}{Local array to return evals}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
diag_std(int g_a, int g_v, evalues=None)  
   g_a (int)     - the array handle of the matrix to diagonalize 
   g_v (int)     - the array handle to return evecs 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solve the standard (non-generalized) eigenvalue problem returning all 
eigenvectors and values in the ascending order. The input matrix is 
neither overwritten nor destroyed.

Returns:
all eigenvectors via the g_v global array, and eigenvalues as an 
ndarray in ascending order

This is a collective operation.
\end{desc}

\apih{LLT_SOLVE}{Cholesky factorization of a global array}

\begin{capi}
\begin{ccode}
int GA_Llt_solve(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{coefficient matrix}{input}
\funcarg{}{g_b}{rhs/solution matrix}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_llt_solve(g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{coefficient matrix}{input}
\funcarg{integer}{g_b}{rhs/solution matrix}{output/output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::lltSolve(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{coefficient matrix}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
llt_solve(int g_a, int g_b) 
   g_a (int)     - the coefficient matrix 
   g_b (int)     - the rhs/solution matrix 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Solves a system of linear equations
\begin{verbatim}
            A * X = B
\end{verbatim}

using the Cholesky factorization of an NxN double precision symmetric 
positive definite matrix A (represented by handle g_a). On successful 
exit B will contain the solution X.

It returns:
\begin{verbatim}
         = 0 : successful exit
         > 0 : the leading minor of this order is not positive
               definite and the factorization could
               not be completed.
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{LU_SOLVE}{LU decomposition of a global array}

\begin{capi}
\begin{ccode}
void GA_Lu_solve(char trans, int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{trans}{transpose or not transpose}{input}
\funcarg{}{g_a}{coefficient matrix}{input}
\funcarg{}{g_b}{rhs/solution matrix}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_lu_solve(trans, g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{character}{trans}{transpose or not transpose}{input}
\funcarg{integer}{g_a}{coefficient matrix}{input}
\funcarg{integer}{g_b}{rhs/solution matrix}{output/output}
\end{funcargs}

   trans = `N' or `n' means that the transpose operator should not be applied.
   trans = `T' or `t' means that the transpose operator should be applied.
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::luSolve(char trans, const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{trans}{transpose or not transpose}{input}
\funcarg{}{g_a}{coefficient matrix}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
lu_solve(int g_a, int g_b, int trans=False) 
   g_a (int)     - the array handle for the coefficient matrix 
   g_b (int)     - the array handle for the solution matrix 
   trans (bool)  - transpose (True) or not transpose (False) 
\end{pycode}
\end{pyapi} 

\gcoll

\begin{desc}


Solve the system of linear equations op(A)X = B based on the LU factorization.

op(A) = A or A' depending on the parameter trans:
\begin{verbatim}
     trans = `N' or `n' means that the transpose operator should not be applied.
     trans = `T' or `t' means that the transpose operator should be applied.
\end{verbatim}

Matrix A is a general real matrix. Matrix B contains possibly multiple rhs vectors. 
The array associated with the handle g_b is overwritten by the solution matrix X.

This is a collective operation.
\end{desc}

\apih{SOLVE}{Solve a system of linear equations}

\begin{capi}
\begin{ccode}
int GA_solve(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{coefficient matrix}{input}
\funcarg{}{g_b}{rhs/solution matrix}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_solve(g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{coefficient matrix}{input}
\funcarg{integer}{g_b}{rhs/solution matrix}{output/output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::solve(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{coefficient matrix}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
solve(int g_a, int g_b) 
\end{pycode}
\end{pyapi} 


\gcoll

\begin{desc}


Solves a system of linear equations
\begin{verbatim}
            A * X = B
\end{verbatim}

It first will call the Cholesky factorization routine and, if sucessfully, will solve the system with the Cholesky solver. If Cholesky will be not be able to factorize A, then it will call the LU factorization routine and will solve the system with forward/backward substitution. On exit B will contain the solution X.

It returns
\begin{verbatim}
         = 0 : Cholesky factoriztion was succesful
         > 0 : the leading minor of this order
               is not positive definite, Cholesky factorization
               could not be completed and LU factoriztion was used
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SPD_INVERT}{Invert a symmetric positive definite matrix a global array}

\begin{capi}
\begin{ccode}
int GA_Spd_invert(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{matrix}{input/output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_spd_invert(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{matrix}{input/output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::spdInvert() const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{coefficient matrix}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
spd_invert(int g_a) 
\end{pycode}
\end{pyapi} 


\gcoll

\begin{desc}


It computes the inverse of a double precision using the Cholesky 
factorization of a NxN double precision symmetric positive definite 
matrix A stored in the global array represented by g_a. On successful 
exit, A will contain the inverse.

It returns
\begin{verbatim}
         = 0 : successful exit
         > 0 : the leading minor of this order is not positive
               definite and the factorization could not be completed
         < 0 : it returns the index i of the (i,i)
               element of the factor L/U that is zero and
               the inverse could not be computed
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SELECT_ELEM}{select an element in a global returned by the chosen operation (eg., min, max, etc.)}

\begin{capi}
\begin{ccode}

void NGA_Select_elem(int g_a, char *op, void* val, int index[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle Control}{input}
\funcarg{}{op}{operator {`min',`max'}}{input}
\funcarg{}{val}{address where value should be stored}{output}
\funcarg{}{index[ndim]}{array index for the selected element}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine nga_select_elem(g_a, op, val, index)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle Control}{input}
\funcarg{character(*)*}{op}{operator {`min',`max'}}{input}
\funcarg{}{val}{address where selected value should be stored}{output}
\funcarg{}{index[ndim]}{array index for the selected element}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::selectElem(char *op, void* val, int index[]) const
void GlobalArray::selectElem(char *op, void* val, int64_t index[]) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{op}{operator {"min","max"}}{input}
\funcarg{}{val}{address where value should be stored}{output}
\funcarg{}{index[ndim]}{array index for the selected element}{output}
\end{funcargs}
\end{cxxapi}


\begin{pyapi}
\begin{pycode}
select_elem(int g_a, char *op)  
\end{pycode}
\end{pyapi} 

\gcoll

\begin{desc}

Returns the value and index for an element that is selected by the 
specified operator in a global array corresponding to g_a handle.
This is a collective operation.
\end{desc}

\apih{SUMMARIZE}{Print summary information on a global array}

\begin{capi}
\begin{ccode}
void GA_Summarize(int verbose)
\end{ccode}
\begin{funcargs}
\funcarg{}{verbose}{If true print distribution info}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_summarize(verbose)
\end{fcode}
\begin{funcargs}
\funcarg{logical}{verbose}{If true print distribution info}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::summarize(int verbose) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{verbose}{If true print distribution info}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
summarize(int verbose)
\end{pycode}
\end{pyapi}
\local
\begin{desc}

Prints info about allocated arrays.
\end{desc}

\apih{SYMMETRIZE}{Symmetrize a global array}

\begin{capi}
\begin{ccode}
void GA_Symmetrize(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{}{}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_symmetrize(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input/output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::symmetrize() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
symmetrize(int g_a) 
\end{pycode}
\end{pyapi} 

\gcoll
\begin{desc}

Symmetrizes matrix A represented with handle g_a: A:= .5 * (A+A').

This is a collective operation.
\end{desc}

\apih{TRANSPOSE}{Transpose a global array}

\begin{capi}
\begin{ccode}
void GA_Transpose(int g_a, int g_b)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{original matrix}{input}
\funcarg{}{g_b}{solution matrix}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_transpose(g_a, g_b)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{remains unchanged}{input}
\funcarg{integer}{g_b}{}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::transpose(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{assign transpose to this GlobalArray}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
transpose(int g_a, int g_b)
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}


Transposes a matrix: B = A', where A and B are represented by handles g_a and g_b.

This is a collective operation.
\end{desc}

\apih{ABS_VALUE}{Convert a global array to contain absolute values of its elements}

\begin{capi}
\begin{ccode}
void GA_Abs_value(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_abs_value(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::absValue() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
abs_value(int g_a, lo=None, hi=None)  
   g_a (int) - the array handle 
   lo (1D array-like) - lower bound patch coordinates, inclusive 
   hi (1D array-like) - higher bound patch coordinates, exclusive 
\end{pycode}
\end{pyapi}



\gcoll

\begin{desc}

Take the element-wise absolute value of the array.
This is a collective operation.
\end{desc}

\apih{ABS_VALUE_PATCH}{Convert a patch of a global array to have absolute values of its elements}

\begin{capi}
\begin{ccode}
void GA_Abs_value_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{lo[], hi[]}{g_a patch coordinates}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_abs_value_patch(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{lo(ndim), hi(ndim)}{g_a patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::absValuePatch(int *lo, int *hi) const
void GlobalArray::absValuePatch(int64_t *lo, int64_t *hi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{lower corner patch coordinates}{input}
\funcarg{}{hi}{upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Take the element-wise absolute value of the patch.
This is a collective operation.
\end{desc}

\apih{ADD_CONSTANT}{Add a constant to all elements in a global array}

\begin{capi}
\begin{ccode}
void GA_Add_constant(int g_a, void *alpha)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{double/complex/int/long/float*}{alpha}{added value}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_constant(g_a,  alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double/complex/integer/float}{alpha}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addConstant(void* alpha) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{alpha}{double/complex/int/long/float constant to be added}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add_constant(int g_a, alpha, lo=None, hi=None) 
   g_a (int)                      - the array handle 
   alpha (object)                 - the constant to add (converted to 
                                    appropriate type) 
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive 
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Add the constant pointed by alpha to each element of the array.
This is a collective operation.
\end{desc}

\apih{ADD_CONSTANT_PATCH}{Add a constant to all elements in a global array patch}

\begin{capi}
\begin{ccode}
void GA_Add_constant_patch(int g_a, int lo[], int hi[], void *alpha)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{lo[], hi[]}{patch coordinates}{input}
\funcarg{double/complex/int/long/float*}{alpha}{added value}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_constant_patch(g_a, lo, hi, alpha)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{lo(ndim), hi(ndim)}{patch coordinates}{input}
\funcarg{double/complex/integer/float}{alpha}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addConstantPatch(int *lo, int *hi, void *alpha) const
void GlobalArray::addConstantPatch(int64_t *lo, int64_t *hi, void *alpha) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{lower corner patch coordinates}{input}
\funcarg{}{hi}{upper corner patch coordinates}{input}
\funcarg{}{alpha}{double/complex/int/long/float constant to be added}{input}
\end{funcargs}
\end{cxxapi}



\gcoll

\begin{desc}

Add the constant pointed by alpha to each element of the patch.
This is a collective operation.
\end{desc}

\apih{RECIP}{Translate a global array to contain reciprocal of its elements}

\begin{capi}
\begin{ccode}
void GA_Recip(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_recip(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::recip() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
recip(int g_a, lo=None, hi=None) 
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Take the element-wise reciprocal of the array.
This is a collective operation.
\end{desc}

\apih{RECIP_PATCH}{Translate a global array patch to contain reciprocal of its elements}

\begin{capi}
\begin{ccode}
void GA_Recip_patch(int g_a, int lo[], int hi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{lo[], hi[]}{patch coordinates}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_recip_patch(g_a, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{lo(ndim), hi(ndim)}{patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::recipPatch(int *lo, int *hi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{lower corner patch coordinates}{input}
\funcarg{}{hi}{upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Take element-wise reciprocal of the patch.
This is a collective operation.
\end{desc}

\apih{ELEM_MULTIPLY}{Element-wise multiplication of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_multiply(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_multiply(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMultiply(const GlobalArray * g_a, const
GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{GlobalArray}{input}
\funcarg{}{g_b}{GlobalArray}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_multiply(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle 
   g_b (int)                       - the array handle 
   g_c (int)                       - the array handle 
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive 
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a,
                                     exclusive 
   blo (1D array-like of integers) - lower bound patch coordinates of g_b, 
                                     inclusive 
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b, 
                                     exclusive 
   clo (1D array-like of integers) - lower bound patch coordinates of g_c,
                                     inclusive 
   chi (1D array-like of integers) - higher bound patch coordinates of g_c, 
                                     exclusive 
\end{pycode}
\end{pyapi}


\gcoll

\begin{desc}

Computes the element-wise product of the two arrays
which must be of the same types and same number of
elements. For two-dimensional arrays,

\begin{verbatim}
        c(i, j)  = a(i,j)*b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_MULTIPLY_PATCH}{Element-wise multiplication of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_multiply_patch(int g_a, int alo[], int ahi[], int g_b, int blo[],
                            int bhi[], int g_c, int clo[], int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\funcarg{}{clo[], chi[]}{g_c patch coordinates}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_multiply_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, clo, 
chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}{input}
\funcarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMultiplyPatch(const GlobalArray * g_a, int *alo, 
                                    int *ahi, const GlobalArray * g_b, 
                                    int *blo, int *bhi, int *clo, 
                                    int *chi) const
void GlobalArray::elemMultiplyPatch(const GlobalArray * g_a, int64_t *alo,
                                    int64_t *ahi, const GlobalArray * g_b,
                                    int64_t *blo, int64_t *bhi, 
                                    int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\funcarg{}{clo}{g_c lower corner patch coordinates}{input}
\funcarg{}{chi}{g_c upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise product of the two patches
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j)  = a(i,j)*b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_DIVIDE}{Element-wise division of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_divide(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_divide(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemDivide(const GlobalArray * g_a, const GlobalArray
                             * g_b) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{GlobalArray}{input}
\funcarg{}{g_b}{GlobalArray}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_divide(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle 
   g_b (int)                       - the array handle 
   g_c (int)                       - the array handle 
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive 
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a, 
                                     exclusive 
   blo (1D array-like of integers) - lower bound patch coordinates of g_b, 
                                     inclusive 
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b, 
                                     exclusive 
   clo (1D array-like of integers) - lower bound patch coordinates of g_c, 
                                     inclusive 
   chi (1D array-like of integers) - higher bound patch coordinates of g_c, 
                                     exclusive 
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the element-wise quotient of the two arrays
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j) = a(i,j)/b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b). 
If one of the elements of array g_b is zero, the quotient 
for the element of g_c will be set to GA_NEGATIVE_INFINITY.

This is a collective operation.
\end{desc}

\apih{ELEM_DIVIDE_PATCH}{Element-wise division of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_divide_patch(int g_a, int alo[], int ahi[], int g_b, 
                          int blo[], int bhi[], int g_c, int clo[], 
                          int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\funcarg{}{clo[], chi[]}{g_c patch coordinates}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_divide_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, 
                                clo, chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}{input}
\funcarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemDividePatch(const GlobalArray * g_a, int *alo, 
                                  int *ahi, const GlobalArray * g_b, 
                                  int *blo, int *bhi, int *clo, 
                                  int *chi) const
void GlobalArray::elemDividePatch(const GlobalArray * g_a, int64_t *alo, 
                                  int64_t *ahi, const GlobalArray * g_b, 
                                  int64_t *blo, int64_t *bhi, 
                                  int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\funcarg{}{clo}{g_c lower corner patch coordinates}{input}
\funcarg{}{chi}{g_c upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise quotient of the two patches
which must be of the same types and same number of
elements. For two-dimensional arrays,
\begin{verbatim}
        c(i,j)  = a(i,j)/b(i,j)
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_MAXIMUM}{Element-wise maximum of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_maximum(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_maximum(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMaximum(const GlobalArray * g_a, const
                              GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
elem_maximum(int g_a, int g_b, int g_c, alo=None, ahi=None, blo=None,
bhi=None, clo=None, chi=None)
   g_a (int)                       - the array handle 
   g_b (int)                       - the array handle 
   g_c (int)                       - the array handle 
   alo (1D array-like of integers) - lower bound patch coordinates of g_a,
                                     inclusive 
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a, 
                                     exclusive 
   blo (1D array-like of integers) - lower bound patch coordinates of g_b, 
                                     inclusive 
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b, 
                                     exclusive 
   clo (1D array-like of integers) - lower bound patch coordinates of g_c, 
                                     inclusive 
   chi (1D array-like of integers) - higher bound patch coordinates of g_c, 
                                     exclusive 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the element-wise maximum of the two arrays
which must be of the same types and same number of
elements. For two dimensional arrays,
\begin{verbatim}
    c(i,j)  = max{a(i,j), b(i,j)}
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_MAXIMUM_PATCH}{Element-wise maximum of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_maximum_patch(int g_a, int alo[], int ahi[], int g_b, 
                           int blo[], int bhi[], int g_c, int clo[], 
                           int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\funcarg{}{clo[], chi[]}{g_c patch coordinates}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_maximum_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, 
                                 clo, chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim), ahi(ndim)}{g_a patch dimensions}{input}
\funcarg{integer}{blo(ndim), bhi(ndim)}{g_b patch dimensions}{input}
\funcarg{integer}{clo(ndim), chi(ndim)}{g_c patch dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMaximumPatch(const GlobalArray * g_a, int *alo, int *ahi,
                                   const GlobalArray * g_b, int *blo, int *bhi,
                                   int *clo, int *chi) const
void GlobalArray::elemMaximumPatch(const GlobalArray * g_a, int64_t *alo, 
                                   int64_t *ahi, const GlobalArray * g_b, 
                                   int64_t *blo, int64_t *bhi, 
                                   int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\funcarg{}{clo}{g_c lower corner patch coordinates}{input}
\funcarg{}{chi}{g_c upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise maximum of the two patches
which must be of the same types and same number of
elements. For two-dimensional noncomplex arrays,
\begin{verbatim}
        c(i,j)  = max{a(i,j), b(i,j)}
\end{verbatim}

If the data type is complex, then
\begin{verbatim}
        c(i,j).real = max{ |a(i,j)|, |b(i,j)| } while c(i,j).image = 0.
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_MINIMUM}{Element-wise minimum of global arrays}

\begin{capi}
\begin{ccode}
void GA_Elem_minimum(int g_a, int g_b, int g_c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_minimum(g_a, g_b, g_c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a, g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMinimum(const GlobalArray * g_a, const 
                              GlobalArray * g_b) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise minimum of the two arrays
which must be of the same types and same number of
elements. For two dimensional arrays,
\begin{verbatim}
        c(i,j)  = min{a(i,j), b(i,j)}
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{ELEM_MINIMUM_PATCH}{Element-wise minimum of global array patches}

\begin{capi}
\begin{ccode}
void GA_Elem_minimum_patch(int g_a, int alo[], int ahi[], int g_b, 
                           int blo[], int bhi[], int g_c, int clo[], 
                           int chi[])
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a, g_b}{array handles}{input}
\funcarg{}{g_c}{array handle}{output}
\funcarg{}{alo[], ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[], bhi[]}{g_b patch coordinates}{input}
\funcarg{}{clo[], chi[]}{g_c patch coordinates}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_elem_minimum_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, 
                                 clo, chi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b}{array handles}{input}
\funcarg{integer}{g_c}{array handle}{output}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim),ahi(ndim)}{g_a patch dimensions}{input}
\funcarg{integer}{blo(ndim),bhi(ndim)}{g_b patch dimensions}{input}
\funcarg{integer}{clo(ndim),chi(ndim)}{g_c patch dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::elemMinimumPatch(const GlobalArray * g_a, int *alo, int *ahi,
                                   const GlobalArray * g_b, int *blo, int *bhi,
                                   int *clo, int *chi) const
void GlobalArray::elemMinimumPatch(const GlobalArray * g_a, int64_t *alo, 
                                   int64_t *ahi, const GlobalArray * g_b, 
                                   int64_t *blo, int64_t *bhi, 
                                   int64_t *clo, int64_t *chi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\funcarg{}{clo}{g_c lower corner patch coordinates}{input}
\funcarg{}{chi}{g_c upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the element-wise minimum of the two patches
which must be of the same types and same number of
elements. For two-dimensional of noncomplex arrays,
\begin{verbatim}
        c(i,j)  = min{a(i,j), b(i,j)}
\end{verbatim}

If the data type is complex, then
\begin{verbatim}
        c(i,j).real = min{ |a(i,j)|, |b(i,j)| } while c(i,j).image = 0.
\end{verbatim}

The result (c) may replace one of the input arrays (a/b).
This is a collective operation.
\end{desc}

\apih{SHIFT_DIAGONAL}{Add specified constant to diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Shift_diagonal(int g_a, void *c)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{double/complex/int/long/float}{c}{shift value}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_shift_diagonal(g_a, c)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double/complex/integer/float}{c}{TODO}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::shiftDiagonal(void *c) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{c}{double/complex/int/long/float constant to add}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
shift_diagoal(int g_a, value=None) 
\end{pycode}
\end{pyapi} 

\gcoll

\begin{desc}

Adds this constant to the diagonal elements of the matrix.
This is a collective operation.
\end{desc}

\apih{SET_DIAGONAL}{Set the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Set_diagonal(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_v}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_diagonal(g_a, g_v)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_v}{array handles}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::setDiagonal(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_v}{global array containing diagonal values}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_diagonal(int g_a, int g_v) 
\end{pycode}
\end{pyapi} 

\gcoll

\begin{desc}

Sets the diagonal elements of this matrix g_a with the elements of the vector g_v.
This is a collective operation.
\end{desc}

\apih{ZERO_DIAGONAL}{Zero the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Zero_diagonal(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_zero_diagonal(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::zeroDiagonal() const
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
zero_diagonal(int g_a)  
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Sets the diagonal elements of this matrix g_a with zeros.
This is a collective operation.
\end{desc}

\apih{ADD_DIAGONAL}{Add to the diagonal elements of a global array}

\begin{capi}
\begin{ccode}
void GA_Add_diagonal(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_v}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_add_diagonal(g_a, g_v)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_v}{array handles}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::addDiagonal(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_v}{global array containing diagonal elements to be added}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
add_diagonal(int g_a, int g_v)  
   g_a (int)     - the array handle 
   g_v (int)     - the vector handle 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Adds the elements of the vector g_v to the diagonal of this matrix g_a.
This is a collective operation.
\end{desc}

\apih{GET_DIAG}{Copy diagonal elements of a global array into another global array}

\begin{capi}
\begin{ccode}
void GA_Get_diag(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_v}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_get_diag(g_a, g_v)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{g_v}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::getDiagonal(const GlobalArray * g_a) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array containing diagonal elements}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_diag(int g_a, int g_v)
   g_a (int)        - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Inserts the diagonal elements of this matrix g_a into the vector g_v.
This is a collective operation.
\end{desc}

\apih{SCALE_ROWS}{Scale the rows of a global array with elements in another global array}

\begin{capi}
\begin{ccode}
void GA_Scale_rows(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_v}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scale_rows(g_a, g_v)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_v}{array handles}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scaleRows(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_v}{global array containing scale factors}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
g_a (int)                 - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales the rows of this matrix g_a using the vector g_v.
This is a collective operation.
\end{desc}

\apih{SCALE_COLS}{Scale columns of a global array with elements in another gobal array}

\begin{capi}
\begin{ccode}
void GA_Scale_cols(int g_a, int g_v)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_v}{array handles}{input}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_scale_cols(g_a, g_v)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_v}{array handles}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scaleCols(const GlobalArray * g_v) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_v}{global array containing scale factors}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scale_cols(int g_a, int g_v)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Scales the columns of this matrix g_a using the vector g_v.
This is a collective operation.
\end{desc}

\apih{NORM1}{Compute a global array's 1-norm}

\begin{capi}
\begin{ccode}
void GA_norm1(int g_a, double *nm)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{nm}{matrix/vector 1-norm value}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_norm1(g_a, nm)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double precision}{nm}{matrix/vector 1-norm value}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::norm1(double *nm) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{nm}{matrix/vector 1-norm value}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
norm1(int g_a)  
   g_a (int)                   - the array handle 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the 1-norm of the matrix or vector g_a.
This is a collective operation.
\end{desc}

\apih{NORM_INFINITY}{Compute a global array's infinite norm}

\begin{capi}
\begin{ccode}
void GA_Norm_infinity(int g_a, double *nm)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{}{nm}{matrix/vector infinity-norm value}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_norm_infinity(g_a, nm)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{double precision}{nm}{matrix/vector infinity-norm value}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::normInfinity(double *nm) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{nm}{matrix/vector infinity-norm value}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
norm_infinity(int g_a)  
   g_a (int)           - the array handle
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

Computes the infinity-norm of the matrix or vector g_a.
Returns: the 1-norm of the matrix or vector g_a. 

This is a collective operation.
\end{desc}

\apih{MEDIAN}{Compute a global arrays median}

\begin{capi}
\begin{ccode}
void GA_Median(int g_a, int g_b, int g_c, int g_m)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_b,g_c}{array handles}{input}
\funcarg{}{g_m}{array handle}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_median(g_a, g_b, g_c, g_m)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b,g_c}{array handles}{input}
\funcarg{integer}{g_m}{array handle}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::median(const GlobalArray * g_a, const GlobalArray * g_b,
                         const GlobalArray * g_c) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{g_c}{global array}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
median(int g_a, int g_b, int g_c, int g_m, alo=None, ahi=None, blo=None, 
bhi=None, clo=None, chi=None, mlo=None, mhi=None) 
   g_a (int)                       - the array handle 
   g_b (int)                       - the array handle 
   g_c (int)                       - the array handle 
   g_m (int)                       - the array handle for the result 
   alo (1D array-like of integers) - lower bound patch coordinates of g_a, 
                                     inclusive 
   ahi (1D array-like of integers) - higher bound patch coordinates of g_a, 
                                     exclusive 
   blo (1D array-like of integers) - lower bound patch coordinates of g_b, 
                                     inclusive 
   bhi (1D array-like of integers) - higher bound patch coordinates of g_b, 
                                     exclusive 
   clo (1D array-like of integers) - lower bound patch coordinates of g_c, 
                                     inclusive 
   chi (1D array-like of integers) - higher bound patch coordinates of g_c, 
                                     exclusive 
   mlo (1D array-like of integers) - lower bound patch coordinates of g_m, 
                                     inclusive 
   mhi (1D array-like of integers) - higher bound patch coordinates of g_m, 
                                     exclusive 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

Computes the componentwise Median of three arrays g_a, g_b, and g_c, and 
stores the result in this array g_m.  The result (m) may replace one of 
the input arrays (a/b/c).
This is a collective operation.
\end{desc}

\apih{MEDIAN_PATCH}{Compute a global array patch's median}

\begin{capi}
\begin{ccode}
void GA_Median_patch(int g_a, int alo[], int ahi[], int g_b, int blo[], 
                     int bhi[], int g_c, int clo[], int chi[], int g_m, 
                     int mlo[], int mhi[])
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_b,g_c}{array handles}{input}
\funcarg{}{g_m}{array handle}{output}
\funcarg{}{alo[],ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[],bhi[]}{g_b patch coordinates}{input}
\funcarg{}{clo[],chi[]}{g_c patch coordinates}{input}
\funcarg{}{mlo[],mhi[]}{g_m patch coordinates}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_median_patch(g_a, alo, ahi, g_b, blo, bhi, g_c, clo, chi, g_m, mlo, mhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b,g_c}{array handles}{input}
\funcarg{integer}{g_m}{array handle}{output}
\funcarg{integer}{ndim}{number of dimensions}{input}
\funcarg{integer}{alo(ndim),ahi(ndim)}{g_a patch dimensions}{input}
\funcarg{integer}{blo(ndim),bhi(ndim)}{g_b patch dimensions}{input}
\funcarg{integer}{clo(ndim),chi(ndim)}{g_c patch dimensions}{input}
\funcarg{integer}{mlo(ndim),mhi(ndim)}{g_m patch dimensions}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::medianPatch(const GlobalArray *g_a, int *alo, int *ahi,
                              const GlobalArray *g_b, int *blo, int *bhi,
                              const GlobalArray *g_c, int *clo, int *chi,
                              int *mlo, int *mhi) const;
void GlobalArray::medianPatch(const GlobalArray *g_a, int64_t *alo, 
                              int64_t *ahi, const GlobalArray *g_b, 
                              int64_t *blo, int64_t *bhi, const 
                              GlobalArray *g_c, int64_t *clo, 
                              int64_t *chi, int64_t *mlo, int64_t *mhi) 
                              const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_a}{global array}{input}
\funcarg{}{g_b}{global array}{input}
\funcarg{}{g_c}{global array}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\funcarg{}{clo}{g_c lower corner patch coordinates}{input}
\funcarg{}{chi}{g_c upper corner patch coordinates}{input}
\funcarg{}{mlo}{g_m lower corner patch coordinates}{input}
\funcarg{}{mhi}{g_m upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Computes the componentwise Median of three patches g_a, g_b, and g_c, 
and stores the result in this patch g_m.  The result (m) may replace 
one of the input patches (a/b/c).
This is a collective operation.
\end{desc}

\apih{STEP_MAX}{Compute a global array's step max}

\begin{capi}
\begin{ccode}
void GA_Step_max(int g_a, int g_b, double *step)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_b}{array handles where g_b is step direction}{input}
\funcarg{}{step}{maximum step size}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_step_max(g_a, g_b, step)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b}{array handles}{input}
\funcarg{double precision}{step}{the maximum step}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stepMax(const GlobalArray * g_b, double *step) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_b}{global array where g_b is the step direction}{input}
\funcarg{}{step}{the maximum step}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
step_max(int g_a, int g_b, alo=None, ahi=None, blo=None, bhi=None) 
\end{pycode}
\end{pyapi} 

\gcoll

\begin{desc}

Calculates the largest multiple of a vector g_b that can be added to 
this vector g_a while keeping each element of this vector non-negative.
This is a collective operation.
\end{desc}

\apih{STEP_MAX2}{Compute a global array's step max2}

\begin{capi}
\begin{ccode}
void GA_Step_max2(int g_xx, int g_vv, int g_xxll, int g_xxuu, double * step2)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_xx}{array handle}{input}
\funcarg{}{g_vv}{step direction array handle}{input}
\funcarg{}{g_xxll}{lower bounds array handle}{input}
\funcarg{}{g_xxuu}{upper bounds array handle}{input}
\funcarg{}{step2}{maximum step size}{output}
\end{funcargs}
\end{capi}
\begin{fapi}
\begin{fcode}
subroutine ga_step_max2(g_xx, g_vv, g_xxll, g_xxuu, step2)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_xx,g_vv,g_xxll,g_xxuu}{array handles}{input}
\funcarg{double precision}{step2}{the maximum step size}{output}
\funcarg{}{g_vv}{the step direction}{input}
\funcarg{}{g_xxll}{lower bounds}{input}
\funcarg{}{g_xxuu}{upper bounds}{input}
\end{funcargs}
\end{fapi}
\gcoll

\begin{desc}

Calculates the largest step size that should be used in a projected bound line search.
This is a collective operation.
\end{desc}

\apih{STEP_MAX_PATCH}{Compute a global array patch's step max}

\begin{capi}
\begin{ccode}
void GA_Step_max_patch(int g_a, int alo[], int ahi[], int g_b, blo[], bhi[], double *step)
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_b}{array handles where g_b is step direction}{input}
\funcarg{}{step}{the maximum step}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_step_max_patch(g_a, alo, ahi, g_b, blo, bhi, step)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b}{array handles where g_b is step direction}{input}
\funcarg{integer}{alo,ahi,blo,bhi}{patch coordinates of g_a and g_b}{input}
\funcarg{double precision}{step}{the maximum step}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::stepMaxPatch(int *alo, int *ahi, const GlobalArray *g_b,
                               int *blo, int *bhi, double *step) const
void GlobalArray::stepMaxPatch(int64_t *alo, int64_t *ahi, const GlobalArray
                               *g_b, int64_t *blo, int64_t *bhi, 
                               double *step) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_b}{global array representing step direction}{input}
\funcarg{}{alo}{g_a lower corner patch coordinates}{input}
\funcarg{}{ahi}{g_a upper corner patch coordinates}{input}
\funcarg{}{blo}{g_b lower corner patch coordinates}{input}
\funcarg{}{bhi}{g_b upper corner patch coordinates}{input}
\end{funcargs}
\end{cxxapi}
\gcoll

\begin{desc}

Calculates the largest multiple of a vector g_b that can be added to this vector g_a while keeping each element of this vector non-negative.
This is a collective operation.
\end{desc}

\apih{PGROUP_GET_DEFAULT}{Set default GA processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_default()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_default()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup* PGroup::getDefault()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_default() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function will return a handle to the default processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation. 
\end{desc}

\apih{PGROUP_GET_MIRROR}{Get the mirrored processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_mirror()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_mirror()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup * PGroup::getMirror()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_mirror() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function will return a handle to the mirrored processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation. 
\end{desc}

\apih{PGROUP_GET_WORLD}{Get the world processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_get_world()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_get_world()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
static PGroup * PGroup::getWorld()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_get_world() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function will return a handle to the world processor group, which can then be used to create a global array using one of the NGA_create_*_config or GA_Set_pgroup calls.

This is a local operation.
\end{desc}

\apih{PGROUP_SYNC}{Synchronize processes in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_sync(int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_sync(p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::sync()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_sync(int pgroup) 
\end{pycode}
\end{pyapi} 

\gcoll
\begin{desc}

This operation executes a synchronization group across the processors in the processor group specified by p_handle. Nodes outside this group are unaffected.

This is a collective operation on the processor group specified by p_handle. 
\end{desc}

\apih{PGROUP_BRDCST}{Broadcast elements among processes in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_brdcst(int p_handle, void* buf, int lenbuf, int root)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{buf}{pointer to buffer containing data}{input/output}
\funcarg{}{lenbuf}{length of data (in bytes)}{input}
\funcarg{}{root}{processor sending message}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_brdcst(p_handle, type, buf, lenbuf, root)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{type}{message index}{input}
\funcarg{byte}{buf(lenbuf)}{local message buffer}{input/output}
\funcarg{integer}{lenbuf}{length of message}{input}
\funcarg{integer}{root}{processor sending message}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::brdcst(void* buf, int lenbuf, int root)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{buf}{pointer to buffer containing data}{input/output}
\funcarg{}{lenbuf}{length of data (in bytes)}{input}
\funcarg{}{root}{processor sending message}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_brdcst(int pgroup, ndarray buffer, int root) 
   pgroup (int)        - processor group handle 
   buffer (array-like) - the message 
   root (int)          - the process which is sending 
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

Broadcast data from processor specified by root to all other processors in the processor group specified by p_handle. The length of the message in bytes is specified by lenbuf. The initial and broadcasted data can be found in the buffer specified by the pointer buf.

If the buffer is not contiguous, an error is raised. This operation is provided only for convenience purposes: it is available regardless of the message-passing library that GA is running with.


This is a collective operation on the processor group specified by p_handle. 

Returns: 
The buffer in case a temporary was passed in. 
\end{desc}

\apih{PGROUP_DGOP}{Double global operation with a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_dgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{buf}{buffer containing data}{input/output}
\funcarg{}{n}{number of elements in x}{input}
\funcarg{}{op}{operation to be performed}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_dgop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{type}{message index}{input}
\funcarg{double precision}{buf(n)}{double precision array}{input/output}
\funcarg{integer}{n}{number elements in array}{input}
\funcarg{character*(*)}{op}{operation on data}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(double *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a double precision array present on each processor in the processor group p_handle. The GA_Pgroup_dgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processor in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle. 
\end{desc}

\apih{PGROUP_IGOP}{Integer global operation within a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_igop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{buf}{buffer containing data}{input/output}
\funcarg{}{n}{number of elements in x}{input}
\funcarg{}{op}{operation to be performed}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_igop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{type}{message index}{input}
\funcarg{integer}{buf(n)}{integer array}{input/output}
\funcarg{integer}{n}{number elements in array}{input}
\funcarg{character*(*)}{op}{operation on data}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(int *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is an integer array present on each processor in the processor group p_handle. The GA_Pgroup_igop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle. 
\end{desc}

\apih{PGROUP_LGOP}{Long global operation in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_lgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{buf}{buffer containing data}{input/output}
\funcarg{}{n}{number of elements in x}{input}
\funcarg{}{op}{operation to be performed}{input}
\end{funcargs}
\end{capi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(long *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a long integer array present on each processor in the processor group p_handle. The GA_Pgroup_lgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle. 
\end{desc}

\apih{PGROUP_FGOP}{Floating point global operation in a processor group}

\begin{capi}
\begin{ccode}
void GA_Pgroup_lgop(int p_handle, double buf*, int n, char* op)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\funcarg{}{buf}{buffer containing data}{input/output}
\funcarg{}{n}{number of elements in x}{input}
\funcarg{}{op}{operation to be performed}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pgroup_sgop(p_handle, type, buf, n, op)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\funcarg{integer}{type}{message index}{input}
\funcarg{real}{buf(n)}{single precision array}{input/output}
\funcarg{integer}{n}{number elements in array}{input}
\funcarg{character*(*)}{op}{operation on data}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void PGroup::gop(float *buf, int n, char* op)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{n}{number of elements}{input}
\funcarg{}{x[n]}{array of elements}{input/output}
\funcarg{}{op}{operator}{input}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

The buf[n] is a single precision array present on each processor in the processor group p_handle. The GA_Pgroup_fgop `sums' all elements in buf[n] across all processors in the group specified by p_handle using the commutative operation specified by the character string op.  The result is broadcast to all processors in p_handle. Allowed strings are `+', `*', `max', `min', `absmax', `absmin'. The use of lowerecase for operators is necessary.

This is a collective operation on the processor group specifed by p_handle. 
\end{desc}

\apih{PGROUP_NNODES}{Number of GA ranks in a processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_nnodes(int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_nnodes(p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int PGroup::nodes()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_nnodes(int pgroup) 
   pgroup (int)                  - the group handle 
\end{pycode}
\end{pyapi}

\local
\begin{desc}

This function returns the number of processors contained in the group specified by p_handle.

Returns the number of processors contained in the group specified by 
pgroup.

This is a local operation.
\end{desc}

\apih{PGROUP_NODEID}{GA rank of invoking process in a processor group}

\begin{capi}
\begin{ccode}
int GA_Pgroup_nodeid(int p_handle)
\end{ccode}
\begin{funcargs}
\funcarg{}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer function ga_pgroup_nodeid(p_handle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{p_handle}{processor group handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int PGroup::nodeid()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pgroup_nodeid(int pgroup) 
   pgroup (int)                  - the group handle
\end{pycode}
\end{pyapi}
\local
\begin{desc}

This function returns the relative index of the processor in the processor group specified by p_handle. This index will generally differ from the absolute processor index returned by GA_Nodeid if the processor group is not the world group.

Returns the relative index of the processor in the processor group 
specified by pgroup.

This is a local operation.

\end{desc}

\apih{MERGE_MIRRORED}{Merge a mirrored global array}

\begin{capi}
\begin{ccode}
int GA_Merge_mirrored(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handles}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_merge_mirrored(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::mergeMirrored()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
merge_mirrored(int g_a) 
   g_a (int)                     - array handle
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

This subroutine merges mirrored arrays by adding the contents of each array across nodes. The result is that each mirrored copy of the array represented by g_a is the sum of the individual arrays before the merge operation. After the merge, all mirrored arrays are equal.

This is a  collective  operation.

\end{desc}

\apih{IS_MIRRORED}{Check whether a global array is mirrored}

\begin{capi}
\begin{ccode}
int GA_Is_mirrored(int g_a)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer ga_is_mirrored(g_a)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::isMirrored()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
is_mirrored(int g_a) 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This subroutine checks if the array is a mirrored array or not. Returns 1 if it is a mirrored array, else it returns 0.

This is a local operation.

\end{desc}

\apih{MERGE_DISTR_PATCH}{Merge a patched of a mirrored global array}

\begin{capi}
\begin{ccode}
int NGA_Merge_distr_patch(int g_a, int alo[], int ahi[], int g_b, int blo[], int bhi[])
\end{ccode}
\begin{funcargs}
\funcarg{g_a}{,g_b}{array handles}{input}
\funcarg{}{alo[],ahi[]}{g_a patch coordinates}{input}
\funcarg{}{blo[],bhi[]}{g_b patch coordinates}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
integer nga_merge_distr_patch(g_a, alo, ahi, g_b, blo, bhi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a,g_b}{array handles}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{NA}
\funcarg{integer}{alo(ndim),ahi(ndim)}{g_a patch coordinates}{input}
\funcarg{integer}{blo(ndim),bhi(ndim)}{g_b patch coordinates}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::mergeDistrPatch(int alo[], int ahi[], GlobalArray *g_a,
                                  int blo[], int bhi[])
void GlobalArray::mergeDistrPatch(int64_t alo[], int64_t ahi[], GlobalArray *g_a,
                                  int64_t blo[], int64_t bhi[])
\end{cxxcode}
\begin{funcargs}
\funcarg{}{alo[ndim]}{patch indices of mirrored array}{input}
\funcarg{}{ahi[ndim]}{patch indices of mirrored array}{input}
\funcarg{}{blo[ndim]}{patch indices of result array}{input}
\funcarg{}{bhi[ndim]}{patch indices of result array}{input}
\funcarg{}{g_a}{global array containing result}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
merge_distr_patch(int g_a, alo, ahi, int g_b, blo, bhi) 
   g_a (int)                       - array handle 
   alo (1D array-like of integers) - g_a patch coordinate 
   ahi (1D array-like of integers) - g_a patch coordinate 
   g_b (int)                       - array handle 
   blo (1D array-like of integers) - g_b patch coordinate 
   bhi (1D array-like of integers) - g_b patch coordinate 
\end{pycode}
\end{pyapi}
\gcoll
\begin{desc}

This function merges all copies of a patch of a mirrored array (g_a) into a patch in a distributed array (g_b).

This is a collective operation.
\end{desc}

\apih{NBGET}{Non-blocking get from a global array}

\begin{capi}
\begin{ccode}
void NGA_NbGet(int g_a, int lo[], int hi[], void* buf, int ld[], ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data goes}{output}
\funcarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{}{nbhandle}{pointer to the non-blocking request handle}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbget(g_a, ilo, ihi, jlo, jhi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ilo,ihi}{starting indices for global array section}{input}
\funcarg{integer}{jlo,jhi}{ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data goes}{output}
\funcarg{integer}{ld}{leading dimension/stride/extent for buffer array}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbget(g_a, lo, hi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{integer}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{type}{buf}{local buffer array where the data goes}{output}
\funcarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbGet(int lo[], int hi[], void *buf, int ld[],
                        GANbhdl *nbhandle)
void GlobalArray::nbGet(int64_t lo[], int64_t hi[], void *buf, int64_t ld[],
                        GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{patch coordinates of block}{input}
\funcarg{}{hi[ndim]}{patch coordinates of block}{input}
\funcarg{}{buf}{local buffer to receive data}{input}
\funcarg{}{ld[ndim-1]}{array of strides for local data}{input}
\funcarg{}{nbhandle}{nonblocking handle}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbget(int g_a, lo=None, hi=None, ndarray buffer=None) 
   g_a (int)                      - the array handle 
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive 
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive 
   buffer (ndarray)               - Fill this buffer instead of allocating a 
                                    new one internally. Must be contiguous and 
                                    have same number of elements as patch. 
\end{pycode}
\end{pyapi}

\ncoll
\begin{desc}

A non-blocking version of the blocking get operation. The get operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Copies data from global array section to the local array buffer.

The local array is assumed to be have the same number of dimensions as the global array. Any detected inconsitencies/errors in the input arguments are fatal.

This is a non-blocking one-sided operation.
Returns: 
The local array buffer. 

\end{desc}

\apih{NBPUT}{Non-blocking put into a global array}

\begin{capi}
\begin{ccode}
void NGA_NbPut(int g_a, int lo[], int hi[], void* buf, int ld[], 
ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data is}{input}
\funcarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{}{nbhandle}{pointer to the non-blocking request handle}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbput(g_a, ilo, ihi, jlo, jhi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ilo,ihi}{starting indices for global array section}{input}
\funcarg{integer}{jlo,jhi}{ending indices for global array section}{input}
\funcarg{}{typebuf}{local buffer array where the data is}{input}
\funcarg{integer}{ld}{leading dimension/stride/extent for buffer array}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbput(g_a, lo, hi, buf, ld, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{integer}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{typebuf}{local buffer array where the data is}{input}
\funcarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbPut(int lo[], int hi[], void *buf, int ld[],
                        GANbhdl *nbhandle)
void GlobalArray::nbPut(int64_t lo[], int64_t hi[], void *buf, int64_t ld[],
                        GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{patch coordinates of block}{input}
\funcarg{}{hi[ndim]}{patch coordinates of block}{input}
\funcarg{}{buf}{local buffer to receive data}{input}
\funcarg{}{ld[ndim-1]}{array of strides for local data}{input}
\funcarg{}{nbhandle}{nonblocking handle}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbput(int g_a, buffer, lo=None, hi=None) 
   g_a (int)                      - the array handle 
   buffer (array-like)            - the data to put 
   lo (1D array-like of integers) - lower bound patch coordinates, 
                                    inclusive 
   hi (1D array-like of integers) - higher bound patch coordinates, 
                                    exclusive 
\end{pycode}
\end{pyapi}

\ncoll

\begin{desc}

A non-blocking version of the blocking put operation. The put operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Copies data from local array buffer to the global array section.

The local array is assumed to be have the same number of dimensions as the global array. Any detected inconsitencies/errors in input arguments are fatal.

This is a non-blocking one-sided operation.
\end{desc}

\apih{NBACC}{Non-blocking accumulate into a global array}

\begin{capi}
\begin{ccode}
void NGA_NbAcc(int g_a, int lo[], int hi[], void* buf, int ld[], void *alpha, 
               ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{global array handle}{input}
\funcarg{}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{buf}{pointer to the local buffer array where the data is}{input}
\funcarg{ld}{[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{double/double complex/long*}{alpha}{scale factor}{input}
\funcarg{}{nbhandle}{pointer to the non-blocking request handle}{input}
\end{funcargs}
\end{capi}

\begin{f2dapi}
\begin{fcode}
subroutine ga_nbacc(g_a, ilo, ihi, jlo, jhi, buf, ld, alpha, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ilo,ihi}{starting indices for global array section}{input}
\funcarg{integer}{jlo,jhi}{ending indices for global array section}{input}
\funcarg{}{typebuf}{local buffer array where the data is}{input}
\funcarg{integer}{ld}{leading dimension/stride/extent for buffer array}{input}
\funcarg{double precision/complex}{alpha}{scale factor}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{f2dapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbacc(g_a, lo, hi, buf, ld, alpha, nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{global array handle}{input}
\funcarg{integer}{ndim}{number of dimensions of the global array}{input}
\funcarg{integer}{lo[ndim]}{array of starting indices for global array section}{input}
\funcarg{integer}{hi[ndim]}{array of ending indices for global array section}{input}
\funcarg{}{typebuf}{local buffer array where the data is}{input}
\funcarg{integer}{ld[ndim-1]}{array specifying leading dimensions/strides/extents for buffer array}{input}
\funcarg{double precision/complex}{alpha}{scale factor}{input}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbAcc(int lo[], int hi[], void *buf, int ld[], void *alpha,
                        GANbhdl *nbhandle)
void GlobalArray::nbAcc(int64_t lo[], int64_t hi[], void *buf, int64_t ld[], 
                        void *alpha, GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo[ndim]}{patch coordinates of block}{input}
\funcarg{}{hi[ndim]}{patch coordinates of block}{input}
\funcarg{}{buf}{local buffer to receive data}{input}
\funcarg{}{ld[ndim-1]}{array of strides for local data}{input}
\funcarg{alpha}{}{multiplier for data before adding to existing results}{input}
\funcarg{}{nbhandle}{nonblocking handle}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbacc(int g_a, buffer, lo=None, hi=None, alpha=None) 
   g_a (int)                      - the array handle 
   buffer (array-like)            - must be contiguous and have same number
                                    of elements as patch 
   lo (1D array-like of integers) - lower bound patch coordinates, inclusive 
   hi (1D array-like of integers) - higher bound patch coordinates, exclusive 
   alpha (object)                 - multiplier (converted to the appropriate type)
\end{pycode}
\end{pyapi}
\ncoll

\begin{desc}

A non-blocking version of the blocking accumulate operation. The accumulate operation can be completed locally by making a call to the wait (e.g., NGA_NbWait) routine.

Non-blocking version of ga.acc.

The accumulate operation can be completed locally by making a call to the ga.nbwait() routine.

Combines data from buffer with data in the global array patch.

The buffer array is assumed to be have the same number of dimensions as the global array. If the buffer is not contiguous, a contiguous copy will be made.

global array section (lo[],hi[]) += alpha * buffer

This is a non-blocking one-sided operation.
\end{desc}

\apih{NBWAIT}{Wait for a non-blocking GA operation}

\begin{capi}
\begin{ccode}
void NGA_NbWait(ga_nbhdl_t* nbhandle)
\end{ccode}
\begin{funcargs}
\funcarg{}{nbhandle}{pointer to the non-blocking request handle}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_nbwait(nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{fapi}

\begin{fapi}
\begin{fcode}
subroutine nga_nbwait(nbhandle)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{nbhandle}{non-blocking request handle}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::nbWait(GANbhdl *nbhandle)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{nbhandle}{nonblocking handle}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
nbwait(ga_nbhdl_t nbhandle) 
\end{pycode}
\end{pyapi} 

\ncoll

\begin{desc}

This function completes a non-blocking one-sided operation locally. Waiting on a nonblocking put or an accumulate operation assures that data was injected into the network and the user buffer can now be reused. Completing a get operation assures data has arrived into the user memory and is ready for use. The wait operation ensures only local completion. 

Unlike their blocking counterparts, the nonblocking operations are not ordered with respect to the destination. Performance being one reason, the other reason is that by ensuring ordering we incur additional and possibly unnecessary overhead on applications that do not require their operations to be ordered. For cases where ordering is necessary, it can be done by calling a fence operation. The fence operation is provided to the user to confirm remote completion if needed.
\end{desc}

\apih{WTIME}{Return time in seconds}

\begin{capi}
\begin{ccode}
double GA_Wtime()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
double precision function ga_wtime()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
double GlobalArray::wtime()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
wtime() 
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function returns a wall (or elapsed) time on the calling processor. Returns time in seconds representing elapsed wall-clock time since an arbitrary time in the past. Example:

\begin{verbatim}
double starttime, endtime;
starttime = GA_Wtime();
.... code snippet to be timed ....
endtime   = GA_Wtime();
printf(\"Time taken = \%lf secondsn\", endtime-starttime);
\end{verbatim}

This is a local operation.
This function is only available in release 4.1 or greater.
\end{desc}

\apih{SET_DEBUG}{Set GA debug flag}

\begin{capi}
\begin{ccode}
void GA_Set_debug(int dbg)
\end{ccode}
\begin{funcargs}
\funcarg{}{dbg}{value to set internal flag}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_set_debug(flag)
\end{fcode}
\begin{funcargs}
\funcarg{logical}{flag}{value to set internal debug flag}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GAServices::setDebug(int dbg);
\end{cxxcode}
\begin{funcargs}
\funcarg{}{dbg}{value to set internal flag}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
set_debug(int debug)  
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function sets an internal flag in the GA library to either true or false. The value of this flag can be recovered at any time using the GA_Get_debug function. The flag is set to false when the the GA library is initialized. This can be useful in a number of debugging situations, especially when examining the behavior of routines that are called in multiple locations in a code.

This is a local operation.
\end{desc}

\apih{GET_DEBUG}{Retrieve value of GA debug flag}

\begin{capi}
\begin{ccode}
int GA_Get_debug()
\end{ccode}
\end{capi}

\begin{fapi}
\begin{fcode}
logical function ga_get_debug()
\end{fcode}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
int GlobalArray::getDebug()
\end{cxxcode}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
get_debug()  
\end{pycode}
\end{pyapi} 

\local
\begin{desc}

This function returns the value of an internal flag in the GA library whose value can be set using the GA_Set_debug subroutine.

This is a local operation.
\end{desc}

\apih{PATCH_ENUM}{Enumerate a global array patch}

\begin{capi}
\begin{ccode}
void GA_Patch_enum(int g_a, int lo, int hi, void *start, void *inc)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_a}{array handle}{input}
\funcarg{lo}{,hi}{patch coordinates}{input}
\funcarg{}{start}{starting value of enumeration}{input}
\funcarg{}{inc}{increment value}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_patch_enum(g_a, lo, hi, start, inc)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_a}{array handle}{input}
\funcarg{integer}{lo,hi}{low and high values of array patch}{input}
\funcarg{integer/double precision/complex}{start}{starting value of enumeration}{input}
\funcarg{integer/double precision/complex}{inc}{increment value}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::patchEnum(int lo, int hi, void *istart, void *inc)
void GlobalArray::patchEnum(int64_t lo, int64_t hi, void *start, void *inc)
\end{cxxcode}
\begin{funcargs}
\funcarg{}{lo}{coordinate interval to enumerate}{input}
\funcarg{}{hi}{coordinate interval to enumerate}{input}
\funcarg{}{istart}{starting value of enumeration}{input}
\funcarg{}{inc}{increment value}{input}
\end{funcargs}
\end{cxxapi}
\gcoll
\begin{desc}

This subroutine enumerates the values of an array between elements lo and hi starting with the value start and incrementing each subsequent value by inc. This operation is only applicable to 1-dimensional arrays. An example of its use is shown below:

\begin{verbatim}
GA_Patch_enum(g_a, 1, n, 7, 2);

g_a:  7  9 11 13 15 17 19 21 23 ...
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SCAN_ADD}{Scan add (fixme)}

\begin{capi}
\begin{ccode}
void GA_Scan_add(int g_src, int g_dest, int g_mask, int lo, int hi, int excl)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_src}{handle for source array}{input}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{lo}{,hi}{low and high values of range on which operation is performed}{input}
\funcarg{}{excl}{value to signify if masked values are included in in add}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scan_add(g_src, g_dest, g_mask, lo, hi, excl)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_src}{handle for source array}{input}
\funcarg{integer}{g_dest}{handle for destination array}{output}
\funcarg{integer}{g_mask}{handle for integer array representing a bit mask}{input}
\funcarg{integer}{lo,hi}{low and high values of range on which operation is performed}{input}
\funcarg{integer}{excl}{value to signify if masked values are included in add}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scanAdd(const GlobalArray *g_dest, const GlobalArray *g_mask,
                          int lo, int hi, int excl) const
void GlobalArray::scanAdd(const GlobalArray *g_dest, const GlobalArray *g_mask,
                          int64_t lo, int64_t hi, int excl) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{}{lo}{low and high values of range on which operation is performed}{input}
\funcarg{}{hi}{low and high values of range on which operation is performed}{input}
\funcarg{}{excl}{value to signify if masked values are included in in add}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scan_add(int g_src, int g_dst, int g_msk, lo=None, hi=None, int excl=False)
   g_src (int)                    - handle for source arrray 
   g_dst (int)                    - handle for destination array 
   g_msk (int)                    - handle for integer array representing mask 
   lo (1D array-like of integers) - low value of range on which operation is
                                    performed 
   hi (1D array-like of integers) - hi value of range on which operation is
                                    performed 
   excl (bool)                    - whether the first value is set to 0 (see above)
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This operation will add successive elements in a source vector g_src and put the results in a destination vector g_dest. The addition will restart based on the values of the integer mask vector g_mask. The scan is performed within the range specified by the integer values lo and hi. Note that this operation can only be applied to 1-dimensional arrays. The excl flag determines whether the sum starts with the value in the source vector corresponding to the location of a 1 in the mask vector (excl=0) or whether the first value is set equal to 0 (excl=1). Some examples of this operation are given below.

\begin{verbatim}
GA_Scan_add(g_src, g_dest, g_mask, 1, n, 0);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  3  6 10 16 21  7 15  9 19 30 12 25 39 15 16 33

GA_Scan_add(g_src, g_dest, g_mask, 1, n, 1);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   0  1  3  6 10 15  0  7  0  9 19  0 12 25  0  0 16
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{SCAN_COPY}{Scan copy (fixme)}

\begin{capi}
\begin{ccode}
void GA_Scan_copy(int g_src, int g_dest, int g_mask, int lo, int hi)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_src}{handle for source array}{input}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{lo}{,hi}{low and high values of range on which operation is performed}{input}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_scan_copy(g_src, g_dest, g_mask, lo, hi)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_src}{handle for source array}{input}
\funcarg{integer}{g_dest}{handle for destination array}{output}
\funcarg{integer}{g_mask}{handle for integer array representing a bit mask}{input}
\funcarg{integer}{lo,hi}{low and high values of range on which operation is performed}{input}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::scanCopy(const GlobalArray *g_dest, const GlobalArray *g_mask,
                           int lo, int hi) const
void GlobalArray::scanCopy(const GlobalArray *g_dest, const GlobalArray *g_mask,
                           int64_t lo, int64_t hi) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{}{lo}{low values of range on which operation is performed}{input}
\funcarg{}{hi}{high values of range on which operation is performed}{input}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
scan_copy(int g_src, int g_dst, int g_msk, lo=None, hi=None)  
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

This subroutine does a segmented scan-copy of values in the source array g_src into a destination array g_dest with segments defined by values in the integer mask array g_mask. The scan-copy operation is only applied to the range between the lo and hi indices. This operation is restriced to 1-dimensional arrays. The resulting destination array will consist of segments of consecutive elements with the same value. An example is shown below.

\begin{verbatim}
GA_Scan_copy(g_src, g_dest, g_mask, 1, n);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  1  1  1  1  1  7  7  9  9  9 12 12 12 15 16 16

This is  a collective operation.
\end{verbatim}

\end{desc}

\apih{PACK}{Pack (fixme)}

\begin{capi}
\begin{ccode}
void GA_Pack(int g_src, int g_dest, int g_mask, int lo, int hi, 
             int *icount)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_src}{handle for source array}{input}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{lo}{,hi}{low and high values of range on which operation is performed}{input}
\funcarg{}{icount}{number of values in compressed array}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_pack(g_src, g_dest, g_mask, lo, hi, icount)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_src}{handle for source array}{input}
\funcarg{integer}{g_dest}{handle for destination array}{output}
\funcarg{integer}{g_mask}{handle for integer array representing a bit mask}{input}
\funcarg{integer}{lo,hi}{low and high values of range on which operation is performed}{input}
\funcarg{integer}{icount}{number of values in compressed array}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::pack(const GlobalArray *g_dest, const GlobalArray *g_mask,
                       int lo, int hi, int *icount) const
void GlobalArray::pack(const GlobalArray *g_dest, const GlobalArray *g_mask,
                       int64_t lo, int64_t hi, int64_t *icount) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_dest}{destination array}{output}
\funcarg{}{g_mask}{mask array}{input}
\funcarg{}{lo}{coordinate interval to pack}{input}
\funcarg{}{hi}{coordinate interval to pack}{input}
\funcarg{}{icount}{number of packed elements}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
pack(int g_src, int g_dst, int g_msk, lo=None, hi=None)
   g_src (int)                    - handle for source arrray 
   g_dst (int)                    - handle for destination array 
   g_msk (int)                    - handle for integer array representing mask 
   lo (1D array-like of integers) - low value of range on which operation 
                                    is performed 
   hi (1D array-like of integers) - hi value of range on which operation 
                                    is performed 
\end{pycode}
\end{pyapi}
\gcoll

\begin{desc}

The pack subroutine is designed to compress the values in the source vector g_src into a smaller destination array g_dest based on the values in an integer mask array g_mask. The values lo and hi denote the range of elements that should be compressed and icount is a variable that on output lists the number of values placed in the compressed array. This operation is the complement of the GA_Unpack operation. An example is shown below

\begin{verbatim}
GA_Pack(g_src, g_dest, g_mask, 1, n, \&icount);

g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_src:    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
g_dest:   1  7  9 12 15 16
icount:   6
\end{verbatim}

This is a collective operation.
\end{desc}

\apih{UNPACK}{Unpack (fixme)}

\begin{capi}
\begin{ccode}
void GA_Unpack(int g_src, int g_dest, int g_mask, int lo, int hi, 
               int *icount)
\end{ccode}
\begin{funcargs}
\funcarg{}{g_src}{handle for source array}{input}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{lo}{,hi}{low and high values of range on which operation is performed}{input}
\funcarg{}{icount}{number of values in uncompressed array}{output}
\end{funcargs}
\end{capi}

\begin{fapi}
\begin{fcode}
subroutine ga_unpack(g_src, g_dest, g_mask, lo, hi, icount)
\end{fcode}
\begin{funcargs}
\funcarg{integer}{g_src}{handle for source array}{input}
\funcarg{integer}{g_dest}{handle for destination array}{output}
\funcarg{integer}{g_mask}{handle for integer array representing a bit mask}{input}
\funcarg{integer}{lo,hi}{low and high values of range on which operation is performed}{input}
\funcarg{integer}{icount}{number of values in uncompressed array}{output}
\end{funcargs}
\end{fapi}

\begin{cxxapi}
\begin{cxxcode}
void GlobalArray::unpack(GlobalArray *g_dest, GlobalArray *g_mask,
                         int lo, int hi, int *icount) const
void GlobalArray::unpack(GlobalArray *g_dest, GlobalArray *g_mask,
                         int64_t lo, int64_t hi, int64_t *icount) const
\end{cxxcode}
\begin{funcargs}
\funcarg{}{g_dest}{handle for destination array}{output}
\funcarg{}{g_mask}{handle for integer array representing mask}{input}
\funcarg{}{lo}{low value of range on which operation is performed}{input}
\funcarg{}{hi}{high value of range on which operation is performed}{input}
\funcarg{}{icount}{number of values in uncompressed array}{output}
\end{funcargs}
\end{cxxapi}

\begin{pyapi}
\begin{pycode}
unpack(int g_src, int g_dst, int g_msk, lo=None, hi=None) 
   g_src (int)                    - handle for source arrray 
   g_dst (int)                    - handle for destination array 
   g_msk (int)                    - handle for integer array representing mask 
   lo (1D array-like of integers) - low value of range on which operation 
                                    is performed 
   hi (1D array-like of integers) - hi value of range on which operation 
                                    is performed 
\end{pycode}
\end{pyapi}

\gcoll

\begin{desc}

The unpack subroutine is designed to expand the values in the source vector g_src into a larger destination array g_dest based on the values in an integer mask array g_mask. The values lo and hi denote the range of elements that should be compressed and icount is a variable that on output lists the number of values placed in the uncompressed array. This operation is the complement of the GA_Pack operation. An example is shown below.

\begin{verbatim}
 GA_Unpack(g_src, g_dest, g_mask, 1, n, \&icount);

g_src:    1  7  9 12 15 16
g_mask:   1  0  0  0  0  0  1  0  1  0  0  1  0  0  1  1  0
g_dest:   1  0  0  0  0  0  7  0  9  0  0 12  0  0 15 16  0
icount:   6
\end{verbatim}

This is a collective operation.

\end{desc}

\end{document}
